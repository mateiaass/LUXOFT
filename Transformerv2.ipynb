{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c1230c26bcd45d8b0951f29a5c59ba4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c89c354142b1444cb0f33ae25a453f78",
              "IPY_MODEL_a7083d5a971f4aa9855f2a857601dac6",
              "IPY_MODEL_bd866ee8e8134b88bbd8cf061e4dbb85",
              "IPY_MODEL_08cbbcf912e04d5ca6f410bf39fe85e4",
              "IPY_MODEL_f719b87fa403412b9d68e689ab87184d"
            ],
            "layout": "IPY_MODEL_080b63327afc4e4c94cf4242f2553d54"
          }
        },
        "c89c354142b1444cb0f33ae25a453f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dd1e9e7af604c2c9fb2741f10aabf14",
            "placeholder": "​",
            "style": "IPY_MODEL_a0968d84b0414c498f6498354035b407",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a7083d5a971f4aa9855f2a857601dac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_dc2d6cc7361a4b538ec82f6f35838a4d",
            "placeholder": "​",
            "style": "IPY_MODEL_3eace533783e49b3a0d099b97a15c1ce",
            "value": ""
          }
        },
        "bd866ee8e8134b88bbd8cf061e4dbb85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_bca1d44dc7244ca19f5b72dfcd2393d1",
            "style": "IPY_MODEL_0b11e2f5a00e4f50a2518e75acb8bedc",
            "value": true
          }
        },
        "08cbbcf912e04d5ca6f410bf39fe85e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c26b84ae85ff48959e31b17cbeb90b89",
            "style": "IPY_MODEL_b0c23f555cd74199b449843e53d4849c",
            "tooltip": ""
          }
        },
        "f719b87fa403412b9d68e689ab87184d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4179f2758c64cdc840dd84e1f9481ba",
            "placeholder": "​",
            "style": "IPY_MODEL_118817e126e74e53b1ae63e69b468693",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "080b63327afc4e4c94cf4242f2553d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "8dd1e9e7af604c2c9fb2741f10aabf14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0968d84b0414c498f6498354035b407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc2d6cc7361a4b538ec82f6f35838a4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eace533783e49b3a0d099b97a15c1ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bca1d44dc7244ca19f5b72dfcd2393d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b11e2f5a00e4f50a2518e75acb8bedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c26b84ae85ff48959e31b17cbeb90b89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0c23f555cd74199b449843e53d4849c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a4179f2758c64cdc840dd84e1f9481ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118817e126e74e53b1ae63e69b468693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "Q70reR67PfNp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "OMdGxf6Gifhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6235038d-dc52-4b1d-86b1-b474254cf2cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_model, dropout_p, max_len):\n",
        "        super().__init__()\n",
        "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "        # max_len determines how far the position can have an effect on a token (window)\n",
        "\n",
        "        # Info\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Encoding - From formula\n",
        "        pos_encoding = torch.zeros(max_len, dim_model)\n",
        "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
        "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
        "\n",
        "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
        "\n",
        "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
        "\n",
        "        # Saving buffer (same as parameter without gradients needed)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
        "        # Residual connection + pos encoding\n",
        "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
      ],
      "metadata": {
        "id": "X7dEc_6HRR3b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    # Constructor\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_tokens,\n",
        "        dim_model,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dropout_p,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # INFO\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.dim_model = dim_model\n",
        "\n",
        "        # LAYERS\n",
        "        self.positional_encoder = PositionalEncoding(\n",
        "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
        "        )\n",
        "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=dim_model,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dropout=dropout_p,\n",
        "        )\n",
        "        self.out = nn.Linear(dim_model, num_tokens)\n",
        "\n",
        "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
        "        # Src size must be (batch_size, src sequence length)\n",
        "        # Tgt size must be (batch_size, tgt sequence length)\n",
        "\n",
        "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
        "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
        "        src = self.positional_encoder(src)\n",
        "        tgt = self.positional_encoder(tgt)\n",
        "\n",
        "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
        "        # to obtain size (sequence length, batch_size, dim_model),\n",
        "        src = src.permute(1,0,2)\n",
        "        tgt = tgt.permute(1,0,2)\n",
        "\n",
        "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
        "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
        "        out = self.out(transformer_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def get_tgt_mask(self, size) -> torch.tensor:\n",
        "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
        "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
        "        mask = mask.float()\n",
        "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
        "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
        "\n",
        "        # EX for size=5:\n",
        "        # [[0., -inf, -inf, -inf, -inf],\n",
        "        #  [0.,   0., -inf, -inf, -inf],\n",
        "        #  [0.,   0.,   0., -inf, -inf],\n",
        "        #  [0.,   0.,   0.,   0., -inf],\n",
        "        #  [0.,   0.,   0.,   0.,   0.]]\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
        "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
        "        # [False, False, False, True, True, True]\n",
        "        return (matrix == pad_token)"
      ],
      "metadata": {
        "id": "fzOm3mDZQTP_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 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\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "JtAKnwd9iNIi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "6c1230c26bcd45d8b0951f29a5c59ba4",
            "c89c354142b1444cb0f33ae25a453f78",
            "a7083d5a971f4aa9855f2a857601dac6",
            "bd866ee8e8134b88bbd8cf061e4dbb85",
            "08cbbcf912e04d5ca6f410bf39fe85e4",
            "f719b87fa403412b9d68e689ab87184d",
            "080b63327afc4e4c94cf4242f2553d54",
            "8dd1e9e7af604c2c9fb2741f10aabf14",
            "a0968d84b0414c498f6498354035b407",
            "dc2d6cc7361a4b538ec82f6f35838a4d",
            "3eace533783e49b3a0d099b97a15c1ce",
            "bca1d44dc7244ca19f5b72dfcd2393d1",
            "0b11e2f5a00e4f50a2518e75acb8bedc",
            "c26b84ae85ff48959e31b17cbeb90b89",
            "b0c23f555cd74199b449843e53d4849c",
            "a4179f2758c64cdc840dd84e1f9481ba",
            "118817e126e74e53b1ae63e69b468693"
          ]
        },
        "outputId": "62045a39-bcde-4ed3-c58d-2c04b3b0b39f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c1230c26bcd45d8b0951f29a5c59ba4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNsZ7A1znxCl",
        "outputId": "7aaf43d1-969f-48e5-8d93-2b2defd04923"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset(\"mateiaassAI/MEID\", split=['train[:5%]', 'train[5%:7%]', 'train[7%:10%]'])\n",
        "dataset = load_dataset(\"mateiaassAI/MEID\", split=['train[:50%]', 'train[50%:55%]', 'train[55%:70%]'])\n",
        "train_dataset = dataset[0]\n",
        "test_dataset = dataset[1]\n",
        "valid_dataset = dataset[2]"
      ],
      "metadata": {
        "id": "fJ0hb3wki33I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736f1300-0dd2-4370-f5a7-5aa36449b916"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import pandas as pd\n",
        "\n",
        "dataset_tok = load_dataset(\"mateiaassAI/MEID\")\n",
        "dataset_tok = dataset_tok[\"train\"]\n",
        "dataset_tok = dataset_tok['right']\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
        "tokenizer.train_from_iterator(dataset_tok, vocab_size=100000, min_frequency=6, show_progress=True,special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n",
        "\n",
        "src_pad_idx = tokenizer.token_to_id('<pad>')\n",
        "trg_pad_idx = src_pad_idx\n",
        "trg_sos_idx = tokenizer.token_to_id('<s>')\n",
        "trg_eos_idx = tokenizer.token_to_id('</s>')\n",
        "\n",
        "enc_voc_size = tokenizer.get_vocab_size()\n",
        "dec_voc_size = enc_voc_size\n",
        "\n",
        "print(trg_sos_idx, trg_eos_idx, src_pad_idx, enc_voc_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkb6axTWkYsr",
        "outputId": "c77d10b2-6373-48f5-8249-7cb84947d95d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2 1 51374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_data(dataframe, tokenizer):\n",
        "    data = []\n",
        "    for example in dataframe:\n",
        "      input_text = example[\"wrong\"]\n",
        "      target_text = example[\"right\"]\n",
        "\n",
        "      input_ids = np.array(tokenizer.encode(input_text).ids)\n",
        "      target_ids = np.array(tokenizer.encode(target_text).ids)\n",
        "\n",
        "      SOS_token = np.array([trg_sos_idx])\n",
        "      EOS_token = np.array([trg_eos_idx])\n",
        "      X = np.concatenate((SOS_token, input_ids, EOS_token))\n",
        "      y = np.concatenate((SOS_token, target_ids, EOS_token))\n",
        "\n",
        "      data.append([X.tolist(), y.tolist()])\n",
        "\n",
        "    np.random.shuffle(data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "Za__KeXoSt0A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = generate_data(train_dataset, tokenizer)\n",
        "val_data = generate_data(valid_dataset, tokenizer)"
      ],
      "metadata": {
        "id": "R2xYKC9gSvwO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(train_data)\n",
        "\n",
        "# Check for NaN values\n",
        "nan_values = df.isnull().sum().sum()\n",
        "\n",
        "\n",
        "if nan_values == 0:\n",
        "    print(\"No NaN values found in the dataset.\")\n",
        "else:\n",
        "    print(f\"Total NaN values found: {nan_values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUcHmzPrwYjv",
        "outputId": "90555bc8-f405-4613-fa6a-2c5c71d1c7fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No NaN values found in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_numeric(val):\n",
        "    try:\n",
        "        pd.to_numeric(val)\n",
        "        return True\n",
        "    except (ValueError, TypeError):\n",
        "        return False\n",
        "\n",
        "# Find non-numeric elements in each column\n",
        "non_numeric_elements = {}\n",
        "for col in df.columns:\n",
        "    non_numeric_elements[col] = df[~df[col].apply(is_numeric)][col]\n",
        "\n",
        "# Print non-numeric elements\n",
        "for col, values in non_numeric_elements.items():\n",
        "    if not values.empty:\n",
        "        print(f\"Non-numeric elements in column '{col}':\")\n",
        "        print(values)"
      ],
      "metadata": {
        "id": "PJkz-6aH6SmX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
        "    batches = []\n",
        "    for idx in range(0, len(data), batch_size):\n",
        "        # We make sure we dont get the last bit if its not batch_size size\n",
        "        if idx + batch_size < len(data):\n",
        "            # Here you would need to get the max length of the batch,\n",
        "            # and normalize the length with the PAD token.\n",
        "            if padding:\n",
        "                max_batch_length = 0\n",
        "\n",
        "                # Get longest sentence in batch\n",
        "                for seq in data[idx : idx + batch_size]:\n",
        "                    if len(seq[0]) > max_batch_length:\n",
        "                        max_batch_length = len(seq[0])\n",
        "                    if len(seq[1]) > max_batch_length:\n",
        "                        max_batch_length = len(seq[1])\n",
        "\n",
        "                # Append X padding tokens until it reaches the max length\n",
        "                for seq_idx in range(batch_size):\n",
        "                    remaining_length = max_batch_length - len(data[idx + seq_idx][0])\n",
        "                    data[idx + seq_idx][0] += [padding_token] * remaining_length\n",
        "                    # padding_array = np.array([padding_token] * remaining_length)\n",
        "                    # data[idx + seq_idx][0] = np.concatenate((data[idx + seq_idx][0], padding_array))\n",
        "\n",
        "                    remaining_length = max_batch_length - len(data[idx + seq_idx][1])\n",
        "                    # padding_array = np.array([padding_token] * remaining_length)\n",
        "                    data[idx + seq_idx][1] += [padding_token] * remaining_length\n",
        "                    # data[idx + seq_idx][1] = np.concatenate((data[idx + seq_idx][1], padding_array))\n",
        "\n",
        "            # batches.append(data[idx : idx + batch_size])\n",
        "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
        "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
        "\n",
        "    return batches"
      ],
      "metadata": {
        "id": "K91y1xqSQ3pp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = batchify_data(train_data, batch_size=2, padding = True, padding_token = src_pad_idx)\n",
        "val_dataloader = batchify_data(val_data, batch_size=2, padding = True, padding_token = src_pad_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0Q26x5to8Tk",
        "outputId": "1227e436-58a5-4185-de46-71317bc0e73a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50326 batches of size 2\n",
            "15098 batches of size 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = Transformer(\n",
        "    num_tokens=enc_voc_size, dim_model=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dropout_p=0.1\n",
        ").to(device)\n",
        "# opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "# loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Wxm1IHVxS8EL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ec6aca-143f-4704-b548-ac19f0b2747b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.kaiming_uniform_(m.weight.data)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "model.apply(initialize_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1o_Y8uJoPK3",
        "outputId": "aca8c258-b396-431d-f459-d4d55d74c54b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 96,798,894 trainable parameters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (positional_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (embedding): Embedding(51374, 512)\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=512, out_features=51374, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "init_lr = 1e-5\n",
        "factor = 0.9\n",
        "adam_eps = 5e-9\n",
        "patience = 10\n",
        "# warmup = 100\n",
        "clip = 1.0\n",
        "weight_decay = 5e-4\n",
        "\n",
        "# optimizer = Adam(params=model.parameters(),\n",
        "#                  lr=init_lr,\n",
        "#                  weight_decay=weight_decay,\n",
        "#                  eps=adam_eps)\n",
        "\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "#                                                  verbose=True,\n",
        "#                                                  factor=factor,\n",
        "#                                                  patience=patience)\n",
        "\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=init_lr, eps=1e-9)\n",
        "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "scheduler = optim.lr_scheduler.CyclicLR(optimizer,base_lr=init_lr,max_lr=0.001,mode='triangular2', cycle_momentum=False)\n"
      ],
      "metadata": {
        "id": "_dz-bYFjoRku"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_loop(model, opt, loss_fn, dataloader):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        # batch = np.array(batch)\n",
        "        # print(batch[0])\n",
        "        X, y = batch[:, 0], batch[:, 1]\n",
        "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
        "\n",
        "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "        y_input = y[:,:-1]\n",
        "        y_expected = y[:,1:]\n",
        "\n",
        "        # Get mask to mask out the next words\n",
        "        sequence_length = y_input.size(1)\n",
        "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "        # Standard training except we pass in y_input and tgt_mask\n",
        "        pred = model(X, y_input, tgt_mask)\n",
        "\n",
        "        # Permute pred to have batch size first again\n",
        "        pred = pred.permute(1, 2, 0)\n",
        "        # print(pred[0,:,:], y_expected[0])\n",
        "        # break\n",
        "        loss = loss_fn(pred, y_expected)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.detach().item()\n",
        "        scheduler.step(loss.detach().item())\n",
        "        if torch.isnan(loss).any():\n",
        "              print(\"NaN loss detected! Debugging information:\")\n",
        "              print(\"Input Data (X):\", X)\n",
        "              print(\"Expected Output (y_expected):\", y_expected)\n",
        "              print(\"Expected Output (y_input):\", y_input)\n",
        "              print(\"Model Predictions (pred):\", pred)\n",
        "              # print(\"Gradients:\", [param.grad for param in model.parameters() if param.grad is not None])\n",
        "\n",
        "        # Print learning rate\n",
        "        learning_rate = opt.param_groups[0]['lr']\n",
        "        if (step + 1) % 10 == 0:  # Adjust frequency as needed\n",
        "            elapsed_time = time.time() - start_time\n",
        "            avg_loss = total_loss / step\n",
        "            print(f\"Step [{step+1}/{len(dataloader)}], Loss: {loss.item():.4f}, Average Loss: {avg_loss:.4f}, Time: {elapsed_time:.2f} seconds, Learning Rate: {learning_rate}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "RMdU626HTCez"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def validation_loop(model, loss_fn, dataloader):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            X, y = batch[:, 0], batch[:, 1]\n",
        "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
        "\n",
        "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "            y_input = y[:,:-1]\n",
        "            y_expected = y[:,1:]\n",
        "\n",
        "            # Get mask to mask out the next words\n",
        "            sequence_length = y_input.size(1)\n",
        "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "            # Standard training except we pass in y_input and src_mask\n",
        "            pred = model(X, y_input, tgt_mask)\n",
        "\n",
        "            # Permute pred to have batch size first again\n",
        "            pred = pred.permute(1, 2, 0)\n",
        "            loss = loss_fn(pred, y_expected)\n",
        "            total_loss += loss.detach().item()\n",
        "\n",
        "            if torch.isnan(loss).any():\n",
        "              print(\"NaN loss detected! Debugging information:\")\n",
        "              print(\"Input Data (X):\", X)\n",
        "              print(\"Expected Output (y_expected):\", y_expected)\n",
        "              print(\"Expected Output (y_input):\", y_input)\n",
        "              print(\"Model Predictions (pred):\", pred)\n",
        "              print(\"Gradients:\", [param.grad for param in model.parameters() if param.grad is not None])\n",
        "\n",
        "              # Additional information\n",
        "              print(\"Model Parameters:\")\n",
        "              for name, param in model.named_parameters():\n",
        "                  print(name, param.data)\n",
        "\n",
        "              print(\"Optimizer State:\")\n",
        "              for param_group in opt.param_groups:\n",
        "                  print(\"Learning Rate:\", param_group['lr'])\n",
        "\n",
        "              # Add more debugging information as needed\n",
        "\n",
        "              print(\"Loss Function Parameters:\")\n",
        "              for name, param in loss_fn.named_parameters():\n",
        "                  print(name, param.data)\n",
        "              break\n",
        "\n",
        "            if (step + 1) % 10 == 0:  # Adjust frequency as needed\n",
        "              elapsed_time = time.time() - start_time\n",
        "              print(f\"Step [{step+1}/{len(dataloader)}], Loss: {loss.item():.4f}, Time: {elapsed_time:.2f} seconds\")\n",
        "              start_time = time.time()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "TZQBz5OiT55I"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "\n",
        "    # Used for plotting later on\n",
        "    train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "    print(\"Training and validating model\")\n",
        "    for epoch in range(epochs):\n",
        "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
        "\n",
        "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
        "        # break\n",
        "        train_loss_list += [train_loss]\n",
        "\n",
        "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
        "        validation_loss_list += [validation_loss]\n",
        "\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
        "        print()\n",
        "\n",
        "    return train_loss_list, validation_loss_list"
      ],
      "metadata": {
        "id": "B-IKivf4T8yQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, validation_loss_list = fit(model, optimizer, loss_fn, train_dataloader, val_dataloader, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfyzQgF0T-6q",
        "outputId": "926f58de-c853-406b-f2c8-a2a6b4dd0575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and validating model\n",
            "------------------------- Epoch 1 -------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [10/50326], Loss: 5.7039, Average Loss: 6.8859, Time: 1.02 seconds, Learning Rate: 1.282343373537081e-05\n",
            "Step [20/50326], Loss: 8.4398, Average Loss: 6.9115, Time: 0.83 seconds, Learning Rate: 1.41776855516434e-05\n",
            "Step [30/50326], Loss: 8.9322, Average Loss: 7.2855, Time: 1.01 seconds, Learning Rate: 1.4421455264091493e-05\n",
            "Step [40/50326], Loss: 8.6742, Average Loss: 7.6442, Time: 0.95 seconds, Learning Rate: 1.429374224662787e-05\n",
            "Step [50/50326], Loss: 9.0756, Average Loss: 7.9004, Time: 1.10 seconds, Learning Rate: 1.4492411923408316e-05\n",
            "Step [60/50326], Loss: 7.9029, Average Loss: 7.9481, Time: 0.99 seconds, Learning Rate: 1.3911955199241467e-05\n",
            "Step [70/50326], Loss: 7.4834, Average Loss: 8.0273, Time: 1.18 seconds, Learning Rate: 1.3704291195869383e-05\n",
            "Step [80/50326], Loss: 8.5425, Average Loss: 8.1204, Time: 1.13 seconds, Learning Rate: 1.4228538689613558e-05\n",
            "Step [90/50326], Loss: 8.6653, Average Loss: 8.1938, Time: 1.15 seconds, Learning Rate: 1.4289326987266545e-05\n",
            "Step [100/50326], Loss: 8.2712, Average Loss: 8.2195, Time: 1.13 seconds, Learning Rate: 1.4094266276359605e-05\n",
            "Step [110/50326], Loss: 7.5747, Average Loss: 8.2521, Time: 1.29 seconds, Learning Rate: 1.3749473135471303e-05\n",
            "Step [120/50326], Loss: 9.8111, Average Loss: 8.2777, Time: 1.00 seconds, Learning Rate: 1.4856479868888825e-05\n",
            "Step [130/50326], Loss: 8.2637, Average Loss: 8.2952, Time: 1.07 seconds, Learning Rate: 1.4090541653633322e-05\n",
            "Step [140/50326], Loss: 8.6930, Average Loss: 8.2970, Time: 1.27 seconds, Learning Rate: 1.4303029727935686e-05\n",
            "Step [150/50326], Loss: 7.4253, Average Loss: 8.2865, Time: 1.15 seconds, Learning Rate: 1.3675545039176727e-05\n",
            "Step [160/50326], Loss: 7.3436, Average Loss: 8.2826, Time: 1.04 seconds, Learning Rate: 1.3635075762272089e-05\n",
            "Step [170/50326], Loss: 7.9621, Average Loss: 8.2846, Time: 1.29 seconds, Learning Rate: 1.3941225116252998e-05\n",
            "Step [180/50326], Loss: 7.9601, Average Loss: 8.2948, Time: 1.32 seconds, Learning Rate: 1.3940226690769189e-05\n",
            "Step [190/50326], Loss: 9.1805, Average Loss: 8.2846, Time: 1.15 seconds, Learning Rate: 1.454435978889471e-05\n",
            "Step [200/50326], Loss: 7.6951, Average Loss: 8.2951, Time: 1.30 seconds, Learning Rate: 1.380908055305488e-05\n",
            "Step [210/50326], Loss: 9.0886, Average Loss: 8.2903, Time: 1.24 seconds, Learning Rate: 1.4498875017166216e-05\n",
            "Step [220/50326], Loss: 6.8323, Average Loss: 8.2859, Time: 1.01 seconds, Learning Rate: 1.3381971125602822e-05\n",
            "Step [230/50326], Loss: 8.3456, Average Loss: 8.2775, Time: 1.14 seconds, Learning Rate: 1.413105978965747e-05\n",
            "Step [240/50326], Loss: 8.5832, Average Loss: 8.2832, Time: 1.09 seconds, Learning Rate: 1.4248679504394324e-05\n",
            "Step [250/50326], Loss: 7.9056, Average Loss: 8.2813, Time: 1.03 seconds, Learning Rate: 1.3913281004428823e-05\n",
            "Step [260/50326], Loss: 7.8796, Average Loss: 8.2705, Time: 1.12 seconds, Learning Rate: 1.390038266897204e-05\n",
            "Step [270/50326], Loss: 7.8275, Average Loss: 8.2746, Time: 1.01 seconds, Learning Rate: 1.3874614794254346e-05\n",
            "Step [280/50326], Loss: 7.2739, Average Loss: 8.2579, Time: 0.96 seconds, Learning Rate: 1.360060081481946e-05\n",
            "Step [290/50326], Loss: 8.8401, Average Loss: 8.2479, Time: 0.95 seconds, Learning Rate: 1.4375860028266848e-05\n",
            "Step [300/50326], Loss: 8.5477, Average Loss: 8.2354, Time: 1.02 seconds, Learning Rate: 1.4231130347251774e-05\n",
            "Step [310/50326], Loss: 8.4047, Average Loss: 8.2382, Time: 1.16 seconds, Learning Rate: 1.4160340328216683e-05\n",
            "Step [320/50326], Loss: 7.6078, Average Loss: 8.2250, Time: 1.25 seconds, Learning Rate: 1.3765855574607727e-05\n",
            "Step [330/50326], Loss: 7.0327, Average Loss: 8.2102, Time: 1.19 seconds, Learning Rate: 1.3481182990074198e-05\n",
            "Step [340/50326], Loss: 8.5989, Average Loss: 8.2039, Time: 0.93 seconds, Learning Rate: 1.4256435122489943e-05\n",
            "Step [350/50326], Loss: 8.9651, Average Loss: 8.1897, Time: 1.26 seconds, Learning Rate: 1.4437710480690172e-05\n",
            "Step [360/50326], Loss: 7.7147, Average Loss: 8.1800, Time: 1.05 seconds, Learning Rate: 1.3818759143352334e-05\n",
            "Step [370/50326], Loss: 7.9733, Average Loss: 8.1823, Time: 0.89 seconds, Learning Rate: 1.394678750276567e-05\n",
            "Step [380/50326], Loss: 8.4205, Average Loss: 8.1635, Time: 0.95 seconds, Learning Rate: 1.4168131351470948e-05\n",
            "Step [390/50326], Loss: 8.5357, Average Loss: 8.1575, Time: 1.12 seconds, Learning Rate: 1.42251888895033e-05\n",
            "Step [400/50326], Loss: 7.6667, Average Loss: 8.1469, Time: 1.59 seconds, Learning Rate: 1.3795032258033566e-05\n",
            "Step [410/50326], Loss: 7.9144, Average Loss: 8.1419, Time: 0.78 seconds, Learning Rate: 1.3917636311054089e-05\n",
            "Step [420/50326], Loss: 7.9312, Average Loss: 8.1333, Time: 1.17 seconds, Learning Rate: 1.3925950386524375e-05\n",
            "Step [430/50326], Loss: 8.5968, Average Loss: 8.1344, Time: 1.01 seconds, Learning Rate: 1.4255408844947995e-05\n",
            "Step [440/50326], Loss: 8.0474, Average Loss: 8.1259, Time: 1.26 seconds, Learning Rate: 1.3983460874557432e-05\n",
            "Step [450/50326], Loss: 8.3057, Average Loss: 8.1228, Time: 1.32 seconds, Learning Rate: 1.4111336755752757e-05\n",
            "Step [460/50326], Loss: 8.7180, Average Loss: 8.1209, Time: 1.21 seconds, Learning Rate: 1.4315420117378338e-05\n",
            "Step [470/50326], Loss: 7.7354, Average Loss: 8.1167, Time: 1.09 seconds, Learning Rate: 1.3829015309810805e-05\n",
            "Step [480/50326], Loss: 8.0410, Average Loss: 8.1181, Time: 0.93 seconds, Learning Rate: 1.3980281019210957e-05\n",
            "Step [490/50326], Loss: 8.0726, Average Loss: 8.1200, Time: 0.94 seconds, Learning Rate: 1.3995925378799422e-05\n",
            "Step [500/50326], Loss: 8.3527, Average Loss: 8.1125, Time: 1.18 seconds, Learning Rate: 1.4134591808318895e-05\n",
            "Step [510/50326], Loss: 8.1579, Average Loss: 8.1019, Time: 1.05 seconds, Learning Rate: 1.4038137769699044e-05\n",
            "Step [520/50326], Loss: 7.8527, Average Loss: 8.1050, Time: 1.04 seconds, Learning Rate: 1.3887080242633624e-05\n",
            "Step [530/50326], Loss: 8.3119, Average Loss: 8.1027, Time: 0.97 seconds, Learning Rate: 1.4114369325637846e-05\n",
            "Step [540/50326], Loss: 8.3020, Average Loss: 8.0986, Time: 1.27 seconds, Learning Rate: 1.410951079368577e-05\n",
            "Step [550/50326], Loss: 8.1810, Average Loss: 8.0885, Time: 0.96 seconds, Learning Rate: 1.404960432052609e-05\n",
            "Step [560/50326], Loss: 7.6911, Average Loss: 8.0797, Time: 1.11 seconds, Learning Rate: 1.3807106597423649e-05\n",
            "Step [570/50326], Loss: 8.1102, Average Loss: 8.0767, Time: 1.19 seconds, Learning Rate: 1.4014539523124614e-05\n",
            "Step [580/50326], Loss: 8.5196, Average Loss: 8.0735, Time: 0.99 seconds, Learning Rate: 1.421717882633219e-05\n",
            "Step [590/50326], Loss: 7.3585, Average Loss: 8.0691, Time: 0.96 seconds, Learning Rate: 1.364245018482202e-05\n",
            "Step [600/50326], Loss: 7.4313, Average Loss: 8.0619, Time: 0.99 seconds, Learning Rate: 1.3678515059947888e-05\n",
            "Step [610/50326], Loss: 7.6038, Average Loss: 8.0544, Time: 1.05 seconds, Learning Rate: 1.3763858487606116e-05\n",
            "Step [620/50326], Loss: 8.3832, Average Loss: 8.0467, Time: 1.19 seconds, Learning Rate: 1.4149696121215755e-05\n",
            "Step [630/50326], Loss: 5.9120, Average Loss: 8.0371, Time: 1.06 seconds, Learning Rate: 1.2926418845653545e-05\n",
            "Step [640/50326], Loss: 7.8339, Average Loss: 8.0297, Time: 1.13 seconds, Learning Rate: 1.3877762548923344e-05\n",
            "Step [650/50326], Loss: 7.0989, Average Loss: 8.0247, Time: 0.95 seconds, Learning Rate: 1.351396344661692e-05\n",
            "Step [660/50326], Loss: 7.9081, Average Loss: 8.0224, Time: 0.80 seconds, Learning Rate: 1.3914498469829475e-05\n",
            "Step [670/50326], Loss: 6.7744, Average Loss: 8.0181, Time: 1.05 seconds, Learning Rate: 1.33533163142204e-05\n",
            "Step [680/50326], Loss: 7.4132, Average Loss: 8.0158, Time: 1.05 seconds, Learning Rate: 1.366951813697794e-05\n",
            "Step [690/50326], Loss: 7.5922, Average Loss: 8.0110, Time: 1.09 seconds, Learning Rate: 1.3758161797523619e-05\n",
            "Step [700/50326], Loss: 7.7688, Average Loss: 8.0060, Time: 1.20 seconds, Learning Rate: 1.3845560376644058e-05\n",
            "Step [710/50326], Loss: 7.6346, Average Loss: 8.0012, Time: 0.97 seconds, Learning Rate: 1.377912495613094e-05\n",
            "Step [720/50326], Loss: 6.9989, Average Loss: 7.9972, Time: 0.88 seconds, Learning Rate: 1.3464436113834558e-05\n",
            "Step [730/50326], Loss: 7.8598, Average Loss: 7.9952, Time: 0.89 seconds, Learning Rate: 1.3890595266819022e-05\n",
            "Step [740/50326], Loss: 7.0086, Average Loss: 7.9891, Time: 0.94 seconds, Learning Rate: 1.3469236581325595e-05\n",
            "Step [750/50326], Loss: 7.8367, Average Loss: 7.9840, Time: 0.96 seconds, Learning Rate: 1.3879159400463302e-05\n",
            "Step [760/50326], Loss: 6.9651, Average Loss: 7.9767, Time: 1.18 seconds, Learning Rate: 1.3447708356380393e-05\n",
            "Step [770/50326], Loss: 7.6656, Average Loss: 7.9719, Time: 1.14 seconds, Learning Rate: 1.3794476161003222e-05\n",
            "Step [780/50326], Loss: 7.4919, Average Loss: 7.9653, Time: 1.26 seconds, Learning Rate: 1.3708477029800613e-05\n",
            "Step [790/50326], Loss: 7.8282, Average Loss: 7.9632, Time: 0.98 seconds, Learning Rate: 1.3874978051185401e-05\n",
            "Step [800/50326], Loss: 7.6176, Average Loss: 7.9600, Time: 0.98 seconds, Learning Rate: 1.3770716702938115e-05\n",
            "Step [810/50326], Loss: 7.9563, Average Loss: 7.9573, Time: 0.97 seconds, Learning Rate: 1.3938392467498839e-05\n",
            "Step [820/50326], Loss: 7.4896, Average Loss: 7.9521, Time: 1.25 seconds, Learning Rate: 1.3707336747646474e-05\n",
            "Step [830/50326], Loss: 6.9240, Average Loss: 7.9483, Time: 1.12 seconds, Learning Rate: 1.3427388627529187e-05\n",
            "Step [840/50326], Loss: 6.8244, Average Loss: 7.9432, Time: 1.15 seconds, Learning Rate: 1.3378097565174128e-05\n",
            "Step [850/50326], Loss: 7.8729, Average Loss: 7.9406, Time: 0.94 seconds, Learning Rate: 1.3897091405391688e-05\n",
            "Step [860/50326], Loss: 6.2316, Average Loss: 7.9368, Time: 0.87 seconds, Learning Rate: 1.3084630103111468e-05\n",
            "Step [870/50326], Loss: 6.8505, Average Loss: 7.9341, Time: 0.80 seconds, Learning Rate: 1.3391013603210232e-05\n",
            "Step [880/50326], Loss: 7.9972, Average Loss: 7.9296, Time: 0.89 seconds, Learning Rate: 1.3958631944656354e-05\n",
            "Step [890/50326], Loss: 6.9808, Average Loss: 7.9248, Time: 1.13 seconds, Learning Rate: 1.3455496783256346e-05\n",
            "Step [900/50326], Loss: 8.7365, Average Loss: 7.9210, Time: 0.89 seconds, Learning Rate: 1.432455606460587e-05\n",
            "Step [910/50326], Loss: 6.8838, Average Loss: 7.9145, Time: 1.14 seconds, Learning Rate: 1.3407484555244587e-05\n",
            "Step [920/50326], Loss: 7.2612, Average Loss: 7.9102, Time: 1.16 seconds, Learning Rate: 1.3594289963245579e-05\n",
            "Step [930/50326], Loss: 7.4421, Average Loss: 7.9083, Time: 0.86 seconds, Learning Rate: 1.3683841884136112e-05\n",
            "Step [940/50326], Loss: 9.4442, Average Loss: 7.9083, Time: 1.28 seconds, Learning Rate: 1.4674873118400585e-05\n",
            "Step [950/50326], Loss: 7.6753, Average Loss: 7.9019, Time: 0.93 seconds, Learning Rate: 1.3799269311428187e-05\n",
            "Step [960/50326], Loss: 8.0572, Average Loss: 7.8987, Time: 1.13 seconds, Learning Rate: 1.3988312325477429e-05\n",
            "Step [970/50326], Loss: 6.5020, Average Loss: 7.8919, Time: 0.96 seconds, Learning Rate: 1.3218479070663439e-05\n",
            "Step [980/50326], Loss: 7.2080, Average Loss: 7.8853, Time: 1.23 seconds, Learning Rate: 1.3567942860126282e-05\n",
            "Step [990/50326], Loss: 7.2027, Average Loss: 7.8807, Time: 1.06 seconds, Learning Rate: 1.3565356159210258e-05\n",
            "Step [1000/50326], Loss: 7.6461, Average Loss: 7.8800, Time: 0.95 seconds, Learning Rate: 1.3784824006557319e-05\n",
            "Step [1010/50326], Loss: 7.7944, Average Loss: 7.8773, Time: 0.98 seconds, Learning Rate: 1.3858207571506517e-05\n",
            "Step [1020/50326], Loss: 8.8587, Average Loss: 7.8779, Time: 0.88 seconds, Learning Rate: 1.4385050735473813e-05\n",
            "Step [1030/50326], Loss: 6.9584, Average Loss: 7.8733, Time: 1.07 seconds, Learning Rate: 1.34443892407419e-05\n",
            "Step [1040/50326], Loss: 5.8425, Average Loss: 7.8680, Time: 0.95 seconds, Learning Rate: 1.2892029578685663e-05\n",
            "Step [1050/50326], Loss: 6.5678, Average Loss: 7.8648, Time: 1.01 seconds, Learning Rate: 1.3251061966419111e-05\n",
            "Step [1060/50326], Loss: 7.0322, Average Loss: 7.8616, Time: 1.02 seconds, Learning Rate: 1.348094152688983e-05\n",
            "Step [1070/50326], Loss: 7.8223, Average Loss: 7.8592, Time: 1.23 seconds, Learning Rate: 1.3872033286094508e-05\n",
            "Step [1080/50326], Loss: 7.5586, Average Loss: 7.8550, Time: 1.09 seconds, Learning Rate: 1.3741501545906016e-05\n",
            "Step [1090/50326], Loss: 8.8325, Average Loss: 7.8558, Time: 1.01 seconds, Learning Rate: 1.4372087254524351e-05\n",
            "Step [1100/50326], Loss: 7.1559, Average Loss: 7.8486, Time: 1.02 seconds, Learning Rate: 1.354215917110428e-05\n",
            "Step [1110/50326], Loss: 7.8456, Average Loss: 7.8460, Time: 0.96 seconds, Learning Rate: 1.3883563094139217e-05\n",
            "Step [1120/50326], Loss: 7.8013, Average Loss: 7.8425, Time: 1.19 seconds, Learning Rate: 1.3861623225212312e-05\n",
            "Step [1130/50326], Loss: 7.3483, Average Loss: 7.8394, Time: 0.90 seconds, Learning Rate: 1.3637418403625433e-05\n",
            "Step [1140/50326], Loss: 8.3429, Average Loss: 7.8353, Time: 0.95 seconds, Learning Rate: 1.4129753575324893e-05\n",
            "Step [1150/50326], Loss: 7.2197, Average Loss: 7.8320, Time: 1.08 seconds, Learning Rate: 1.3573749778270937e-05\n",
            "Step [1160/50326], Loss: 6.5392, Average Loss: 7.8298, Time: 1.24 seconds, Learning Rate: 1.3236915953159218e-05\n",
            "Step [1170/50326], Loss: 7.3591, Average Loss: 7.8248, Time: 1.17 seconds, Learning Rate: 1.364275584936134e-05\n",
            "Step [1180/50326], Loss: 7.5303, Average Loss: 7.8209, Time: 1.07 seconds, Learning Rate: 1.3727519812583974e-05\n",
            "Step [1190/50326], Loss: 7.3753, Average Loss: 7.8200, Time: 1.17 seconds, Learning Rate: 1.3650773937702257e-05\n",
            "Step [1200/50326], Loss: 6.9641, Average Loss: 7.8193, Time: 1.07 seconds, Learning Rate: 1.344722779035554e-05\n",
            "Step [1210/50326], Loss: 7.6289, Average Loss: 7.8184, Time: 1.41 seconds, Learning Rate: 1.3776292543411213e-05\n",
            "Step [1220/50326], Loss: 8.5684, Average Loss: 7.8177, Time: 1.19 seconds, Learning Rate: 1.424134402751908e-05\n",
            "Step [1230/50326], Loss: 6.9612, Average Loss: 7.8182, Time: 1.17 seconds, Learning Rate: 1.3445800962447994e-05\n",
            "Step [1240/50326], Loss: 7.7276, Average Loss: 7.8161, Time: 1.03 seconds, Learning Rate: 1.3825156383514257e-05\n",
            "Step [1250/50326], Loss: 7.7878, Average Loss: 7.8140, Time: 0.96 seconds, Learning Rate: 1.3854970595836731e-05\n",
            "Step [1260/50326], Loss: 7.4503, Average Loss: 7.8111, Time: 1.35 seconds, Learning Rate: 1.368787618398662e-05\n",
            "Step [1270/50326], Loss: 7.9103, Average Loss: 7.8077, Time: 1.05 seconds, Learning Rate: 1.3915583755970193e-05\n",
            "Step [1280/50326], Loss: 6.7685, Average Loss: 7.8033, Time: 0.96 seconds, Learning Rate: 1.3350416395664113e-05\n",
            "Step [1290/50326], Loss: 7.7126, Average Loss: 7.8016, Time: 0.87 seconds, Learning Rate: 1.381771610736835e-05\n",
            "Step [1300/50326], Loss: 6.2204, Average Loss: 7.8007, Time: 0.89 seconds, Learning Rate: 1.307908305883424e-05\n",
            "Step [1310/50326], Loss: 7.6969, Average Loss: 7.7991, Time: 1.28 seconds, Learning Rate: 1.3809954116344386e-05\n",
            "Step [1320/50326], Loss: 6.7206, Average Loss: 7.7964, Time: 0.98 seconds, Learning Rate: 1.332672019481667e-05\n",
            "Step [1330/50326], Loss: 7.5908, Average Loss: 7.7940, Time: 1.07 seconds, Learning Rate: 1.3757421829700643e-05\n",
            "Step [1340/50326], Loss: 8.6320, Average Loss: 7.7939, Time: 1.40 seconds, Learning Rate: 1.4272856507301493e-05\n",
            "Step [1350/50326], Loss: 7.2427, Average Loss: 7.7928, Time: 1.03 seconds, Learning Rate: 1.3585151655674165e-05\n",
            "Step [1360/50326], Loss: 7.3134, Average Loss: 7.7921, Time: 1.13 seconds, Learning Rate: 1.3620121095180499e-05\n",
            "Step [1370/50326], Loss: 8.7403, Average Loss: 7.7913, Time: 0.97 seconds, Learning Rate: 1.4326451420783998e-05\n",
            "Step [1380/50326], Loss: 7.7099, Average Loss: 7.7918, Time: 0.95 seconds, Learning Rate: 1.3816393606662605e-05\n",
            "Step [1390/50326], Loss: 6.2396, Average Loss: 7.7897, Time: 1.23 seconds, Learning Rate: 1.3088614835739168e-05\n",
            "Step [1400/50326], Loss: 6.5764, Average Loss: 7.7862, Time: 1.17 seconds, Learning Rate: 1.3255334661006809e-05\n",
            "Step [1410/50326], Loss: 7.9809, Average Loss: 7.7861, Time: 1.02 seconds, Learning Rate: 1.3950534076690611e-05\n",
            "Step [1420/50326], Loss: 6.8759, Average Loss: 7.7834, Time: 1.14 seconds, Learning Rate: 1.3403550806045407e-05\n",
            "Step [1430/50326], Loss: 7.6854, Average Loss: 7.7816, Time: 0.95 seconds, Learning Rate: 1.3804294247627126e-05\n",
            "Step [1440/50326], Loss: 7.1649, Average Loss: 7.7782, Time: 0.97 seconds, Learning Rate: 1.3546627066135588e-05\n",
            "Step [1450/50326], Loss: 7.1585, Average Loss: 7.7738, Time: 1.11 seconds, Learning Rate: 1.3543479075431715e-05\n",
            "Step [1460/50326], Loss: 7.5510, Average Loss: 7.7720, Time: 1.31 seconds, Learning Rate: 1.373776960611322e-05\n",
            "Step [1470/50326], Loss: 7.7210, Average Loss: 7.7683, Time: 1.12 seconds, Learning Rate: 1.3821884238719817e-05\n",
            "Step [1480/50326], Loss: 7.7913, Average Loss: 7.7660, Time: 1.25 seconds, Learning Rate: 1.3856717958450174e-05\n",
            "Step [1490/50326], Loss: 7.1317, Average Loss: 7.7656, Time: 1.08 seconds, Learning Rate: 1.353017924547205e-05\n",
            "Step [1500/50326], Loss: 6.6108, Average Loss: 7.7624, Time: 0.90 seconds, Learning Rate: 1.3272348256111082e-05\n",
            "Step [1510/50326], Loss: 7.5048, Average Loss: 7.7602, Time: 1.15 seconds, Learning Rate: 1.3714886779785234e-05\n",
            "Step [1520/50326], Loss: 8.6829, Average Loss: 7.7600, Time: 1.28 seconds, Learning Rate: 1.4298033823967049e-05\n",
            "Step [1530/50326], Loss: 7.5490, Average Loss: 7.7603, Time: 1.44 seconds, Learning Rate: 1.3736774721145451e-05\n",
            "Step [1540/50326], Loss: 7.1819, Average Loss: 7.7580, Time: 1.24 seconds, Learning Rate: 1.355505609035491e-05\n",
            "Step [1550/50326], Loss: 7.1292, Average Loss: 7.7552, Time: 1.32 seconds, Learning Rate: 1.3528967208862453e-05\n",
            "Step [1560/50326], Loss: 8.0280, Average Loss: 7.7531, Time: 1.19 seconds, Learning Rate: 1.397386938095088e-05\n",
            "Step [1570/50326], Loss: 8.0372, Average Loss: 7.7526, Time: 0.97 seconds, Learning Rate: 1.3978390855789016e-05\n",
            "Step [1580/50326], Loss: 7.1392, Average Loss: 7.7526, Time: 1.13 seconds, Learning Rate: 1.353388380527513e-05\n",
            "Step [1590/50326], Loss: 6.8596, Average Loss: 7.7507, Time: 0.97 seconds, Learning Rate: 1.3395512182712422e-05\n",
            "Step [1600/50326], Loss: 7.3773, Average Loss: 7.7502, Time: 1.13 seconds, Learning Rate: 1.3651773071288922e-05\n",
            "Step [1610/50326], Loss: 8.0337, Average Loss: 7.7514, Time: 1.01 seconds, Learning Rate: 1.3976685271263003e-05\n",
            "Step [1620/50326], Loss: 7.1546, Average Loss: 7.7521, Time: 0.88 seconds, Learning Rate: 1.354155114650725e-05\n",
            "Step [1630/50326], Loss: 7.0527, Average Loss: 7.7494, Time: 1.15 seconds, Learning Rate: 1.3491111068725386e-05\n",
            "Step [1640/50326], Loss: 8.8721, Average Loss: 7.7470, Time: 0.97 seconds, Learning Rate: 1.4391665363311558e-05\n",
            "Step [1650/50326], Loss: 7.7724, Average Loss: 7.7467, Time: 0.87 seconds, Learning Rate: 1.3847315528392878e-05\n",
            "Step [1660/50326], Loss: 7.7565, Average Loss: 7.7465, Time: 1.56 seconds, Learning Rate: 1.3839486739635648e-05\n",
            "Step [1670/50326], Loss: 7.8389, Average Loss: 7.7439, Time: 1.06 seconds, Learning Rate: 1.3880241854190835e-05\n",
            "Step [1680/50326], Loss: 9.2522, Average Loss: 7.7440, Time: 1.20 seconds, Learning Rate: 1.457983339786514e-05\n",
            "Step [1690/50326], Loss: 7.4646, Average Loss: 7.7429, Time: 1.22 seconds, Learning Rate: 1.3694974918365697e-05\n",
            "Step [1700/50326], Loss: 7.4163, Average Loss: 7.7413, Time: 0.96 seconds, Learning Rate: 1.3671051652431601e-05\n",
            "Step [1710/50326], Loss: 5.8758, Average Loss: 7.7383, Time: 0.95 seconds, Learning Rate: 1.2908521065712085e-05\n",
            "Step [1720/50326], Loss: 7.3756, Average Loss: 7.7367, Time: 1.10 seconds, Learning Rate: 1.3650916266441451e-05\n",
            "Step [1730/50326], Loss: 8.6009, Average Loss: 7.7389, Time: 1.15 seconds, Learning Rate: 1.4257460455894604e-05\n",
            "Step [1740/50326], Loss: 7.6128, Average Loss: 7.7369, Time: 1.13 seconds, Learning Rate: 1.3768327798843575e-05\n",
            "Step [1750/50326], Loss: 6.9201, Average Loss: 7.7384, Time: 2.57 seconds, Learning Rate: 1.3425447952747152e-05\n",
            "Step [1760/50326], Loss: 7.4573, Average Loss: 7.7383, Time: 1.02 seconds, Learning Rate: 1.3691349902153013e-05\n",
            "Step [1770/50326], Loss: 7.6569, Average Loss: 7.7362, Time: 1.13 seconds, Learning Rate: 1.3790142097473318e-05\n",
            "Step [1780/50326], Loss: 7.6355, Average Loss: 7.7386, Time: 0.88 seconds, Learning Rate: 1.377958309888827e-05\n",
            "Step [1790/50326], Loss: 7.7695, Average Loss: 7.7366, Time: 1.17 seconds, Learning Rate: 1.3845878078937652e-05\n",
            "Step [1800/50326], Loss: 7.6053, Average Loss: 7.7361, Time: 0.98 seconds, Learning Rate: 1.3764642829894833e-05\n",
            "Step [1810/50326], Loss: 7.2565, Average Loss: 7.7377, Time: 1.04 seconds, Learning Rate: 1.3591970453262176e-05\n",
            "Step [1820/50326], Loss: 7.1477, Average Loss: 7.7376, Time: 0.82 seconds, Learning Rate: 1.3538101739883396e-05\n",
            "Step [1830/50326], Loss: 7.4375, Average Loss: 7.7349, Time: 1.10 seconds, Learning Rate: 1.3681571941375516e-05\n",
            "Step [1840/50326], Loss: 7.4881, Average Loss: 7.7351, Time: 0.87 seconds, Learning Rate: 1.37066085815429e-05\n",
            "Step [1850/50326], Loss: 7.0198, Average Loss: 7.7330, Time: 1.03 seconds, Learning Rate: 1.3474804632663756e-05\n",
            "Step [1860/50326], Loss: 7.5542, Average Loss: 7.7305, Time: 0.94 seconds, Learning Rate: 1.3739317283630164e-05\n",
            "Step [1870/50326], Loss: 7.5741, Average Loss: 7.7310, Time: 0.79 seconds, Learning Rate: 1.3749163458347518e-05\n",
            "Step [1880/50326], Loss: 8.0532, Average Loss: 7.7300, Time: 1.10 seconds, Learning Rate: 1.3986351351737758e-05\n",
            "Step [1890/50326], Loss: 7.6629, Average Loss: 7.7292, Time: 1.08 seconds, Learning Rate: 1.3793159797191828e-05\n",
            "Step [1900/50326], Loss: 7.0122, Average Loss: 7.7267, Time: 1.17 seconds, Learning Rate: 1.3471019585132554e-05\n",
            "Step [1910/50326], Loss: 6.8985, Average Loss: 7.7266, Time: 1.03 seconds, Learning Rate: 1.341473954439171e-05\n",
            "Step [1920/50326], Loss: 6.7044, Average Loss: 7.7252, Time: 1.24 seconds, Learning Rate: 1.3318697149753562e-05\n",
            "Step [1930/50326], Loss: 7.6464, Average Loss: 7.7228, Time: 0.99 seconds, Learning Rate: 1.3784954297542678e-05\n",
            "Step [1940/50326], Loss: 7.5070, Average Loss: 7.7211, Time: 0.99 seconds, Learning Rate: 1.3715983631610921e-05\n",
            "Step [1950/50326], Loss: 7.7216, Average Loss: 7.7184, Time: 0.91 seconds, Learning Rate: 1.3822179753780319e-05\n",
            "Step [1960/50326], Loss: 7.1376, Average Loss: 7.7161, Time: 0.85 seconds, Learning Rate: 1.3533108904361932e-05\n",
            "Step [1970/50326], Loss: 7.8661, Average Loss: 7.7144, Time: 1.28 seconds, Learning Rate: 1.3893735232353085e-05\n",
            "Step [1980/50326], Loss: 7.7015, Average Loss: 7.7124, Time: 1.12 seconds, Learning Rate: 1.3812236804962113e-05\n",
            "Step [1990/50326], Loss: 7.6211, Average Loss: 7.7113, Time: 0.89 seconds, Learning Rate: 1.377245627641662e-05\n",
            "Step [2000/50326], Loss: 7.7436, Average Loss: 7.7094, Time: 0.97 seconds, Learning Rate: 1.3833102717399718e-05\n",
            "Step [2010/50326], Loss: 7.7066, Average Loss: 7.7074, Time: 0.89 seconds, Learning Rate: 1.38147914052011e-05\n",
            "Step [2020/50326], Loss: 7.3359, Average Loss: 7.7047, Time: 1.09 seconds, Learning Rate: 1.3631272540092262e-05\n",
            "Step [2030/50326], Loss: 6.1197, Average Loss: 7.7014, Time: 1.37 seconds, Learning Rate: 1.3029248881339957e-05\n",
            "Step [2040/50326], Loss: 7.4259, Average Loss: 7.6987, Time: 1.09 seconds, Learning Rate: 1.367580231666584e-05\n",
            "Step [2050/50326], Loss: 8.3665, Average Loss: 7.6990, Time: 1.15 seconds, Learning Rate: 1.41414368057249e-05\n",
            "Step [2060/50326], Loss: 8.0168, Average Loss: 7.6998, Time: 1.03 seconds, Learning Rate: 1.3968311243057104e-05\n",
            "Step [2070/50326], Loss: 7.5618, Average Loss: 7.6997, Time: 0.97 seconds, Learning Rate: 1.374310752391799e-05\n",
            "Step [2080/50326], Loss: 6.7562, Average Loss: 7.6990, Time: 1.04 seconds, Learning Rate: 1.3344300980567834e-05\n",
            "Step [2090/50326], Loss: 7.7593, Average Loss: 7.6973, Time: 0.96 seconds, Learning Rate: 1.38408333158495e-05\n",
            "Step [2100/50326], Loss: 7.6491, Average Loss: 7.6948, Time: 1.13 seconds, Learning Rate: 1.3786288127898964e-05\n",
            "Step [2110/50326], Loss: 7.5411, Average Loss: 7.6942, Time: 1.13 seconds, Learning Rate: 1.3732824449539109e-05\n",
            "Step [2120/50326], Loss: 7.3048, Average Loss: 7.6937, Time: 1.04 seconds, Learning Rate: 1.361589584350572e-05\n",
            "Step [2130/50326], Loss: 8.0189, Average Loss: 7.6929, Time: 1.06 seconds, Learning Rate: 1.396935121059435e-05\n",
            "Step [2140/50326], Loss: 7.7213, Average Loss: 7.6916, Time: 0.99 seconds, Learning Rate: 1.382206338882425e-05\n",
            "Step [2150/50326], Loss: 7.1894, Average Loss: 7.6917, Time: 1.09 seconds, Learning Rate: 1.35587549853325e-05\n",
            "Step [2160/50326], Loss: 8.3275, Average Loss: 7.6910, Time: 1.00 seconds, Learning Rate: 1.412209048271167e-05\n",
            "Step [2170/50326], Loss: 7.3253, Average Loss: 7.6892, Time: 1.06 seconds, Learning Rate: 1.3626002128124052e-05\n",
            "Step [2180/50326], Loss: 7.5257, Average Loss: 7.6873, Time: 1.19 seconds, Learning Rate: 1.3725232639312918e-05\n",
            "Step [2190/50326], Loss: 7.1729, Average Loss: 7.6864, Time: 0.88 seconds, Learning Rate: 1.3550572144985305e-05\n",
            "Step [2200/50326], Loss: 7.6476, Average Loss: 7.6858, Time: 1.07 seconds, Learning Rate: 1.3785576956272294e-05\n",
            "Step [2210/50326], Loss: 8.1694, Average Loss: 7.6851, Time: 1.19 seconds, Learning Rate: 1.4043863964080705e-05\n",
            "Step [2220/50326], Loss: 8.1876, Average Loss: 7.6822, Time: 1.09 seconds, Learning Rate: 1.4052865843772846e-05\n",
            "Step [2230/50326], Loss: 7.5056, Average Loss: 7.6821, Time: 0.79 seconds, Learning Rate: 1.3715276236534284e-05\n",
            "Step [2240/50326], Loss: 5.9542, Average Loss: 7.6816, Time: 0.93 seconds, Learning Rate: 1.2947313554287017e-05\n",
            "Step [2250/50326], Loss: 7.3826, Average Loss: 7.6788, Time: 1.04 seconds, Learning Rate: 1.3654375586509691e-05\n",
            "Step [2260/50326], Loss: 7.4198, Average Loss: 7.6780, Time: 0.71 seconds, Learning Rate: 1.3672818133830958e-05\n",
            "Step [2270/50326], Loss: 7.4191, Average Loss: 7.6763, Time: 1.54 seconds, Learning Rate: 1.3672444491386213e-05\n",
            "Step [2280/50326], Loss: 7.3134, Average Loss: 7.6756, Time: 1.19 seconds, Learning Rate: 1.3620153431892407e-05\n",
            "Step [2290/50326], Loss: 7.7238, Average Loss: 7.6749, Time: 0.90 seconds, Learning Rate: 1.3823304693699018e-05\n",
            "Step [2300/50326], Loss: 7.1136, Average Loss: 7.6748, Time: 1.10 seconds, Learning Rate: 1.3521238734722117e-05\n",
            "Step [2310/50326], Loss: 7.9391, Average Loss: 7.6731, Time: 0.93 seconds, Learning Rate: 1.3929832680225293e-05\n",
            "Step [2320/50326], Loss: 7.3023, Average Loss: 7.6723, Time: 0.72 seconds, Learning Rate: 1.3614632587432732e-05\n",
            "Step [2330/50326], Loss: 7.1124, Average Loss: 7.6728, Time: 0.95 seconds, Learning Rate: 1.3520657618045938e-05\n",
            "Step [2340/50326], Loss: 7.1500, Average Loss: 7.6716, Time: 1.31 seconds, Learning Rate: 1.3539230456352125e-05\n",
            "Step [2350/50326], Loss: 7.0971, Average Loss: 7.6709, Time: 0.94 seconds, Learning Rate: 1.3513054950237193e-05\n",
            "Step [2360/50326], Loss: 7.2180, Average Loss: 7.6712, Time: 0.87 seconds, Learning Rate: 1.3572923893928342e-05\n",
            "Step [2370/50326], Loss: 7.1797, Average Loss: 7.6699, Time: 1.14 seconds, Learning Rate: 1.3553932566642804e-05\n",
            "Step [2380/50326], Loss: 9.4260, Average Loss: 7.6694, Time: 1.05 seconds, Learning Rate: 1.4665874543190052e-05\n",
            "Step [2390/50326], Loss: 8.1718, Average Loss: 7.6694, Time: 1.05 seconds, Learning Rate: 1.4045044608116123e-05\n",
            "Step [2400/50326], Loss: 8.8942, Average Loss: 7.6694, Time: 0.93 seconds, Learning Rate: 1.4402650403976459e-05\n",
            "Step [2410/50326], Loss: 6.2010, Average Loss: 7.6674, Time: 1.05 seconds, Learning Rate: 1.3069470322132322e-05\n",
            "Step [2420/50326], Loss: 8.8424, Average Loss: 7.6667, Time: 1.33 seconds, Learning Rate: 1.437697505474072e-05\n",
            "Step [2430/50326], Loss: 7.4010, Average Loss: 7.6664, Time: 1.19 seconds, Learning Rate: 1.3663505160808442e-05\n",
            "Step [2440/50326], Loss: 7.3314, Average Loss: 7.6653, Time: 1.10 seconds, Learning Rate: 1.3629036114215731e-05\n",
            "Step [2450/50326], Loss: 7.2940, Average Loss: 7.6642, Time: 1.33 seconds, Learning Rate: 1.3610532197952257e-05\n",
            "Step [2460/50326], Loss: 7.2777, Average Loss: 7.6630, Time: 1.29 seconds, Learning Rate: 1.3602457697391324e-05\n",
            "Step [2470/50326], Loss: 6.7957, Average Loss: 7.6625, Time: 1.28 seconds, Learning Rate: 1.3363848168850164e-05\n",
            "Step [2480/50326], Loss: 7.3105, Average Loss: 7.6616, Time: 1.04 seconds, Learning Rate: 1.3618681285381392e-05\n",
            "Step [2490/50326], Loss: 7.1405, Average Loss: 7.6606, Time: 1.19 seconds, Learning Rate: 1.3534539036750646e-05\n",
            "Step [2500/50326], Loss: 7.2984, Average Loss: 7.6604, Time: 0.96 seconds, Learning Rate: 1.361273180246355e-05\n",
            "Step [2510/50326], Loss: 7.6092, Average Loss: 7.6594, Time: 1.34 seconds, Learning Rate: 1.376653252124791e-05\n",
            "Step [2520/50326], Loss: 6.0094, Average Loss: 7.6565, Time: 1.04 seconds, Learning Rate: 1.2974634535312713e-05\n",
            "Step [2530/50326], Loss: 7.1125, Average Loss: 7.6550, Time: 1.00 seconds, Learning Rate: 1.3520677917003572e-05\n",
            "Step [2540/50326], Loss: 5.8000, Average Loss: 7.6559, Time: 1.12 seconds, Learning Rate: 1.2870986404419083e-05\n",
            "Step [2550/50326], Loss: 7.6578, Average Loss: 7.6551, Time: 1.02 seconds, Learning Rate: 1.37906271481515e-05\n",
            "Step [2560/50326], Loss: 6.9766, Average Loss: 7.6537, Time: 0.96 seconds, Learning Rate: 1.3453431010246453e-05\n",
            "Step [2570/50326], Loss: 7.6697, Average Loss: 7.6534, Time: 0.96 seconds, Learning Rate: 1.3796482689380795e-05\n",
            "Step [2580/50326], Loss: 7.0282, Average Loss: 7.6522, Time: 1.08 seconds, Learning Rate: 1.3478970167636911e-05\n",
            "Step [2590/50326], Loss: 6.9668, Average Loss: 7.6489, Time: 1.19 seconds, Learning Rate: 1.3448574130534961e-05\n",
            "Step [2600/50326], Loss: 7.6057, Average Loss: 7.6493, Time: 1.14 seconds, Learning Rate: 1.3764817023277072e-05\n",
            "Step [2610/50326], Loss: 7.3293, Average Loss: 7.6500, Time: 1.17 seconds, Learning Rate: 1.3627990717887868e-05\n",
            "Step [2620/50326], Loss: 7.1069, Average Loss: 7.6482, Time: 1.21 seconds, Learning Rate: 1.3517890822887315e-05\n",
            "Step [2630/50326], Loss: 8.6098, Average Loss: 7.6478, Time: 1.10 seconds, Learning Rate: 1.426186532974224e-05\n",
            "Step [2640/50326], Loss: 7.7460, Average Loss: 7.6472, Time: 1.31 seconds, Learning Rate: 1.3834278640746933e-05\n",
            "Step [2650/50326], Loss: 7.7536, Average Loss: 7.6451, Time: 1.03 seconds, Learning Rate: 1.3838009164333136e-05\n",
            "Step [2660/50326], Loss: 7.0980, Average Loss: 7.6439, Time: 1.31 seconds, Learning Rate: 1.3513523242473782e-05\n",
            "Step [2670/50326], Loss: 7.1233, Average Loss: 7.6426, Time: 0.92 seconds, Learning Rate: 1.3526017251014496e-05\n",
            "Step [2680/50326], Loss: 8.1859, Average Loss: 7.6414, Time: 1.07 seconds, Learning Rate: 1.4052044916152885e-05\n",
            "Step [2690/50326], Loss: 7.3247, Average Loss: 7.6406, Time: 1.13 seconds, Learning Rate: 1.3625744850635378e-05\n",
            "Step [2700/50326], Loss: 6.8892, Average Loss: 7.6386, Time: 0.81 seconds, Learning Rate: 1.3410140414238195e-05\n",
            "Step [2710/50326], Loss: 8.0230, Average Loss: 7.6372, Time: 0.78 seconds, Learning Rate: 1.3971406598090994e-05\n",
            "Step [2720/50326], Loss: 7.4895, Average Loss: 7.6369, Time: 1.04 seconds, Learning Rate: 1.3707305355071853e-05\n",
            "Step [2730/50326], Loss: 7.6707, Average Loss: 7.6363, Time: 0.96 seconds, Learning Rate: 1.3796973404884466e-05\n",
            "Step [2740/50326], Loss: 6.9400, Average Loss: 7.6374, Time: 1.14 seconds, Learning Rate: 1.3435311830043828e-05\n",
            "Step [2750/50326], Loss: 7.4422, Average Loss: 7.6377, Time: 1.06 seconds, Learning Rate: 1.3683902781009456e-05\n",
            "Step [2760/50326], Loss: 6.9557, Average Loss: 7.6365, Time: 0.96 seconds, Learning Rate: 1.3443053050041295e-05\n",
            "Step [2770/50326], Loss: 7.0281, Average Loss: 7.6348, Time: 1.14 seconds, Learning Rate: 1.3478898413181454e-05\n",
            "Step [2780/50326], Loss: 6.9936, Average Loss: 7.6339, Time: 1.18 seconds, Learning Rate: 1.3461850121021391e-05\n",
            "Step [2790/50326], Loss: 7.3929, Average Loss: 7.6339, Time: 1.16 seconds, Learning Rate: 1.3659469916820646e-05\n",
            "Step [2800/50326], Loss: 7.3552, Average Loss: 7.6338, Time: 1.13 seconds, Learning Rate: 1.364080785751323e-05\n",
            "Step [2810/50326], Loss: 6.5678, Average Loss: 7.6316, Time: 0.96 seconds, Learning Rate: 1.3251075892448402e-05\n",
            "Step [2820/50326], Loss: 7.1704, Average Loss: 7.6307, Time: 0.88 seconds, Learning Rate: 1.3549354915619084e-05\n",
            "Step [2830/50326], Loss: 8.7026, Average Loss: 7.6310, Time: 1.24 seconds, Learning Rate: 1.4307787237167573e-05\n",
            "Step [2840/50326], Loss: 6.5547, Average Loss: 7.6310, Time: 0.95 seconds, Learning Rate: 1.3244554026127046e-05\n",
            "Step [2850/50326], Loss: 8.1508, Average Loss: 7.6308, Time: 1.06 seconds, Learning Rate: 1.4034638795852825e-05\n",
            "Step [2860/50326], Loss: 6.8251, Average Loss: 7.6286, Time: 1.14 seconds, Learning Rate: 1.3378410782813953e-05\n",
            "Step [2870/50326], Loss: 5.8636, Average Loss: 7.6269, Time: 0.93 seconds, Learning Rate: 1.2902500772476074e-05\n",
            "Step [2880/50326], Loss: 8.1329, Average Loss: 7.6265, Time: 1.11 seconds, Learning Rate: 1.4025794115066455e-05\n",
            "Step [2890/50326], Loss: 7.6761, Average Loss: 7.6253, Time: 1.25 seconds, Learning Rate: 1.3799683315753769e-05\n",
            "Step [2900/50326], Loss: 8.0646, Average Loss: 7.6251, Time: 0.99 seconds, Learning Rate: 1.3991959056854341e-05\n",
            "Step [2910/50326], Loss: 7.9815, Average Loss: 7.6243, Time: 0.99 seconds, Learning Rate: 1.3950843517780403e-05\n",
            "Step [2920/50326], Loss: 7.1955, Average Loss: 7.6234, Time: 1.22 seconds, Learning Rate: 1.3561762535572194e-05\n",
            "Step [2930/50326], Loss: 7.3386, Average Loss: 7.6213, Time: 1.29 seconds, Learning Rate: 1.3632630681991526e-05\n",
            "Step [2940/50326], Loss: 7.7342, Average Loss: 7.6215, Time: 1.26 seconds, Learning Rate: 1.3828420975208191e-05\n",
            "Step [2950/50326], Loss: 8.5794, Average Loss: 7.6228, Time: 0.90 seconds, Learning Rate: 1.424679925441721e-05\n",
            "Step [2960/50326], Loss: 6.6900, Average Loss: 7.6211, Time: 0.78 seconds, Learning Rate: 1.331152595281617e-05\n",
            "Step [2970/50326], Loss: 6.9956, Average Loss: 7.6196, Time: 1.06 seconds, Learning Rate: 1.3462842881679711e-05\n",
            "Step [2980/50326], Loss: 6.5301, Average Loss: 7.6173, Time: 1.21 seconds, Learning Rate: 1.3232421622276363e-05\n",
            "Step [2990/50326], Loss: 7.1808, Average Loss: 7.6156, Time: 1.03 seconds, Learning Rate: 1.3554481346607076e-05\n",
            "Step [3000/50326], Loss: 8.9733, Average Loss: 7.6148, Time: 1.08 seconds, Learning Rate: 1.4441783490180933e-05\n",
            "Step [3010/50326], Loss: 7.0289, Average Loss: 7.6136, Time: 1.09 seconds, Learning Rate: 1.3479284565448895e-05\n",
            "Step [3020/50326], Loss: 8.1250, Average Loss: 7.6142, Time: 1.28 seconds, Learning Rate: 1.4021854701042226e-05\n",
            "Step [3030/50326], Loss: 7.1983, Average Loss: 7.6140, Time: 1.39 seconds, Learning Rate: 1.3563172369003272e-05\n",
            "Step [3040/50326], Loss: 5.6469, Average Loss: 7.6139, Time: 1.19 seconds, Learning Rate: 1.2795207326412123e-05\n",
            "Step [3050/50326], Loss: 7.0450, Average Loss: 7.6128, Time: 1.21 seconds, Learning Rate: 1.3487280466556722e-05\n",
            "Step [3060/50326], Loss: 6.9961, Average Loss: 7.6105, Time: 1.05 seconds, Learning Rate: 1.34630730152131e-05\n",
            "Step [3070/50326], Loss: 8.4990, Average Loss: 7.6085, Time: 0.81 seconds, Learning Rate: 1.4206982612609776e-05\n",
            "Step [3080/50326], Loss: 7.5376, Average Loss: 7.6078, Time: 0.86 seconds, Learning Rate: 1.3731120045185255e-05\n",
            "Step [3090/50326], Loss: 8.0659, Average Loss: 7.6065, Time: 1.13 seconds, Learning Rate: 1.3992609095573231e-05\n",
            "Step [3100/50326], Loss: 6.3911, Average Loss: 7.6055, Time: 1.10 seconds, Learning Rate: 1.3163615236282458e-05\n",
            "Step [3110/50326], Loss: 6.8160, Average Loss: 7.6048, Time: 0.88 seconds, Learning Rate: 1.3373905830383418e-05\n",
            "Step [3120/50326], Loss: 7.5060, Average Loss: 7.6038, Time: 1.35 seconds, Learning Rate: 1.3715471909046318e-05\n",
            "Step [3130/50326], Loss: 7.9588, Average Loss: 7.6032, Time: 1.28 seconds, Learning Rate: 1.3939595062732914e-05\n",
            "Step [3140/50326], Loss: 8.0194, Average Loss: 7.6038, Time: 1.43 seconds, Learning Rate: 1.3969622178077882e-05\n",
            "Step [3150/50326], Loss: 7.7169, Average Loss: 7.6032, Time: 1.20 seconds, Learning Rate: 1.3819843957424187e-05\n",
            "Step [3160/50326], Loss: 7.9068, Average Loss: 7.6013, Time: 0.81 seconds, Learning Rate: 1.3913877935409747e-05\n",
            "Step [3170/50326], Loss: 8.4869, Average Loss: 7.5998, Time: 0.98 seconds, Learning Rate: 1.4200998668670578e-05\n",
            "Step [3180/50326], Loss: 6.3584, Average Loss: 7.5985, Time: 1.12 seconds, Learning Rate: 1.3147418792247674e-05\n",
            "Step [3190/50326], Loss: 7.3921, Average Loss: 7.5976, Time: 1.05 seconds, Learning Rate: 1.3659109020233033e-05\n",
            "Step [3200/50326], Loss: 7.9177, Average Loss: 7.5970, Time: 0.98 seconds, Learning Rate: 1.391927179336567e-05\n",
            "Step [3210/50326], Loss: 7.8417, Average Loss: 7.5954, Time: 0.96 seconds, Learning Rate: 1.3881617226600557e-05\n",
            "Step [3220/50326], Loss: 6.2960, Average Loss: 7.5963, Time: 1.33 seconds, Learning Rate: 1.3116537940501974e-05\n",
            "Step [3230/50326], Loss: 7.1200, Average Loss: 7.5959, Time: 1.13 seconds, Learning Rate: 1.3524393334388766e-05\n",
            "Step [3240/50326], Loss: 7.0236, Average Loss: 7.5958, Time: 1.02 seconds, Learning Rate: 1.3476700460910745e-05\n",
            "Step [3250/50326], Loss: 7.5004, Average Loss: 7.5951, Time: 1.07 seconds, Learning Rate: 1.371269756078719e-05\n",
            "Step [3260/50326], Loss: 7.0555, Average Loss: 7.5958, Time: 0.93 seconds, Learning Rate: 1.3492458825111401e-05\n",
            "Step [3270/50326], Loss: 7.6673, Average Loss: 7.5956, Time: 1.31 seconds, Learning Rate: 1.379530818223973e-05\n",
            "Step [3280/50326], Loss: 6.1882, Average Loss: 7.5948, Time: 0.98 seconds, Learning Rate: 1.3063149557113616e-05\n",
            "Step [3290/50326], Loss: 6.9699, Average Loss: 7.5953, Time: 1.36 seconds, Learning Rate: 1.3450121808051906e-05\n",
            "Step [3300/50326], Loss: 8.5614, Average Loss: 7.5949, Time: 0.99 seconds, Learning Rate: 1.4237896981239104e-05\n",
            "Step [3310/50326], Loss: 8.8309, Average Loss: 7.5935, Time: 1.07 seconds, Learning Rate: 1.4371281433105399e-05\n",
            "Step [3320/50326], Loss: 8.9979, Average Loss: 7.5936, Time: 0.98 seconds, Learning Rate: 1.4453960504531788e-05\n",
            "Step [3330/50326], Loss: 8.4993, Average Loss: 7.5932, Time: 0.96 seconds, Learning Rate: 1.4207170968055737e-05\n",
            "Step [3340/50326], Loss: 7.4149, Average Loss: 7.5926, Time: 1.07 seconds, Learning Rate: 1.3670351338386606e-05\n",
            "Step [3350/50326], Loss: 7.1067, Average Loss: 7.5910, Time: 1.35 seconds, Learning Rate: 1.3517802073955392e-05\n",
            "Step [3360/50326], Loss: 8.5821, Average Loss: 7.5913, Time: 1.11 seconds, Learning Rate: 1.424815928459149e-05\n",
            "Step [3370/50326], Loss: 8.8883, Average Loss: 7.5904, Time: 0.91 seconds, Learning Rate: 1.439971555232995e-05\n",
            "Step [3380/50326], Loss: 7.7084, Average Loss: 7.5896, Time: 1.02 seconds, Learning Rate: 1.3815652222633475e-05\n",
            "Step [3390/50326], Loss: 7.7138, Average Loss: 7.5900, Time: 1.08 seconds, Learning Rate: 1.3818343250751296e-05\n",
            "Step [3400/50326], Loss: 6.9420, Average Loss: 7.5888, Time: 1.08 seconds, Learning Rate: 1.3436270365715225e-05\n",
            "Step [3410/50326], Loss: 8.0543, Average Loss: 7.5879, Time: 0.89 seconds, Learning Rate: 1.3986877236366082e-05\n",
            "Step [3420/50326], Loss: 7.9115, Average Loss: 7.5872, Time: 1.06 seconds, Learning Rate: 1.391618328332899e-05\n",
            "Step [3430/50326], Loss: 7.9163, Average Loss: 7.5867, Time: 0.82 seconds, Learning Rate: 1.3918567938804634e-05\n",
            "Step [3440/50326], Loss: 7.1947, Average Loss: 7.5861, Time: 1.06 seconds, Learning Rate: 1.356136458158491e-05\n",
            "Step [3450/50326], Loss: 6.8087, Average Loss: 7.5849, Time: 1.02 seconds, Learning Rate: 1.337030040502551e-05\n",
            "Step [3460/50326], Loss: 7.8904, Average Loss: 7.5849, Time: 1.10 seconds, Learning Rate: 1.3905746550559934e-05\n",
            "Step [3470/50326], Loss: 7.2984, Average Loss: 7.5840, Time: 1.01 seconds, Learning Rate: 1.3612688136100664e-05\n",
            "Step [3480/50326], Loss: 6.9929, Average Loss: 7.5834, Time: 0.96 seconds, Learning Rate: 1.346150126218805e-05\n",
            "Step [3490/50326], Loss: 7.5367, Average Loss: 7.5831, Time: 0.80 seconds, Learning Rate: 1.3730646088123178e-05\n",
            "Step [3500/50326], Loss: 6.5177, Average Loss: 7.5826, Time: 0.97 seconds, Learning Rate: 1.322626395702379e-05\n",
            "Step [3510/50326], Loss: 7.6396, Average Loss: 7.5827, Time: 1.04 seconds, Learning Rate: 1.3781593403816315e-05\n",
            "Step [3520/50326], Loss: 6.8618, Average Loss: 7.5821, Time: 1.05 seconds, Learning Rate: 1.3396588027477177e-05\n",
            "Step [3530/50326], Loss: 7.7558, Average Loss: 7.5818, Time: 1.26 seconds, Learning Rate: 1.3839137408733444e-05\n",
            "Step [3540/50326], Loss: 7.4982, Average Loss: 7.5812, Time: 0.82 seconds, Learning Rate: 1.3711597404479897e-05\n",
            "Step [3550/50326], Loss: 6.6907, Average Loss: 7.5807, Time: 1.16 seconds, Learning Rate: 1.331188991785052e-05\n",
            "Step [3560/50326], Loss: 6.9297, Average Loss: 7.5790, Time: 0.95 seconds, Learning Rate: 1.3430209946632369e-05\n",
            "Step [3570/50326], Loss: 8.3310, Average Loss: 7.5790, Time: 0.94 seconds, Learning Rate: 1.4123831472396766e-05\n",
            "Step [3580/50326], Loss: 8.0504, Average Loss: 7.5776, Time: 0.78 seconds, Learning Rate: 1.3984935145377896e-05\n",
            "Step [3590/50326], Loss: 7.2579, Average Loss: 7.5771, Time: 1.20 seconds, Learning Rate: 1.3592670295238305e-05\n",
            "Step [3600/50326], Loss: 6.4921, Average Loss: 7.5760, Time: 0.79 seconds, Learning Rate: 1.3213578052520637e-05\n",
            "Step [3610/50326], Loss: 6.9796, Average Loss: 7.5744, Time: 1.33 seconds, Learning Rate: 1.3454897255897548e-05\n",
            "Step [3620/50326], Loss: 6.8222, Average Loss: 7.5740, Time: 1.24 seconds, Learning Rate: 1.3376994340419658e-05\n",
            "Step [3630/50326], Loss: 6.7990, Average Loss: 7.5731, Time: 1.07 seconds, Learning Rate: 1.3365507490634978e-05\n",
            "Step [3640/50326], Loss: 7.0141, Average Loss: 7.5716, Time: 1.07 seconds, Learning Rate: 1.3471970331669012e-05\n",
            "Step [3650/50326], Loss: 7.3868, Average Loss: 7.5715, Time: 1.01 seconds, Learning Rate: 1.3656458353996489e-05\n",
            "Step [3660/50326], Loss: 6.4162, Average Loss: 7.5706, Time: 1.24 seconds, Learning Rate: 1.3176007277965694e-05\n",
            "Step [3670/50326], Loss: 7.0630, Average Loss: 7.5697, Time: 1.05 seconds, Learning Rate: 1.3496201150417445e-05\n",
            "Step [3680/50326], Loss: 7.7135, Average Loss: 7.5693, Time: 0.89 seconds, Learning Rate: 1.3818181567192195e-05\n",
            "Step [3690/50326], Loss: 7.8502, Average Loss: 7.5681, Time: 0.89 seconds, Learning Rate: 1.3885837049484278e-05\n",
            "Step [3700/50326], Loss: 6.5295, Average Loss: 7.5674, Time: 1.10 seconds, Learning Rate: 1.323211666584034e-05\n",
            "Step [3710/50326], Loss: 7.8822, Average Loss: 7.5663, Time: 1.13 seconds, Learning Rate: 1.3901707057952804e-05\n",
            "Step [3720/50326], Loss: 7.2454, Average Loss: 7.5652, Time: 1.29 seconds, Learning Rate: 1.3586481473445985e-05\n",
            "Step [3730/50326], Loss: 6.9599, Average Loss: 7.5646, Time: 1.17 seconds, Learning Rate: 1.3445143134594164e-05\n",
            "Step [3740/50326], Loss: 6.2374, Average Loss: 7.5642, Time: 1.04 seconds, Learning Rate: 1.308751538753517e-05\n",
            "Step [3750/50326], Loss: 6.8571, Average Loss: 7.5642, Time: 1.33 seconds, Learning Rate: 1.339426002025598e-05\n",
            "Step [3760/50326], Loss: 8.1866, Average Loss: 7.5635, Time: 1.24 seconds, Learning Rate: 1.4052349400520483e-05\n",
            "Step [3770/50326], Loss: 8.4725, Average Loss: 7.5624, Time: 1.10 seconds, Learning Rate: 1.4193875150680536e-05\n",
            "Step [3780/50326], Loss: 8.5956, Average Loss: 7.5620, Time: 1.23 seconds, Learning Rate: 1.4254829144477968e-05\n",
            "Step [3790/50326], Loss: 6.6472, Average Loss: 7.5609, Time: 1.03 seconds, Learning Rate: 1.3290382463932256e-05\n",
            "Step [3800/50326], Loss: 7.6571, Average Loss: 7.5603, Time: 1.14 seconds, Learning Rate: 1.3790249021053427e-05\n",
            "Step [3810/50326], Loss: 8.0961, Average Loss: 7.5587, Time: 1.05 seconds, Learning Rate: 1.4007547240257222e-05\n",
            "Step [3820/50326], Loss: 7.0208, Average Loss: 7.5591, Time: 0.80 seconds, Learning Rate: 1.3475292987823546e-05\n",
            "Step [3830/50326], Loss: 7.2415, Average Loss: 7.5583, Time: 0.97 seconds, Learning Rate: 1.3584562041759312e-05\n",
            "Step [3840/50326], Loss: 8.1087, Average Loss: 7.5577, Time: 0.71 seconds, Learning Rate: 1.401382292270645e-05\n",
            "Step [3850/50326], Loss: 7.3948, Average Loss: 7.5567, Time: 1.12 seconds, Learning Rate: 1.3660422315597689e-05\n",
            "Step [3860/50326], Loss: 7.3887, Average Loss: 7.5568, Time: 1.20 seconds, Learning Rate: 1.3657407684326356e-05\n",
            "Step [3870/50326], Loss: 6.1961, Average Loss: 7.5545, Time: 1.05 seconds, Learning Rate: 1.3067072448730685e-05\n",
            "Step [3880/50326], Loss: 6.6330, Average Loss: 7.5534, Time: 1.19 seconds, Learning Rate: 1.328333164453496e-05\n",
            "Step [3890/50326], Loss: 7.2126, Average Loss: 7.5534, Time: 1.06 seconds, Learning Rate: 1.3570213274955744e-05\n",
            "Step [3900/50326], Loss: 8.7974, Average Loss: 7.5535, Time: 1.28 seconds, Learning Rate: 1.435473683834057e-05\n",
            "Step [3910/50326], Loss: 7.4308, Average Loss: 7.5529, Time: 0.82 seconds, Learning Rate: 1.3678254005909181e-05\n",
            "Step [3920/50326], Loss: 8.8297, Average Loss: 7.5523, Time: 1.12 seconds, Learning Rate: 1.4370679073333856e-05\n",
            "Step [3930/50326], Loss: 7.3587, Average Loss: 7.5512, Time: 1.16 seconds, Learning Rate: 1.3642562301158754e-05\n",
            "Step [3940/50326], Loss: 6.1935, Average Loss: 7.5507, Time: 0.72 seconds, Learning Rate: 1.3065800459385032e-05\n",
            "Step [3950/50326], Loss: 7.9562, Average Loss: 7.5491, Time: 1.05 seconds, Learning Rate: 1.3938325669765575e-05\n",
            "Step [3960/50326], Loss: 7.7338, Average Loss: 7.5486, Time: 1.27 seconds, Learning Rate: 1.3828237340450431e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'transformer_model.pth'\n",
        "model_path = '/content/drive/MyDrive/transformer/'\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "os.chdir(model_path)\n",
        "\n",
        "torch.save(model.state_dict(), model_name)"
      ],
      "metadata": {
        "id": "nxU_vo1cphkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_saved = Transformer(\n",
        "    num_tokens=enc_voc_size, dim_model=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dropout_p=0.1\n",
        ").to(device)\n",
        "\n",
        "model_location = os.path.join(model_path, model_name)\n",
        "model_saved.load_state_dict(torch.load(model_location, map_location=device))"
      ],
      "metadata": {
        "id": "0DDNm0Rypor8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, input_sequence, max_length=512, SOS_token=trg_sos_idx, EOS_token=trg_eos_idx):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
        "\n",
        "    num_tokens = len(input_sequence[0])\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Get source mask\n",
        "        tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
        "\n",
        "        pred = model(input_sequence, y_input, tgt_mask)\n",
        "\n",
        "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
        "        next_item = torch.tensor([[next_item]], device=device)\n",
        "\n",
        "        # Concatenate previous input with predicted best word\n",
        "        y_input = torch.cat((y_input, next_item), dim=1)\n",
        "\n",
        "        # Stop if model predicts end of sentence\n",
        "        if next_item.view(-1).item() == EOS_token:\n",
        "            break\n",
        "\n",
        "    return y_input.view(-1).tolist()"
      ],
      "metadata": {
        "id": "09QJSOOGUGT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_encode(text, tokenizer):\n",
        "  input_ids = np.array(tokenizer.encode(text).ids)\n",
        "\n",
        "  SOS_token = np.array([trg_sos_idx])\n",
        "  EOS_token = np.array([trg_eos_idx])\n",
        "  #print(SOS_token, EOS_token)\n",
        "  X = np.concatenate((SOS_token, input_ids, EOS_token))\n",
        "  print(X)\n",
        "  return X"
      ],
      "metadata": {
        "id": "Lc_CBTvSUNdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    torch.tensor([text_to_encode(\"sa dus la mare sa se uite la soare.\", tokenizer)], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 0, 1, 3]], dtype=torch.long, device=device)\n",
        "]\n",
        "\n",
        "for idx, example in enumerate(examples):\n",
        "    result = predict(model_saved, example)\n",
        "    print(f\"Example {idx}\")\n",
        "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
        "    print(f\"Continuation: {result}\")\n",
        "    print(f\"Text-OUTPUT:\", tokenizer.decode(result[1:-1]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "8Q3F9SSXpuHi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}