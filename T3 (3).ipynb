{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "64621e6a9a124f9db253e257bc3c23b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9536ab7dee540b88ea33a9f477858eb",
              "IPY_MODEL_6fcb62959c324b5db50842695a9d292f",
              "IPY_MODEL_253e9b5770af4cfaa84f720704d402bf",
              "IPY_MODEL_eb6866ab514f41caa0c4e982f3808187",
              "IPY_MODEL_ac0243c16e1249ecb9dbc692cac4b27c"
            ],
            "layout": "IPY_MODEL_92db287d83954f149944eca53a630ba1"
          }
        },
        "a9536ab7dee540b88ea33a9f477858eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed8e1ba1b6a4cc0a6b13127853e392d",
            "placeholder": "​",
            "style": "IPY_MODEL_b5ee11cfe6f94fedae0258dc3bc92c53",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "6fcb62959c324b5db50842695a9d292f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_6ea498a7204c4250967f81e9572b46cc",
            "placeholder": "​",
            "style": "IPY_MODEL_674809c2fb71415289a3b7c0f5ead414",
            "value": ""
          }
        },
        "253e9b5770af4cfaa84f720704d402bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_6fac5b1623b6437fb4e9072a8fb6e99a",
            "style": "IPY_MODEL_0c470a869dc648ea99d0545bbef9b01f",
            "value": true
          }
        },
        "eb6866ab514f41caa0c4e982f3808187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_2d7a4af157e9471fb07a0910fb1d153b",
            "style": "IPY_MODEL_2d587e804f734899a5746e09a038f8d0",
            "tooltip": ""
          }
        },
        "ac0243c16e1249ecb9dbc692cac4b27c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c51ad8809a5b4090872e2d51f78c697a",
            "placeholder": "​",
            "style": "IPY_MODEL_1b5d88ab52c1406eabfdf146d1c0fb94",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "92db287d83954f149944eca53a630ba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "5ed8e1ba1b6a4cc0a6b13127853e392d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ee11cfe6f94fedae0258dc3bc92c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ea498a7204c4250967f81e9572b46cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "674809c2fb71415289a3b7c0f5ead414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fac5b1623b6437fb4e9072a8fb6e99a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c470a869dc648ea99d0545bbef9b01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d7a4af157e9471fb07a0910fb1d153b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d587e804f734899a5746e09a038f8d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c51ad8809a5b4090872e2d51f78c697a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b5d88ab52c1406eabfdf146d1c0fb94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "Q70reR67PfNp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets"
      ],
      "metadata": {
        "id": "OMdGxf6Gifhc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdAm5owLhNB2",
        "outputId": "8aea795c-6c27-4818-e728-7f57543bd36b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_model, dropout_p, max_len):\n",
        "        super().__init__()\n",
        "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "        # max_len determines how far the position can have an effect on a token (window)\n",
        "\n",
        "        # Info\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Encoding - From formula\n",
        "        pos_encoding = torch.zeros(max_len, dim_model)\n",
        "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
        "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
        "\n",
        "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
        "\n",
        "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
        "\n",
        "        # Saving buffer (same as parameter without gradients needed)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
        "        # Residual connection + pos encoding\n",
        "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
      ],
      "metadata": {
        "id": "X7dEc_6HRR3b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    # Constructor\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_tokens,\n",
        "        dim_model,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dropout_p,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # INFO\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.dim_model = dim_model\n",
        "\n",
        "        # LAYERS\n",
        "        self.positional_encoder = PositionalEncoding(\n",
        "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
        "        )\n",
        "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=dim_model,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dropout=dropout_p,\n",
        "        )\n",
        "        self.out = nn.Linear(dim_model, num_tokens)\n",
        "\n",
        "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
        "        # Src size must be (batch_size, src sequence length)\n",
        "        # Tgt size must be (batch_size, tgt sequence length)\n",
        "\n",
        "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
        "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
        "        src = self.positional_encoder(src)\n",
        "        tgt = self.positional_encoder(tgt)\n",
        "\n",
        "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
        "        # to obtain size (sequence length, batch_size, dim_model),\n",
        "        src = src.permute(1,0,2)\n",
        "        tgt = tgt.permute(1,0,2)\n",
        "\n",
        "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
        "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
        "        out = self.out(transformer_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def get_tgt_mask(self, size) -> torch.tensor:\n",
        "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
        "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
        "        mask = mask.float()\n",
        "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
        "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
        "\n",
        "        # EX for size=5:\n",
        "        # [[0., -inf, -inf, -inf, -inf],\n",
        "        #  [0.,   0., -inf, -inf, -inf],\n",
        "        #  [0.,   0.,   0., -inf, -inf],\n",
        "        #  [0.,   0.,   0.,   0., -inf],\n",
        "        #  [0.,   0.,   0.,   0.,   0.]]\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
        "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
        "        # [False, False, False, True, True, True]\n",
        "        return (matrix == pad_token)"
      ],
      "metadata": {
        "id": "fzOm3mDZQTP_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 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\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "JtAKnwd9iNIi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "64621e6a9a124f9db253e257bc3c23b4",
            "a9536ab7dee540b88ea33a9f477858eb",
            "6fcb62959c324b5db50842695a9d292f",
            "253e9b5770af4cfaa84f720704d402bf",
            "eb6866ab514f41caa0c4e982f3808187",
            "ac0243c16e1249ecb9dbc692cac4b27c",
            "92db287d83954f149944eca53a630ba1",
            "5ed8e1ba1b6a4cc0a6b13127853e392d",
            "b5ee11cfe6f94fedae0258dc3bc92c53",
            "6ea498a7204c4250967f81e9572b46cc",
            "674809c2fb71415289a3b7c0f5ead414",
            "6fac5b1623b6437fb4e9072a8fb6e99a",
            "0c470a869dc648ea99d0545bbef9b01f",
            "2d7a4af157e9471fb07a0910fb1d153b",
            "2d587e804f734899a5746e09a038f8d0",
            "c51ad8809a5b4090872e2d51f78c697a",
            "1b5d88ab52c1406eabfdf146d1c0fb94"
          ]
        },
        "outputId": "9ada2792-f3e0-44ab-a8f9-913e57dba1f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64621e6a9a124f9db253e257bc3c23b4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"mateiaassAI/MEID\", split=['train[:10%]', 'train[1%:2%]', 'train[2%:3%]'])\n",
        "# dataset = load_dataset(\"mateiaassAI/MEID\", split=['train[:50%]', 'train[50%:55%]', 'train[55%:70%]'])\n",
        "train_dataset = dataset[0]\n",
        "test_dataset = dataset[1]\n",
        "valid_dataset = dataset[2]"
      ],
      "metadata": {
        "id": "fJ0hb3wki33I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860ba7a5-1d4a-4ea5-b113-95d1a2372704"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import pandas as pd\n",
        "\n",
        "dataset_tok = load_dataset(\"mateiaassAI/MEID\")\n",
        "dataset_tok = dataset_tok[\"train\"]\n",
        "dataset_tok = dataset_tok['right']\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
        "tokenizer.train_from_iterator(dataset_tok, vocab_size=100000, min_frequency=6, show_progress=True,special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n",
        "\n",
        "src_pad_idx = tokenizer.token_to_id('<pad>')\n",
        "trg_pad_idx = src_pad_idx\n",
        "trg_sos_idx = tokenizer.token_to_id('<s>')\n",
        "trg_eos_idx = tokenizer.token_to_id('</s>')\n",
        "\n",
        "enc_voc_size = tokenizer.get_vocab_size()\n",
        "dec_voc_size = enc_voc_size\n",
        "\n",
        "print(trg_sos_idx, trg_eos_idx, src_pad_idx, enc_voc_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkb6axTWkYsr",
        "outputId": "3e606561-7faf-4be3-99ac-ae1555a64c25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2 1 51374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_data(dataframe, tokenizer):\n",
        "    data = []\n",
        "    for example in dataframe:\n",
        "      input_text = example[\"wrong\"]\n",
        "      target_text = example[\"right\"]\n",
        "\n",
        "      input_ids = np.array(tokenizer.encode(input_text).ids)\n",
        "      target_ids = np.array(tokenizer.encode(target_text).ids)\n",
        "\n",
        "      SOS_token = np.array([trg_sos_idx])\n",
        "      EOS_token = np.array([trg_eos_idx])\n",
        "      X = np.concatenate((SOS_token, input_ids, EOS_token))\n",
        "      y = np.concatenate((SOS_token, target_ids, EOS_token))\n",
        "\n",
        "      data.append([X.tolist(), y.tolist()])\n",
        "\n",
        "    np.random.shuffle(data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "Za__KeXoSt0A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = generate_data(train_dataset, tokenizer)\n",
        "val_data = generate_data(valid_dataset, tokenizer)"
      ],
      "metadata": {
        "id": "R2xYKC9gSvwO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
        "    batches = []\n",
        "    for idx in range(0, len(data), batch_size):\n",
        "        # We make sure we dont get the last bit if its not batch_size size\n",
        "        if idx + batch_size < len(data):\n",
        "            # Here you would need to get the max length of the batch,\n",
        "            # and normalize the length with the PAD token.\n",
        "            if padding:\n",
        "                max_batch_length = 0\n",
        "\n",
        "                # Get longest sentence in batch\n",
        "                for seq in data[idx : idx + batch_size]:\n",
        "                    if len(seq[0]) > max_batch_length:\n",
        "                        max_batch_length = len(seq[0])\n",
        "                    if len(seq[1]) > max_batch_length:\n",
        "                        max_batch_length = len(seq[1])\n",
        "\n",
        "                # Append X padding tokens until it reaches the max length\n",
        "                for seq_idx in range(batch_size):\n",
        "                    remaining_length = max_batch_length - len(data[idx + seq_idx][0])\n",
        "                    data[idx + seq_idx][0] += [padding_token] * remaining_length\n",
        "                    # padding_array = np.array([padding_token] * remaining_length)\n",
        "                    # data[idx + seq_idx][0] = np.concatenate((data[idx + seq_idx][0], padding_array))\n",
        "\n",
        "                    remaining_length = max_batch_length - len(data[idx + seq_idx][1])\n",
        "                    # padding_array = np.array([padding_token] * remaining_length)\n",
        "                    data[idx + seq_idx][1] += [padding_token] * remaining_length\n",
        "                    # data[idx + seq_idx][1] = np.concatenate((data[idx + seq_idx][1], padding_array))\n",
        "\n",
        "            # batches.append(data[idx : idx + batch_size])\n",
        "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
        "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
        "\n",
        "    return batches"
      ],
      "metadata": {
        "id": "K91y1xqSQ3pp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = batchify_data(train_data, batch_size=4, padding = True, padding_token = src_pad_idx)\n",
        "val_dataloader = batchify_data(val_data, batch_size=4, padding = True, padding_token = src_pad_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0Q26x5to8Tk",
        "outputId": "539a13f9-07d7-4f0e-a607-858f58e59c27"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5032 batches of size 4\n",
            "503 batches of size 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = Transformer(\n",
        "    num_tokens=enc_voc_size, dim_model=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dropout_p=0.1\n",
        ").to(device)\n",
        "# opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "# loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Wxm1IHVxS8EL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912bb048-d162-433e-af63-fb136d34fcd2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.kaiming_uniform_(m.weight.data)"
      ],
      "metadata": {
        "id": "zYmONkLQh_97"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "model.apply(initialize_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkRzfNssiBvf",
        "outputId": "9d6cf46b-0f23-4564-88e6-bc182e4386f4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 96,798,894 trainable parameters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (positional_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (embedding): Embedding(51374, 512)\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=512, out_features=51374, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "init_lr = 1e-5\n",
        "factor = 0.9\n",
        "adam_eps = 5e-9\n",
        "patience = 10\n",
        "warmup = 100\n",
        "clip = 1.0\n",
        "weight_decay = 5e-4\n",
        "\n",
        "optimizer = Adam(params=model.parameters(),\n",
        "                 lr=init_lr,\n",
        "                 weight_decay=weight_decay,\n",
        "                 eps=adam_eps)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                                 verbose=True,\n",
        "                                                 factor=factor,\n",
        "                                                 patience=patience)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=src_pad_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0gFgW5njPBk",
        "outputId": "99c3afaf-696e-46b4-fe95-dd1989bcba76"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_loop(model, opt, loss_fn, dataloader):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        # batch = np.array(batch)\n",
        "        # print(batch[0])\n",
        "        X, y = batch[:, 0], batch[:, 1]\n",
        "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
        "\n",
        "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "        y_input = y[:,:-1]\n",
        "        y_expected = y[:,1:]\n",
        "\n",
        "        # Get mask to mask out the next words\n",
        "        sequence_length = y_input.size(1)\n",
        "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "        # Standard training except we pass in y_input and tgt_mask\n",
        "        pred = model(X, y_input, tgt_mask)\n",
        "\n",
        "        # Permute pred to have batch size first again\n",
        "        pred = pred.permute(1, 2, 0)\n",
        "        # print(pred[0,:,:], y_expected[0])\n",
        "        # break\n",
        "        loss = loss_fn(pred, y_expected)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.detach().item()\n",
        "\n",
        "        if (step + 1) % 10 == 0:  # Adjust frequency as needed\n",
        "            elapsed_time = time.time() - start_time\n",
        "            avg_loss = total_loss / step\n",
        "            print(f\"Step [{step+1}/{len(dataloader)}], Loss: {loss.item():.4f}, Average Loss: {avg_loss:.4f},Time: {elapsed_time:.2f} seconds\")\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "RMdU626HTCez"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def validation_loop(model, loss_fn, dataloader):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            X, y = batch[:, 0], batch[:, 1]\n",
        "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
        "\n",
        "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "            y_input = y[:,:-1]\n",
        "            y_expected = y[:,1:]\n",
        "\n",
        "            # Get mask to mask out the next words\n",
        "            sequence_length = y_input.size(1)\n",
        "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "            # Standard training except we pass in y_input and src_mask\n",
        "            pred = model(X, y_input, tgt_mask)\n",
        "\n",
        "            # Permute pred to have batch size first again\n",
        "            pred = pred.permute(1, 2, 0)\n",
        "            loss = loss_fn(pred, y_expected)\n",
        "            total_loss += loss.detach().item()\n",
        "\n",
        "            if (step + 1) % 10 == 0:  # Adjust frequency as needed\n",
        "              elapsed_time = time.time() - start_time\n",
        "              print(f\"Step [{step+1}/{len(dataloader)}], Loss: {loss.item():.4f}, Time: {elapsed_time:.2f} seconds\")\n",
        "              start_time = time.time()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "TZQBz5OiT55I"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "\n",
        "    # Used for plotting later on\n",
        "    train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "    print(\"Training and validating model\")\n",
        "    for epoch in range(epochs):\n",
        "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
        "\n",
        "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
        "        # break\n",
        "        train_loss_list += [train_loss]\n",
        "\n",
        "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
        "        validation_loss_list += [validation_loss]\n",
        "\n",
        "        if epoch > warmup:\n",
        "            scheduler.step(validation_loss)\n",
        "\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
        "        print()\n",
        "\n",
        "    return train_loss_list, validation_loss_list"
      ],
      "metadata": {
        "id": "B-IKivf4T8yQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, validation_loss_list = fit(model, optimizer, loss_fn, train_dataloader, val_dataloader, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfyzQgF0T-6q",
        "outputId": "a13aa27c-09a2-4500-a57e-38421cb1e36e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and validating model\n",
            "------------------------- Epoch 1 -------------------------\n",
            "Step [10/5032], Loss: 10.8092, Average Loss: 12.4485,Time: 2.36 seconds\n",
            "Step [20/5032], Loss: 10.3646, Average Loss: 11.4161,Time: 1.52 seconds\n",
            "Step [30/5032], Loss: 9.8388, Average Loss: 10.9671,Time: 1.48 seconds\n",
            "Step [40/5032], Loss: 10.0400, Average Loss: 10.6862,Time: 1.67 seconds\n",
            "Step [50/5032], Loss: 9.6633, Average Loss: 10.4941,Time: 1.55 seconds\n",
            "Step [60/5032], Loss: 9.7967, Average Loss: 10.3913,Time: 1.57 seconds\n",
            "Step [70/5032], Loss: 9.3780, Average Loss: 10.2996,Time: 1.35 seconds\n",
            "Step [80/5032], Loss: 9.6605, Average Loss: 10.1941,Time: 1.96 seconds\n",
            "Step [90/5032], Loss: 9.6059, Average Loss: 10.1066,Time: 2.17 seconds\n",
            "Step [100/5032], Loss: 9.2169, Average Loss: 10.0509,Time: 1.64 seconds\n",
            "Step [110/5032], Loss: 9.3070, Average Loss: 9.9943,Time: 1.51 seconds\n",
            "Step [120/5032], Loss: 9.3034, Average Loss: 9.9477,Time: 1.54 seconds\n",
            "Step [130/5032], Loss: 10.5562, Average Loss: 9.9328,Time: 1.77 seconds\n",
            "Step [140/5032], Loss: 9.1248, Average Loss: 9.8769,Time: 1.16 seconds\n",
            "Step [150/5032], Loss: 10.1212, Average Loss: 9.8524,Time: 2.22 seconds\n",
            "Step [160/5032], Loss: 9.0774, Average Loss: 9.8151,Time: 1.62 seconds\n",
            "Step [170/5032], Loss: 9.1118, Average Loss: 9.7897,Time: 1.66 seconds\n",
            "Step [180/5032], Loss: 9.2553, Average Loss: 9.7598,Time: 1.68 seconds\n",
            "Step [190/5032], Loss: 8.9075, Average Loss: 9.7195,Time: 1.62 seconds\n",
            "Step [200/5032], Loss: 9.2999, Average Loss: 9.7058,Time: 1.44 seconds\n",
            "Step [210/5032], Loss: 9.8590, Average Loss: 9.6815,Time: 1.33 seconds\n",
            "Step [220/5032], Loss: 9.1128, Average Loss: 9.6612,Time: 1.49 seconds\n",
            "Step [230/5032], Loss: 9.0230, Average Loss: 9.6284,Time: 1.69 seconds\n",
            "Step [240/5032], Loss: 9.7580, Average Loss: 9.6121,Time: 1.53 seconds\n",
            "Step [250/5032], Loss: 8.9809, Average Loss: 9.5907,Time: 1.88 seconds\n",
            "Step [260/5032], Loss: 9.6567, Average Loss: 9.5797,Time: 1.78 seconds\n",
            "Step [270/5032], Loss: 9.3665, Average Loss: 9.5609,Time: 1.56 seconds\n",
            "Step [280/5032], Loss: 9.0459, Average Loss: 9.5391,Time: 1.58 seconds\n",
            "Step [290/5032], Loss: 8.6034, Average Loss: 9.5216,Time: 1.58 seconds\n",
            "Step [300/5032], Loss: 8.9485, Average Loss: 9.5039,Time: 1.60 seconds\n",
            "Step [310/5032], Loss: 8.8352, Average Loss: 9.4865,Time: 1.82 seconds\n",
            "Step [320/5032], Loss: 8.8612, Average Loss: 9.4696,Time: 1.53 seconds\n",
            "Step [330/5032], Loss: 8.9689, Average Loss: 9.4473,Time: 1.12 seconds\n",
            "Step [340/5032], Loss: 8.9331, Average Loss: 9.4323,Time: 1.26 seconds\n",
            "Step [350/5032], Loss: 8.1466, Average Loss: 9.4140,Time: 1.37 seconds\n",
            "Step [360/5032], Loss: 8.1244, Average Loss: 9.4028,Time: 2.06 seconds\n",
            "Step [370/5032], Loss: 9.0037, Average Loss: 9.3837,Time: 2.09 seconds\n",
            "Step [380/5032], Loss: 8.0347, Average Loss: 9.3732,Time: 2.30 seconds\n",
            "Step [390/5032], Loss: 8.9316, Average Loss: 9.3584,Time: 1.42 seconds\n",
            "Step [400/5032], Loss: 8.2675, Average Loss: 9.3443,Time: 1.64 seconds\n",
            "Step [410/5032], Loss: 8.6340, Average Loss: 9.3256,Time: 1.55 seconds\n",
            "Step [420/5032], Loss: 8.9862, Average Loss: 9.3140,Time: 1.69 seconds\n",
            "Step [430/5032], Loss: 8.5133, Average Loss: 9.2984,Time: 1.41 seconds\n",
            "Step [440/5032], Loss: 8.3630, Average Loss: 9.2827,Time: 1.59 seconds\n",
            "Step [450/5032], Loss: 8.9617, Average Loss: 9.2721,Time: 1.56 seconds\n",
            "Step [460/5032], Loss: 8.6615, Average Loss: 9.2575,Time: 1.58 seconds\n",
            "Step [470/5032], Loss: 7.8444, Average Loss: 9.2409,Time: 1.37 seconds\n",
            "Step [480/5032], Loss: 8.7125, Average Loss: 9.2261,Time: 1.72 seconds\n",
            "Step [490/5032], Loss: 7.3720, Average Loss: 9.2117,Time: 1.33 seconds\n",
            "Step [500/5032], Loss: 9.2436, Average Loss: 9.2024,Time: 1.34 seconds\n",
            "Step [510/5032], Loss: 8.7279, Average Loss: 9.1910,Time: 1.82 seconds\n",
            "Step [520/5032], Loss: 8.6990, Average Loss: 9.1780,Time: 1.71 seconds\n",
            "Step [530/5032], Loss: 10.5007, Average Loss: 9.1686,Time: 1.50 seconds\n",
            "Step [540/5032], Loss: 8.1195, Average Loss: 9.1521,Time: 1.64 seconds\n",
            "Step [550/5032], Loss: 8.2217, Average Loss: 9.1409,Time: 1.66 seconds\n",
            "Step [560/5032], Loss: 8.5628, Average Loss: 9.1281,Time: 1.40 seconds\n",
            "Step [570/5032], Loss: 8.0724, Average Loss: 9.1128,Time: 1.25 seconds\n",
            "Step [580/5032], Loss: 9.0280, Average Loss: 9.0980,Time: 1.22 seconds\n",
            "Step [590/5032], Loss: 9.2638, Average Loss: 9.0861,Time: 1.31 seconds\n",
            "Step [600/5032], Loss: 8.5064, Average Loss: 9.0720,Time: 1.81 seconds\n",
            "Step [610/5032], Loss: 8.1755, Average Loss: 9.0639,Time: 1.51 seconds\n",
            "Step [620/5032], Loss: 9.2627, Average Loss: 9.0585,Time: 1.62 seconds\n",
            "Step [630/5032], Loss: 7.7571, Average Loss: 9.0500,Time: 2.20 seconds\n",
            "Step [640/5032], Loss: 8.6249, Average Loss: 9.0393,Time: 1.60 seconds\n",
            "Step [650/5032], Loss: 9.1653, Average Loss: 9.0305,Time: 1.47 seconds\n",
            "Step [660/5032], Loss: 7.9895, Average Loss: 9.0213,Time: 1.43 seconds\n",
            "Step [670/5032], Loss: 8.4241, Average Loss: 9.0124,Time: 1.48 seconds\n",
            "Step [680/5032], Loss: 8.2687, Average Loss: 8.9999,Time: 1.53 seconds\n",
            "Step [690/5032], Loss: 8.4935, Average Loss: 8.9898,Time: 2.06 seconds\n",
            "Step [700/5032], Loss: 8.4345, Average Loss: 8.9826,Time: 1.40 seconds\n",
            "Step [710/5032], Loss: 7.8327, Average Loss: 8.9745,Time: 1.66 seconds\n",
            "Step [720/5032], Loss: 8.3548, Average Loss: 8.9664,Time: 1.48 seconds\n",
            "Step [730/5032], Loss: 8.8338, Average Loss: 8.9571,Time: 1.61 seconds\n",
            "Step [740/5032], Loss: 8.8038, Average Loss: 8.9461,Time: 1.42 seconds\n",
            "Step [750/5032], Loss: 8.2653, Average Loss: 8.9378,Time: 1.71 seconds\n",
            "Step [760/5032], Loss: 8.4577, Average Loss: 8.9270,Time: 1.44 seconds\n",
            "Step [770/5032], Loss: 8.0978, Average Loss: 8.9212,Time: 1.46 seconds\n",
            "Step [780/5032], Loss: 7.8002, Average Loss: 8.9108,Time: 1.35 seconds\n",
            "Step [790/5032], Loss: 8.0882, Average Loss: 8.9028,Time: 1.95 seconds\n",
            "Step [800/5032], Loss: 8.5664, Average Loss: 8.8941,Time: 1.78 seconds\n",
            "Step [810/5032], Loss: 8.1001, Average Loss: 8.8874,Time: 1.47 seconds\n",
            "Step [820/5032], Loss: 8.2790, Average Loss: 8.8802,Time: 1.34 seconds\n",
            "Step [830/5032], Loss: 8.2924, Average Loss: 8.8685,Time: 1.37 seconds\n",
            "Step [840/5032], Loss: 8.4097, Average Loss: 8.8611,Time: 1.69 seconds\n",
            "Step [850/5032], Loss: 7.7893, Average Loss: 8.8537,Time: 1.80 seconds\n",
            "Step [860/5032], Loss: 8.2628, Average Loss: 8.8492,Time: 1.79 seconds\n",
            "Step [870/5032], Loss: 8.5313, Average Loss: 8.8398,Time: 1.58 seconds\n",
            "Step [880/5032], Loss: 8.2689, Average Loss: 8.8294,Time: 1.59 seconds\n",
            "Step [890/5032], Loss: 8.3186, Average Loss: 8.8222,Time: 1.78 seconds\n",
            "Step [900/5032], Loss: 8.6426, Average Loss: 8.8141,Time: 1.64 seconds\n",
            "Step [910/5032], Loss: 7.3347, Average Loss: 8.8058,Time: 1.76 seconds\n",
            "Step [920/5032], Loss: 8.1657, Average Loss: 8.7970,Time: 1.48 seconds\n",
            "Step [930/5032], Loss: 7.0960, Average Loss: 8.7932,Time: 1.84 seconds\n",
            "Step [940/5032], Loss: 8.5086, Average Loss: 8.7872,Time: 2.13 seconds\n",
            "Step [950/5032], Loss: 8.0706, Average Loss: 8.7810,Time: 1.73 seconds\n",
            "Step [960/5032], Loss: 7.4530, Average Loss: 8.7748,Time: 1.52 seconds\n",
            "Step [970/5032], Loss: 7.9779, Average Loss: 8.7672,Time: 1.66 seconds\n",
            "Step [980/5032], Loss: 7.9897, Average Loss: 8.7629,Time: 1.78 seconds\n",
            "Step [990/5032], Loss: 7.5902, Average Loss: 8.7544,Time: 1.45 seconds\n",
            "Step [1000/5032], Loss: 8.4598, Average Loss: 8.7489,Time: 1.87 seconds\n",
            "Step [1010/5032], Loss: 7.9169, Average Loss: 8.7394,Time: 1.68 seconds\n",
            "Step [1020/5032], Loss: 7.2205, Average Loss: 8.7337,Time: 1.70 seconds\n",
            "Step [1030/5032], Loss: 7.3558, Average Loss: 8.7243,Time: 1.33 seconds\n",
            "Step [1040/5032], Loss: 8.2744, Average Loss: 8.7170,Time: 1.68 seconds\n",
            "Step [1050/5032], Loss: 7.9964, Average Loss: 8.7116,Time: 1.76 seconds\n",
            "Step [1060/5032], Loss: 7.8790, Average Loss: 8.7053,Time: 1.79 seconds\n",
            "Step [1070/5032], Loss: 8.5483, Average Loss: 8.6995,Time: 1.44 seconds\n",
            "Step [1080/5032], Loss: 8.2277, Average Loss: 8.6923,Time: 1.34 seconds\n",
            "Step [1090/5032], Loss: 8.0962, Average Loss: 8.6847,Time: 2.09 seconds\n",
            "Step [1100/5032], Loss: 7.1736, Average Loss: 8.6764,Time: 1.46 seconds\n",
            "Step [1110/5032], Loss: 7.7869, Average Loss: 8.6683,Time: 1.48 seconds\n",
            "Step [1120/5032], Loss: 7.8867, Average Loss: 8.6624,Time: 1.63 seconds\n",
            "Step [1130/5032], Loss: 9.0965, Average Loss: 8.6566,Time: 1.59 seconds\n",
            "Step [1140/5032], Loss: 8.2747, Average Loss: 8.6508,Time: 1.67 seconds\n",
            "Step [1150/5032], Loss: 7.6974, Average Loss: 8.6438,Time: 1.62 seconds\n",
            "Step [1160/5032], Loss: 7.1556, Average Loss: 8.6384,Time: 1.72 seconds\n",
            "Step [1170/5032], Loss: 7.3974, Average Loss: 8.6319,Time: 1.58 seconds\n",
            "Step [1180/5032], Loss: 8.1377, Average Loss: 8.6247,Time: 1.54 seconds\n",
            "Step [1190/5032], Loss: 7.5313, Average Loss: 8.6178,Time: 1.59 seconds\n",
            "Step [1200/5032], Loss: 7.5119, Average Loss: 8.6088,Time: 1.82 seconds\n",
            "Step [1210/5032], Loss: 7.9678, Average Loss: 8.6035,Time: 1.54 seconds\n",
            "Step [1220/5032], Loss: 8.3137, Average Loss: 8.5961,Time: 1.42 seconds\n",
            "Step [1230/5032], Loss: 8.2337, Average Loss: 8.5884,Time: 1.59 seconds\n",
            "Step [1240/5032], Loss: 7.9065, Average Loss: 8.5819,Time: 2.14 seconds\n",
            "Step [1250/5032], Loss: 8.0356, Average Loss: 8.5759,Time: 1.53 seconds\n",
            "Step [1260/5032], Loss: 7.0947, Average Loss: 8.5696,Time: 1.72 seconds\n",
            "Step [1270/5032], Loss: 7.2713, Average Loss: 8.5648,Time: 1.50 seconds\n",
            "Step [1280/5032], Loss: 8.5336, Average Loss: 8.5572,Time: 1.41 seconds\n",
            "Step [1290/5032], Loss: 7.5912, Average Loss: 8.5507,Time: 1.36 seconds\n",
            "Step [1300/5032], Loss: 7.7973, Average Loss: 8.5467,Time: 2.61 seconds\n",
            "Step [1310/5032], Loss: 7.4322, Average Loss: 8.5412,Time: 1.70 seconds\n",
            "Step [1320/5032], Loss: 7.7824, Average Loss: 8.5337,Time: 1.65 seconds\n",
            "Step [1330/5032], Loss: 7.7976, Average Loss: 8.5262,Time: 1.31 seconds\n",
            "Step [1340/5032], Loss: 7.7348, Average Loss: 8.5191,Time: 1.71 seconds\n",
            "Step [1350/5032], Loss: 7.2339, Average Loss: 8.5133,Time: 1.87 seconds\n",
            "Step [1360/5032], Loss: 10.5840, Average Loss: 8.5092,Time: 1.68 seconds\n",
            "Step [1370/5032], Loss: 7.5736, Average Loss: 8.5007,Time: 1.47 seconds\n",
            "Step [1380/5032], Loss: 7.7236, Average Loss: 8.4925,Time: 1.26 seconds\n",
            "Step [1390/5032], Loss: 7.2155, Average Loss: 8.4894,Time: 1.24 seconds\n",
            "Step [1400/5032], Loss: 7.3215, Average Loss: 8.4841,Time: 1.55 seconds\n",
            "Step [1410/5032], Loss: 8.0851, Average Loss: 8.4776,Time: 1.63 seconds\n",
            "Step [1420/5032], Loss: 6.9564, Average Loss: 8.4707,Time: 1.99 seconds\n",
            "Step [1430/5032], Loss: 7.7362, Average Loss: 8.4663,Time: 2.21 seconds\n",
            "Step [1440/5032], Loss: 7.9985, Average Loss: 8.4595,Time: 1.34 seconds\n",
            "Step [1450/5032], Loss: 8.3769, Average Loss: 8.4531,Time: 1.51 seconds\n",
            "Step [1460/5032], Loss: 7.4510, Average Loss: 8.4493,Time: 1.42 seconds\n",
            "Step [1470/5032], Loss: 7.5768, Average Loss: 8.4435,Time: 1.19 seconds\n",
            "Step [1480/5032], Loss: 7.5923, Average Loss: 8.4354,Time: 1.58 seconds\n",
            "Step [1490/5032], Loss: 7.6186, Average Loss: 8.4311,Time: 1.58 seconds\n",
            "Step [1500/5032], Loss: 7.5144, Average Loss: 8.4242,Time: 1.38 seconds\n",
            "Step [1510/5032], Loss: 7.8177, Average Loss: 8.4202,Time: 1.85 seconds\n",
            "Step [1520/5032], Loss: 7.3386, Average Loss: 8.4134,Time: 1.34 seconds\n",
            "Step [1530/5032], Loss: 7.1016, Average Loss: 8.4093,Time: 1.92 seconds\n",
            "Step [1540/5032], Loss: 5.9868, Average Loss: 8.4024,Time: 1.66 seconds\n",
            "Step [1550/5032], Loss: 8.0866, Average Loss: 8.3994,Time: 1.83 seconds\n",
            "Step [1560/5032], Loss: 7.1295, Average Loss: 8.3918,Time: 1.30 seconds\n",
            "Step [1570/5032], Loss: 6.9846, Average Loss: 8.3862,Time: 1.79 seconds\n",
            "Step [1580/5032], Loss: 8.1007, Average Loss: 8.3809,Time: 1.61 seconds\n",
            "Step [1590/5032], Loss: 6.7234, Average Loss: 8.3744,Time: 1.20 seconds\n",
            "Step [1600/5032], Loss: 7.9613, Average Loss: 8.3698,Time: 1.42 seconds\n",
            "Step [1610/5032], Loss: 8.0769, Average Loss: 8.3646,Time: 1.77 seconds\n",
            "Step [1620/5032], Loss: 7.7810, Average Loss: 8.3606,Time: 1.53 seconds\n",
            "Step [1630/5032], Loss: 7.5806, Average Loss: 8.3547,Time: 1.88 seconds\n",
            "Step [1640/5032], Loss: 7.5865, Average Loss: 8.3493,Time: 1.73 seconds\n",
            "Step [1650/5032], Loss: 8.8620, Average Loss: 8.3454,Time: 1.22 seconds\n",
            "Step [1660/5032], Loss: 7.3670, Average Loss: 8.3396,Time: 1.15 seconds\n",
            "Step [1670/5032], Loss: 7.2800, Average Loss: 8.3359,Time: 1.60 seconds\n",
            "Step [1680/5032], Loss: 7.2800, Average Loss: 8.3295,Time: 1.44 seconds\n",
            "Step [1690/5032], Loss: 7.2538, Average Loss: 8.3245,Time: 1.33 seconds\n",
            "Step [1700/5032], Loss: 7.6220, Average Loss: 8.3176,Time: 1.38 seconds\n",
            "Step [1710/5032], Loss: 7.7641, Average Loss: 8.3124,Time: 1.77 seconds\n",
            "Step [1720/5032], Loss: 7.9232, Average Loss: 8.3078,Time: 1.37 seconds\n",
            "Step [1730/5032], Loss: 7.6472, Average Loss: 8.3026,Time: 1.53 seconds\n",
            "Step [1740/5032], Loss: 6.8926, Average Loss: 8.2963,Time: 1.40 seconds\n",
            "Step [1750/5032], Loss: 7.3499, Average Loss: 8.2922,Time: 1.82 seconds\n",
            "Step [1760/5032], Loss: 7.1261, Average Loss: 8.2884,Time: 1.86 seconds\n",
            "Step [1770/5032], Loss: 7.3833, Average Loss: 8.2846,Time: 1.33 seconds\n",
            "Step [1780/5032], Loss: 8.7009, Average Loss: 8.2821,Time: 3.28 seconds\n",
            "Step [1790/5032], Loss: 8.0091, Average Loss: 8.2785,Time: 1.78 seconds\n",
            "Step [1800/5032], Loss: 5.7814, Average Loss: 8.2726,Time: 1.26 seconds\n",
            "Step [1810/5032], Loss: 7.7364, Average Loss: 8.2667,Time: 1.69 seconds\n",
            "Step [1820/5032], Loss: 6.3076, Average Loss: 8.2613,Time: 1.39 seconds\n",
            "Step [1830/5032], Loss: 7.5235, Average Loss: 8.2570,Time: 1.57 seconds\n",
            "Step [1840/5032], Loss: 6.3222, Average Loss: 8.2511,Time: 1.55 seconds\n",
            "Step [1850/5032], Loss: 7.0904, Average Loss: 8.2454,Time: 1.48 seconds\n",
            "Step [1860/5032], Loss: 7.6660, Average Loss: 8.2402,Time: 1.57 seconds\n",
            "Step [1870/5032], Loss: 8.2387, Average Loss: 8.2358,Time: 1.42 seconds\n",
            "Step [1880/5032], Loss: 6.6103, Average Loss: 8.2317,Time: 1.32 seconds\n",
            "Step [1890/5032], Loss: 8.6869, Average Loss: 8.2280,Time: 1.46 seconds\n",
            "Step [1900/5032], Loss: 7.8486, Average Loss: 8.2238,Time: 2.05 seconds\n",
            "Step [1910/5032], Loss: 7.4229, Average Loss: 8.2207,Time: 1.48 seconds\n",
            "Step [1920/5032], Loss: 7.2690, Average Loss: 8.2167,Time: 1.82 seconds\n",
            "Step [1930/5032], Loss: 7.4256, Average Loss: 8.2122,Time: 1.88 seconds\n",
            "Step [1940/5032], Loss: 6.4958, Average Loss: 8.2073,Time: 1.61 seconds\n",
            "Step [1950/5032], Loss: 9.1630, Average Loss: 8.2052,Time: 2.13 seconds\n",
            "Step [1960/5032], Loss: 7.1674, Average Loss: 8.2007,Time: 1.43 seconds\n",
            "Step [1970/5032], Loss: 7.5522, Average Loss: 8.1951,Time: 1.89 seconds\n",
            "Step [1980/5032], Loss: 6.8506, Average Loss: 8.1905,Time: 1.44 seconds\n",
            "Step [1990/5032], Loss: 7.1242, Average Loss: 8.1862,Time: 1.53 seconds\n",
            "Step [2000/5032], Loss: 7.1139, Average Loss: 8.1818,Time: 1.80 seconds\n",
            "Step [2010/5032], Loss: 7.4880, Average Loss: 8.1762,Time: 1.29 seconds\n",
            "Step [2020/5032], Loss: 7.1845, Average Loss: 8.1720,Time: 1.42 seconds\n",
            "Step [2030/5032], Loss: 6.9316, Average Loss: 8.1679,Time: 1.87 seconds\n",
            "Step [2040/5032], Loss: 7.3795, Average Loss: 8.1658,Time: 1.66 seconds\n",
            "Step [2050/5032], Loss: 8.1830, Average Loss: 8.1611,Time: 1.58 seconds\n",
            "Step [2060/5032], Loss: 6.9882, Average Loss: 8.1576,Time: 1.78 seconds\n",
            "Step [2070/5032], Loss: 7.5187, Average Loss: 8.1532,Time: 1.29 seconds\n",
            "Step [2080/5032], Loss: 7.2645, Average Loss: 8.1488,Time: 1.77 seconds\n",
            "Step [2090/5032], Loss: 7.3947, Average Loss: 8.1446,Time: 1.63 seconds\n",
            "Step [2100/5032], Loss: 7.0146, Average Loss: 8.1390,Time: 1.72 seconds\n",
            "Step [2110/5032], Loss: 6.9599, Average Loss: 8.1340,Time: 1.39 seconds\n",
            "Step [2120/5032], Loss: 7.0147, Average Loss: 8.1296,Time: 1.53 seconds\n",
            "Step [2130/5032], Loss: 7.5952, Average Loss: 8.1252,Time: 1.65 seconds\n",
            "Step [2140/5032], Loss: 7.3849, Average Loss: 8.1207,Time: 1.60 seconds\n",
            "Step [2150/5032], Loss: 7.3313, Average Loss: 8.1166,Time: 1.45 seconds\n",
            "Step [2160/5032], Loss: 7.3963, Average Loss: 8.1134,Time: 1.71 seconds\n",
            "Step [2170/5032], Loss: 7.5597, Average Loss: 8.1102,Time: 1.83 seconds\n",
            "Step [2180/5032], Loss: 7.3951, Average Loss: 8.1049,Time: 1.73 seconds\n",
            "Step [2190/5032], Loss: 6.9321, Average Loss: 8.1018,Time: 2.12 seconds\n",
            "Step [2200/5032], Loss: 6.8031, Average Loss: 8.0974,Time: 1.61 seconds\n",
            "Step [2210/5032], Loss: 6.4441, Average Loss: 8.0917,Time: 1.43 seconds\n",
            "Step [2220/5032], Loss: 7.3446, Average Loss: 8.0864,Time: 1.48 seconds\n",
            "Step [2230/5032], Loss: 7.5103, Average Loss: 8.0812,Time: 1.59 seconds\n",
            "Step [2240/5032], Loss: 7.6314, Average Loss: 8.0779,Time: 1.74 seconds\n",
            "Step [2250/5032], Loss: 7.0322, Average Loss: 8.0743,Time: 1.68 seconds\n",
            "Step [2260/5032], Loss: 6.9584, Average Loss: 8.0706,Time: 1.67 seconds\n",
            "Step [2270/5032], Loss: 6.1049, Average Loss: 8.0656,Time: 1.74 seconds\n",
            "Step [2280/5032], Loss: 6.5671, Average Loss: 8.0609,Time: 1.62 seconds\n",
            "Step [2290/5032], Loss: 6.2365, Average Loss: 8.0568,Time: 1.80 seconds\n",
            "Step [2300/5032], Loss: 7.6571, Average Loss: 8.0527,Time: 1.48 seconds\n",
            "Step [2310/5032], Loss: 8.2761, Average Loss: 8.0501,Time: 1.44 seconds\n",
            "Step [2320/5032], Loss: 8.1374, Average Loss: 8.0472,Time: 1.46 seconds\n",
            "Step [2330/5032], Loss: 7.3170, Average Loss: 8.0433,Time: 1.56 seconds\n",
            "Step [2340/5032], Loss: 6.6658, Average Loss: 8.0402,Time: 1.59 seconds\n",
            "Step [2350/5032], Loss: 7.5037, Average Loss: 8.0373,Time: 1.30 seconds\n",
            "Step [2360/5032], Loss: 7.8144, Average Loss: 8.0336,Time: 1.74 seconds\n",
            "Step [2370/5032], Loss: 6.8316, Average Loss: 8.0305,Time: 1.44 seconds\n",
            "Step [2380/5032], Loss: 6.3801, Average Loss: 8.0273,Time: 2.02 seconds\n",
            "Step [2390/5032], Loss: 7.4496, Average Loss: 8.0233,Time: 1.68 seconds\n",
            "Step [2400/5032], Loss: 6.4153, Average Loss: 8.0200,Time: 2.07 seconds\n",
            "Step [2410/5032], Loss: 6.6415, Average Loss: 8.0160,Time: 1.78 seconds\n",
            "Step [2420/5032], Loss: 6.8211, Average Loss: 8.0130,Time: 2.01 seconds\n",
            "Step [2430/5032], Loss: 7.4487, Average Loss: 8.0101,Time: 1.21 seconds\n",
            "Step [2440/5032], Loss: 7.3228, Average Loss: 8.0067,Time: 1.58 seconds\n",
            "Step [2450/5032], Loss: 7.8816, Average Loss: 8.0031,Time: 1.38 seconds\n",
            "Step [2460/5032], Loss: 7.0869, Average Loss: 7.9987,Time: 1.34 seconds\n",
            "Step [2470/5032], Loss: 7.1752, Average Loss: 7.9970,Time: 1.61 seconds\n",
            "Step [2480/5032], Loss: 7.9624, Average Loss: 7.9929,Time: 1.59 seconds\n",
            "Step [2490/5032], Loss: 7.5201, Average Loss: 7.9899,Time: 1.52 seconds\n",
            "Step [2500/5032], Loss: 6.7267, Average Loss: 7.9861,Time: 1.56 seconds\n",
            "Step [2510/5032], Loss: 6.1675, Average Loss: 7.9829,Time: 1.50 seconds\n",
            "Step [2520/5032], Loss: 8.2670, Average Loss: 7.9794,Time: 1.86 seconds\n",
            "Step [2530/5032], Loss: 7.6475, Average Loss: 7.9767,Time: 1.95 seconds\n",
            "Step [2540/5032], Loss: 7.0048, Average Loss: 7.9735,Time: 1.72 seconds\n",
            "Step [2550/5032], Loss: 7.4922, Average Loss: 7.9685,Time: 1.95 seconds\n",
            "Step [2560/5032], Loss: 6.0276, Average Loss: 7.9632,Time: 1.70 seconds\n",
            "Step [2570/5032], Loss: 8.1620, Average Loss: 7.9607,Time: 1.63 seconds\n",
            "Step [2580/5032], Loss: 6.5757, Average Loss: 7.9574,Time: 1.66 seconds\n",
            "Step [2590/5032], Loss: 7.1596, Average Loss: 7.9541,Time: 1.70 seconds\n",
            "Step [2600/5032], Loss: 5.9128, Average Loss: 7.9501,Time: 1.39 seconds\n",
            "Step [2610/5032], Loss: 7.0187, Average Loss: 7.9471,Time: 1.87 seconds\n",
            "Step [2620/5032], Loss: 7.3955, Average Loss: 7.9439,Time: 1.58 seconds\n",
            "Step [2630/5032], Loss: 7.5297, Average Loss: 7.9409,Time: 1.51 seconds\n",
            "Step [2640/5032], Loss: 7.0364, Average Loss: 7.9382,Time: 1.49 seconds\n",
            "Step [2650/5032], Loss: 6.8346, Average Loss: 7.9350,Time: 1.32 seconds\n",
            "Step [2660/5032], Loss: 6.0315, Average Loss: 7.9307,Time: 1.48 seconds\n",
            "Step [2670/5032], Loss: 5.9119, Average Loss: 7.9266,Time: 1.24 seconds\n",
            "Step [2680/5032], Loss: 7.2572, Average Loss: 7.9256,Time: 2.88 seconds\n",
            "Step [2690/5032], Loss: 7.1820, Average Loss: 7.9230,Time: 1.62 seconds\n",
            "Step [2700/5032], Loss: 6.9032, Average Loss: 7.9199,Time: 1.55 seconds\n",
            "Step [2710/5032], Loss: 8.5185, Average Loss: 7.9179,Time: 1.45 seconds\n",
            "Step [2720/5032], Loss: 7.0493, Average Loss: 7.9145,Time: 1.54 seconds\n",
            "Step [2730/5032], Loss: 7.3197, Average Loss: 7.9115,Time: 1.64 seconds\n",
            "Step [2740/5032], Loss: 7.3039, Average Loss: 7.9089,Time: 1.74 seconds\n",
            "Step [2750/5032], Loss: 7.4519, Average Loss: 7.9075,Time: 2.37 seconds\n",
            "Step [2760/5032], Loss: 6.0656, Average Loss: 7.9026,Time: 1.40 seconds\n",
            "Step [2770/5032], Loss: 6.5460, Average Loss: 7.8978,Time: 1.83 seconds\n",
            "Step [2780/5032], Loss: 6.5548, Average Loss: 7.8951,Time: 1.47 seconds\n",
            "Step [2790/5032], Loss: 7.9309, Average Loss: 7.8933,Time: 1.25 seconds\n",
            "Step [2800/5032], Loss: 7.3413, Average Loss: 7.8898,Time: 1.52 seconds\n",
            "Step [2810/5032], Loss: 6.9879, Average Loss: 7.8864,Time: 1.89 seconds\n",
            "Step [2820/5032], Loss: 6.4129, Average Loss: 7.8832,Time: 1.32 seconds\n",
            "Step [2830/5032], Loss: 6.7571, Average Loss: 7.8796,Time: 1.30 seconds\n",
            "Step [2840/5032], Loss: 6.9737, Average Loss: 7.8791,Time: 2.51 seconds\n",
            "Step [2850/5032], Loss: 7.4980, Average Loss: 7.8767,Time: 1.77 seconds\n",
            "Step [2860/5032], Loss: 6.1488, Average Loss: 7.8735,Time: 1.35 seconds\n",
            "Step [2870/5032], Loss: 7.2822, Average Loss: 7.8711,Time: 1.31 seconds\n",
            "Step [2880/5032], Loss: 6.9417, Average Loss: 7.8669,Time: 1.77 seconds\n",
            "Step [2890/5032], Loss: 6.0907, Average Loss: 7.8642,Time: 1.80 seconds\n",
            "Step [2900/5032], Loss: 7.2078, Average Loss: 7.8617,Time: 2.23 seconds\n",
            "Step [2910/5032], Loss: 7.5732, Average Loss: 7.8579,Time: 1.43 seconds\n",
            "Step [2920/5032], Loss: 7.3681, Average Loss: 7.8554,Time: 2.01 seconds\n",
            "Step [2930/5032], Loss: 7.0306, Average Loss: 7.8522,Time: 1.40 seconds\n",
            "Step [2940/5032], Loss: 5.8197, Average Loss: 7.8490,Time: 1.77 seconds\n",
            "Step [2950/5032], Loss: 6.7629, Average Loss: 7.8467,Time: 1.78 seconds\n",
            "Step [2960/5032], Loss: 9.5267, Average Loss: 7.8446,Time: 2.18 seconds\n",
            "Step [2970/5032], Loss: 6.8392, Average Loss: 7.8421,Time: 1.52 seconds\n",
            "Step [2980/5032], Loss: 7.6299, Average Loss: 7.8374,Time: 1.38 seconds\n",
            "Step [2990/5032], Loss: 6.3147, Average Loss: 7.8343,Time: 1.44 seconds\n",
            "Step [3000/5032], Loss: 6.8373, Average Loss: 7.8305,Time: 1.54 seconds\n",
            "Step [3010/5032], Loss: 6.0168, Average Loss: 7.8280,Time: 1.82 seconds\n",
            "Step [3020/5032], Loss: 4.8710, Average Loss: 7.8229,Time: 1.72 seconds\n",
            "Step [3030/5032], Loss: 6.5430, Average Loss: 7.8216,Time: 1.73 seconds\n",
            "Step [3040/5032], Loss: 7.9782, Average Loss: 7.8184,Time: 1.13 seconds\n",
            "Step [3050/5032], Loss: 6.9507, Average Loss: 7.8158,Time: 1.80 seconds\n",
            "Step [3060/5032], Loss: 6.9564, Average Loss: 7.8136,Time: 1.71 seconds\n",
            "Step [3070/5032], Loss: 6.9075, Average Loss: 7.8108,Time: 1.36 seconds\n",
            "Step [3080/5032], Loss: 8.3202, Average Loss: 7.8081,Time: 1.25 seconds\n",
            "Step [3090/5032], Loss: 5.9250, Average Loss: 7.8058,Time: 2.36 seconds\n",
            "Step [3100/5032], Loss: 6.4819, Average Loss: 7.8033,Time: 2.04 seconds\n",
            "Step [3110/5032], Loss: 6.6246, Average Loss: 7.8003,Time: 1.62 seconds\n",
            "Step [3120/5032], Loss: 7.5887, Average Loss: 7.7984,Time: 1.86 seconds\n",
            "Step [3130/5032], Loss: 6.2713, Average Loss: 7.7949,Time: 1.53 seconds\n",
            "Step [3140/5032], Loss: 6.6267, Average Loss: 7.7923,Time: 1.61 seconds\n",
            "Step [3150/5032], Loss: 7.2530, Average Loss: 7.7912,Time: 2.39 seconds\n",
            "Step [3160/5032], Loss: 7.1448, Average Loss: 7.7878,Time: 1.61 seconds\n",
            "Step [3170/5032], Loss: 6.7349, Average Loss: 7.7854,Time: 1.49 seconds\n",
            "Step [3180/5032], Loss: 6.4693, Average Loss: 7.7835,Time: 1.53 seconds\n",
            "Step [3190/5032], Loss: 6.9082, Average Loss: 7.7799,Time: 1.55 seconds\n",
            "Step [3200/5032], Loss: 6.7097, Average Loss: 7.7772,Time: 1.77 seconds\n",
            "Step [3210/5032], Loss: 4.3377, Average Loss: 7.7730,Time: 1.70 seconds\n",
            "Step [3220/5032], Loss: 7.3221, Average Loss: 7.7692,Time: 1.63 seconds\n",
            "Step [3230/5032], Loss: 7.2317, Average Loss: 7.7665,Time: 1.65 seconds\n",
            "Step [3240/5032], Loss: 6.4945, Average Loss: 7.7633,Time: 1.81 seconds\n",
            "Step [3250/5032], Loss: 6.3494, Average Loss: 7.7601,Time: 1.54 seconds\n",
            "Step [3260/5032], Loss: 6.8217, Average Loss: 7.7580,Time: 1.75 seconds\n",
            "Step [3270/5032], Loss: 5.6855, Average Loss: 7.7545,Time: 1.51 seconds\n",
            "Step [3280/5032], Loss: 7.5094, Average Loss: 7.7511,Time: 1.43 seconds\n",
            "Step [3290/5032], Loss: 7.2234, Average Loss: 7.7490,Time: 1.47 seconds\n",
            "Step [3300/5032], Loss: 6.8679, Average Loss: 7.7461,Time: 1.73 seconds\n",
            "Step [3310/5032], Loss: 6.0976, Average Loss: 7.7439,Time: 1.14 seconds\n",
            "Step [3320/5032], Loss: 6.1070, Average Loss: 7.7406,Time: 1.47 seconds\n",
            "Step [3330/5032], Loss: 6.4438, Average Loss: 7.7378,Time: 1.32 seconds\n",
            "Step [3340/5032], Loss: 7.1939, Average Loss: 7.7352,Time: 2.60 seconds\n",
            "Step [3350/5032], Loss: 6.5888, Average Loss: 7.7327,Time: 1.66 seconds\n",
            "Step [3360/5032], Loss: 6.8149, Average Loss: 7.7277,Time: 1.42 seconds\n",
            "Step [3370/5032], Loss: 5.5076, Average Loss: 7.7248,Time: 1.42 seconds\n",
            "Step [3380/5032], Loss: 6.6722, Average Loss: 7.7213,Time: 1.66 seconds\n",
            "Step [3390/5032], Loss: 6.6558, Average Loss: 7.7182,Time: 1.30 seconds\n",
            "Step [3400/5032], Loss: 6.7779, Average Loss: 7.7149,Time: 1.41 seconds\n",
            "Step [3410/5032], Loss: 7.5975, Average Loss: 7.7120,Time: 1.67 seconds\n",
            "Step [3420/5032], Loss: 7.4002, Average Loss: 7.7081,Time: 1.38 seconds\n",
            "Step [3430/5032], Loss: 6.8865, Average Loss: 7.7053,Time: 1.49 seconds\n",
            "Step [3440/5032], Loss: 6.7721, Average Loss: 7.7036,Time: 1.75 seconds\n",
            "Step [3450/5032], Loss: 9.1498, Average Loss: 7.7019,Time: 1.82 seconds\n",
            "Step [3460/5032], Loss: 7.0743, Average Loss: 7.6997,Time: 1.25 seconds\n",
            "Step [3470/5032], Loss: 7.3767, Average Loss: 7.6971,Time: 1.85 seconds\n",
            "Step [3480/5032], Loss: 6.4078, Average Loss: 7.6946,Time: 1.84 seconds\n",
            "Step [3490/5032], Loss: 7.7799, Average Loss: 7.6929,Time: 2.05 seconds\n",
            "Step [3500/5032], Loss: 6.8862, Average Loss: 7.6898,Time: 1.75 seconds\n",
            "Step [3510/5032], Loss: 6.1849, Average Loss: 7.6868,Time: 1.61 seconds\n",
            "Step [3520/5032], Loss: 7.2181, Average Loss: 7.6848,Time: 1.25 seconds\n",
            "Step [3530/5032], Loss: 6.9706, Average Loss: 7.6819,Time: 1.41 seconds\n",
            "Step [3540/5032], Loss: 6.3929, Average Loss: 7.6792,Time: 1.53 seconds\n",
            "Step [3550/5032], Loss: 6.9684, Average Loss: 7.6766,Time: 1.97 seconds\n",
            "Step [3560/5032], Loss: 6.7189, Average Loss: 7.6739,Time: 1.62 seconds\n",
            "Step [3570/5032], Loss: 7.6751, Average Loss: 7.6713,Time: 1.42 seconds\n",
            "Step [3580/5032], Loss: 6.4649, Average Loss: 7.6689,Time: 1.15 seconds\n",
            "Step [3590/5032], Loss: 7.8219, Average Loss: 7.6671,Time: 1.48 seconds\n",
            "Step [3600/5032], Loss: 6.4811, Average Loss: 7.6644,Time: 1.64 seconds\n",
            "Step [3610/5032], Loss: 7.1577, Average Loss: 7.6616,Time: 1.72 seconds\n",
            "Step [3620/5032], Loss: 6.0203, Average Loss: 7.6592,Time: 1.92 seconds\n",
            "Step [3630/5032], Loss: 6.3539, Average Loss: 7.6564,Time: 1.60 seconds\n",
            "Step [3640/5032], Loss: 6.4369, Average Loss: 7.6531,Time: 1.58 seconds\n",
            "Step [3650/5032], Loss: 7.6721, Average Loss: 7.6496,Time: 1.44 seconds\n",
            "Step [3660/5032], Loss: 5.1425, Average Loss: 7.6469,Time: 1.77 seconds\n",
            "Step [3670/5032], Loss: 6.3637, Average Loss: 7.6445,Time: 1.38 seconds\n",
            "Step [3680/5032], Loss: 6.6668, Average Loss: 7.6417,Time: 1.66 seconds\n",
            "Step [3690/5032], Loss: 7.4883, Average Loss: 7.6392,Time: 1.53 seconds\n",
            "Step [3700/5032], Loss: 5.9535, Average Loss: 7.6362,Time: 1.40 seconds\n",
            "Step [3710/5032], Loss: 7.0347, Average Loss: 7.6344,Time: 1.53 seconds\n",
            "Step [3720/5032], Loss: 5.9080, Average Loss: 7.6325,Time: 1.46 seconds\n",
            "Step [3730/5032], Loss: 5.7355, Average Loss: 7.6301,Time: 1.46 seconds\n",
            "Step [3740/5032], Loss: 5.7178, Average Loss: 7.6270,Time: 1.77 seconds\n",
            "Step [3750/5032], Loss: 6.5603, Average Loss: 7.6248,Time: 1.46 seconds\n",
            "Step [3760/5032], Loss: 6.1736, Average Loss: 7.6215,Time: 2.03 seconds\n",
            "Step [3770/5032], Loss: 6.6488, Average Loss: 7.6178,Time: 1.72 seconds\n",
            "Step [3780/5032], Loss: 6.3288, Average Loss: 7.6157,Time: 1.35 seconds\n",
            "Step [3790/5032], Loss: 7.1317, Average Loss: 7.6136,Time: 1.53 seconds\n",
            "Step [3800/5032], Loss: 5.6348, Average Loss: 7.6105,Time: 1.52 seconds\n",
            "Step [3810/5032], Loss: 7.2501, Average Loss: 7.6086,Time: 1.68 seconds\n",
            "Step [3820/5032], Loss: 6.1047, Average Loss: 7.6064,Time: 1.54 seconds\n",
            "Step [3830/5032], Loss: 7.2359, Average Loss: 7.6040,Time: 2.06 seconds\n",
            "Step [3840/5032], Loss: 8.0005, Average Loss: 7.6016,Time: 1.73 seconds\n",
            "Step [3850/5032], Loss: 6.7078, Average Loss: 7.5995,Time: 2.25 seconds\n",
            "Step [3860/5032], Loss: 7.6299, Average Loss: 7.5964,Time: 1.53 seconds\n",
            "Step [3870/5032], Loss: 6.7567, Average Loss: 7.5933,Time: 1.37 seconds\n",
            "Step [3880/5032], Loss: 5.6243, Average Loss: 7.5899,Time: 1.29 seconds\n",
            "Step [3890/5032], Loss: 6.5290, Average Loss: 7.5878,Time: 1.46 seconds\n",
            "Step [3900/5032], Loss: 6.8467, Average Loss: 7.5859,Time: 1.73 seconds\n",
            "Step [3910/5032], Loss: 6.4498, Average Loss: 7.5840,Time: 1.65 seconds\n",
            "Step [3920/5032], Loss: 5.7763, Average Loss: 7.5813,Time: 1.97 seconds\n",
            "Step [3930/5032], Loss: 6.5855, Average Loss: 7.5779,Time: 1.43 seconds\n",
            "Step [3940/5032], Loss: 6.8497, Average Loss: 7.5744,Time: 1.56 seconds\n",
            "Step [3950/5032], Loss: 6.7742, Average Loss: 7.5724,Time: 1.44 seconds\n",
            "Step [3960/5032], Loss: 7.2919, Average Loss: 7.5703,Time: 1.62 seconds\n",
            "Step [3970/5032], Loss: 6.0793, Average Loss: 7.5683,Time: 2.11 seconds\n",
            "Step [3980/5032], Loss: 6.5562, Average Loss: 7.5661,Time: 1.67 seconds\n",
            "Step [3990/5032], Loss: 6.6416, Average Loss: 7.5630,Time: 1.78 seconds\n",
            "Step [4000/5032], Loss: 5.8805, Average Loss: 7.5605,Time: 2.63 seconds\n",
            "Step [4010/5032], Loss: 6.1259, Average Loss: 7.5586,Time: 1.43 seconds\n",
            "Step [4020/5032], Loss: 6.3254, Average Loss: 7.5558,Time: 1.61 seconds\n",
            "Step [4030/5032], Loss: 4.2391, Average Loss: 7.5531,Time: 1.47 seconds\n",
            "Step [4040/5032], Loss: 8.8929, Average Loss: 7.5520,Time: 4.79 seconds\n",
            "Step [4050/5032], Loss: 7.2694, Average Loss: 7.5496,Time: 1.87 seconds\n",
            "Step [4060/5032], Loss: 6.3467, Average Loss: 7.5470,Time: 1.50 seconds\n",
            "Step [4070/5032], Loss: 7.8797, Average Loss: 7.5454,Time: 1.91 seconds\n",
            "Step [4080/5032], Loss: 5.7449, Average Loss: 7.5430,Time: 1.84 seconds\n",
            "Step [4090/5032], Loss: 7.1144, Average Loss: 7.5411,Time: 1.45 seconds\n",
            "Step [4100/5032], Loss: 6.7138, Average Loss: 7.5371,Time: 1.66 seconds\n",
            "Step [4110/5032], Loss: 6.4205, Average Loss: 7.5347,Time: 1.31 seconds\n",
            "Step [4120/5032], Loss: 6.4915, Average Loss: 7.5328,Time: 1.51 seconds\n",
            "Step [4130/5032], Loss: 6.9764, Average Loss: 7.5310,Time: 1.20 seconds\n",
            "Step [4140/5032], Loss: 5.0416, Average Loss: 7.5288,Time: 1.90 seconds\n",
            "Step [4150/5032], Loss: 7.0241, Average Loss: 7.5260,Time: 1.59 seconds\n",
            "Step [4160/5032], Loss: 6.5072, Average Loss: 7.5233,Time: 1.85 seconds\n",
            "Step [4170/5032], Loss: 5.7639, Average Loss: 7.5208,Time: 1.63 seconds\n",
            "Step [4180/5032], Loss: 6.5517, Average Loss: 7.5186,Time: 1.42 seconds\n",
            "Step [4190/5032], Loss: 5.2872, Average Loss: 7.5168,Time: 1.70 seconds\n",
            "Step [4200/5032], Loss: 6.4098, Average Loss: 7.5139,Time: 1.45 seconds\n",
            "Step [4210/5032], Loss: 5.0967, Average Loss: 7.5104,Time: 1.61 seconds\n",
            "Step [4220/5032], Loss: 6.3010, Average Loss: 7.5080,Time: 2.00 seconds\n",
            "Step [4230/5032], Loss: 5.2379, Average Loss: 7.5053,Time: 2.23 seconds\n",
            "Step [4240/5032], Loss: 6.5646, Average Loss: 7.5031,Time: 1.62 seconds\n",
            "Step [4250/5032], Loss: 7.2875, Average Loss: 7.5011,Time: 1.70 seconds\n",
            "Step [4260/5032], Loss: 5.6722, Average Loss: 7.4982,Time: 1.62 seconds\n",
            "Step [4270/5032], Loss: 6.9279, Average Loss: 7.4962,Time: 1.71 seconds\n",
            "Step [4280/5032], Loss: 6.6000, Average Loss: 7.4938,Time: 1.54 seconds\n",
            "Step [4290/5032], Loss: 7.2920, Average Loss: 7.4920,Time: 1.54 seconds\n",
            "Step [4300/5032], Loss: 6.7024, Average Loss: 7.4891,Time: 1.67 seconds\n",
            "Step [4310/5032], Loss: 6.1641, Average Loss: 7.4874,Time: 1.48 seconds\n",
            "Step [4320/5032], Loss: 8.6543, Average Loss: 7.4858,Time: 1.57 seconds\n",
            "Step [4330/5032], Loss: 6.5654, Average Loss: 7.4831,Time: 1.42 seconds\n",
            "Step [4340/5032], Loss: 6.3069, Average Loss: 7.4815,Time: 1.75 seconds\n",
            "Step [4350/5032], Loss: 5.8851, Average Loss: 7.4792,Time: 1.33 seconds\n",
            "Step [4360/5032], Loss: 6.7355, Average Loss: 7.4769,Time: 1.70 seconds\n",
            "Step [4370/5032], Loss: 7.7379, Average Loss: 7.4749,Time: 1.71 seconds\n",
            "Step [4380/5032], Loss: 5.8051, Average Loss: 7.4723,Time: 1.10 seconds\n",
            "Step [4390/5032], Loss: 6.7377, Average Loss: 7.4706,Time: 1.37 seconds\n",
            "Step [4400/5032], Loss: 6.5579, Average Loss: 7.4689,Time: 1.68 seconds\n",
            "Step [4410/5032], Loss: 7.3637, Average Loss: 7.4675,Time: 1.57 seconds\n",
            "Step [4420/5032], Loss: 5.7000, Average Loss: 7.4659,Time: 1.49 seconds\n",
            "Step [4430/5032], Loss: 6.8866, Average Loss: 7.4630,Time: 1.64 seconds\n",
            "Step [4440/5032], Loss: 6.5091, Average Loss: 7.4602,Time: 1.61 seconds\n",
            "Step [4450/5032], Loss: 6.6597, Average Loss: 7.4585,Time: 1.76 seconds\n",
            "Step [4460/5032], Loss: 6.4424, Average Loss: 7.4575,Time: 2.13 seconds\n",
            "Step [4470/5032], Loss: 7.2751, Average Loss: 7.4557,Time: 1.67 seconds\n",
            "Step [4480/5032], Loss: 6.3233, Average Loss: 7.4529,Time: 1.50 seconds\n",
            "Step [4490/5032], Loss: 7.2610, Average Loss: 7.4499,Time: 1.35 seconds\n",
            "Step [4500/5032], Loss: 5.9876, Average Loss: 7.4477,Time: 1.81 seconds\n",
            "Step [4510/5032], Loss: 6.6932, Average Loss: 7.4477,Time: 2.53 seconds\n",
            "Step [4520/5032], Loss: 6.5344, Average Loss: 7.4452,Time: 1.34 seconds\n",
            "Step [4530/5032], Loss: 4.5382, Average Loss: 7.4418,Time: 1.28 seconds\n",
            "Step [4540/5032], Loss: 6.6954, Average Loss: 7.4389,Time: 1.40 seconds\n",
            "Step [4550/5032], Loss: 6.1408, Average Loss: 7.4363,Time: 1.80 seconds\n",
            "Step [4560/5032], Loss: 5.5650, Average Loss: 7.4350,Time: 1.63 seconds\n",
            "Step [4570/5032], Loss: 7.1021, Average Loss: 7.4324,Time: 1.50 seconds\n",
            "Step [4580/5032], Loss: 6.2649, Average Loss: 7.4298,Time: 1.46 seconds\n",
            "Step [4590/5032], Loss: 6.3221, Average Loss: 7.4274,Time: 1.46 seconds\n",
            "Step [4600/5032], Loss: 7.5274, Average Loss: 7.4259,Time: 1.96 seconds\n",
            "Step [4610/5032], Loss: 5.3569, Average Loss: 7.4235,Time: 2.29 seconds\n",
            "Step [4620/5032], Loss: 6.9033, Average Loss: 7.4220,Time: 1.81 seconds\n",
            "Step [4630/5032], Loss: 7.0946, Average Loss: 7.4197,Time: 1.36 seconds\n",
            "Step [4640/5032], Loss: 6.6499, Average Loss: 7.4173,Time: 1.51 seconds\n",
            "Step [4650/5032], Loss: 6.3262, Average Loss: 7.4150,Time: 1.76 seconds\n",
            "Step [4660/5032], Loss: 5.1472, Average Loss: 7.4118,Time: 1.54 seconds\n",
            "Step [4670/5032], Loss: 6.7781, Average Loss: 7.4092,Time: 1.76 seconds\n",
            "Step [4680/5032], Loss: 6.3877, Average Loss: 7.4071,Time: 1.27 seconds\n",
            "Step [4690/5032], Loss: 5.9852, Average Loss: 7.4050,Time: 1.83 seconds\n",
            "Step [4700/5032], Loss: 6.3564, Average Loss: 7.4030,Time: 1.59 seconds\n",
            "Step [4710/5032], Loss: 5.2790, Average Loss: 7.4008,Time: 1.71 seconds\n",
            "Step [4720/5032], Loss: 6.4520, Average Loss: 7.3983,Time: 2.01 seconds\n",
            "Step [4730/5032], Loss: 6.6052, Average Loss: 7.3964,Time: 1.36 seconds\n",
            "Step [4740/5032], Loss: 5.7741, Average Loss: 7.3942,Time: 1.79 seconds\n",
            "Step [4750/5032], Loss: 6.2580, Average Loss: 7.3923,Time: 1.19 seconds\n",
            "Step [4760/5032], Loss: 5.3506, Average Loss: 7.3898,Time: 1.65 seconds\n",
            "Step [4770/5032], Loss: 6.4056, Average Loss: 7.3868,Time: 1.55 seconds\n",
            "Step [4780/5032], Loss: 6.9510, Average Loss: 7.3858,Time: 2.16 seconds\n",
            "Step [4790/5032], Loss: 7.3270, Average Loss: 7.3833,Time: 1.64 seconds\n",
            "Step [4800/5032], Loss: 6.8699, Average Loss: 7.3811,Time: 1.55 seconds\n",
            "Step [4810/5032], Loss: 6.4812, Average Loss: 7.3787,Time: 1.40 seconds\n",
            "Step [4820/5032], Loss: 7.1043, Average Loss: 7.3774,Time: 2.15 seconds\n",
            "Step [4830/5032], Loss: 6.6309, Average Loss: 7.3754,Time: 1.28 seconds\n",
            "Step [4840/5032], Loss: 7.2374, Average Loss: 7.3734,Time: 1.72 seconds\n",
            "Step [4850/5032], Loss: 6.2340, Average Loss: 7.3709,Time: 1.41 seconds\n",
            "Step [4860/5032], Loss: 7.2085, Average Loss: 7.3697,Time: 1.55 seconds\n",
            "Step [4870/5032], Loss: 7.9622, Average Loss: 7.3682,Time: 1.37 seconds\n",
            "Step [4880/5032], Loss: 6.4781, Average Loss: 7.3665,Time: 1.75 seconds\n",
            "Step [4890/5032], Loss: 6.0565, Average Loss: 7.3645,Time: 1.68 seconds\n",
            "Step [4900/5032], Loss: 6.9483, Average Loss: 7.3630,Time: 1.79 seconds\n",
            "Step [4910/5032], Loss: 7.9024, Average Loss: 7.3613,Time: 1.75 seconds\n",
            "Step [4920/5032], Loss: 8.0360, Average Loss: 7.3600,Time: 2.16 seconds\n",
            "Step [4930/5032], Loss: 5.5904, Average Loss: 7.3580,Time: 1.97 seconds\n",
            "Step [4940/5032], Loss: 7.2151, Average Loss: 7.3559,Time: 1.78 seconds\n",
            "Step [4950/5032], Loss: 7.0358, Average Loss: 7.3538,Time: 1.45 seconds\n",
            "Step [4960/5032], Loss: 5.3409, Average Loss: 7.3519,Time: 1.68 seconds\n",
            "Step [4970/5032], Loss: 6.5188, Average Loss: 7.3494,Time: 1.92 seconds\n",
            "Step [4980/5032], Loss: 5.3355, Average Loss: 7.3473,Time: 1.42 seconds\n",
            "Step [4990/5032], Loss: 7.1587, Average Loss: 7.3458,Time: 1.27 seconds\n",
            "Step [5000/5032], Loss: 5.8422, Average Loss: 7.3430,Time: 1.41 seconds\n",
            "Step [5010/5032], Loss: 6.0074, Average Loss: 7.3405,Time: 1.67 seconds\n",
            "Step [5020/5032], Loss: 6.7002, Average Loss: 7.3384,Time: 1.24 seconds\n",
            "Step [5030/5032], Loss: 7.1405, Average Loss: 7.3368,Time: 1.46 seconds\n",
            "Step [10/503], Loss: 5.5634, Time: 0.59 seconds\n",
            "Step [20/503], Loss: 6.3127, Time: 0.61 seconds\n",
            "Step [30/503], Loss: 6.3834, Time: 0.57 seconds\n",
            "Step [40/503], Loss: 5.4591, Time: 0.58 seconds\n",
            "Step [50/503], Loss: 6.9538, Time: 0.54 seconds\n",
            "Step [60/503], Loss: 6.1255, Time: 0.66 seconds\n",
            "Step [70/503], Loss: 5.4243, Time: 0.51 seconds\n",
            "Step [80/503], Loss: 4.8981, Time: 0.58 seconds\n",
            "Step [90/503], Loss: 6.7841, Time: 0.59 seconds\n",
            "Step [100/503], Loss: 5.7484, Time: 0.68 seconds\n",
            "Step [110/503], Loss: 6.7821, Time: 0.60 seconds\n",
            "Step [120/503], Loss: 6.6646, Time: 0.49 seconds\n",
            "Step [130/503], Loss: 5.4796, Time: 0.35 seconds\n",
            "Step [140/503], Loss: 6.6907, Time: 0.69 seconds\n",
            "Step [150/503], Loss: 4.7527, Time: 0.58 seconds\n",
            "Step [160/503], Loss: 6.2746, Time: 0.64 seconds\n",
            "Step [170/503], Loss: 5.0522, Time: 0.50 seconds\n",
            "Step [180/503], Loss: 5.6901, Time: 0.37 seconds\n",
            "Step [190/503], Loss: 6.6247, Time: 0.54 seconds\n",
            "Step [200/503], Loss: 5.9694, Time: 0.58 seconds\n",
            "Step [210/503], Loss: 4.9258, Time: 0.54 seconds\n",
            "Step [220/503], Loss: 3.8259, Time: 0.53 seconds\n",
            "Step [230/503], Loss: 5.6225, Time: 0.58 seconds\n",
            "Step [240/503], Loss: 4.0901, Time: 0.51 seconds\n",
            "Step [250/503], Loss: 3.8618, Time: 0.41 seconds\n",
            "Step [260/503], Loss: 6.6126, Time: 0.56 seconds\n",
            "Step [270/503], Loss: 6.9147, Time: 0.53 seconds\n",
            "Step [280/503], Loss: 6.4985, Time: 0.65 seconds\n",
            "Step [290/503], Loss: 5.9417, Time: 0.51 seconds\n",
            "Step [300/503], Loss: 6.2957, Time: 0.52 seconds\n",
            "Step [310/503], Loss: 4.3939, Time: 0.53 seconds\n",
            "Step [320/503], Loss: 6.3141, Time: 0.67 seconds\n",
            "Step [330/503], Loss: 7.6428, Time: 0.68 seconds\n",
            "Step [340/503], Loss: 6.9236, Time: 0.79 seconds\n",
            "Step [350/503], Loss: 6.6733, Time: 0.76 seconds\n",
            "Step [360/503], Loss: 5.6355, Time: 0.41 seconds\n",
            "Step [370/503], Loss: 6.3048, Time: 0.52 seconds\n",
            "Step [380/503], Loss: 6.8062, Time: 0.67 seconds\n",
            "Step [390/503], Loss: 6.2285, Time: 0.56 seconds\n",
            "Step [400/503], Loss: 6.2252, Time: 0.55 seconds\n",
            "Step [410/503], Loss: 6.5357, Time: 0.54 seconds\n",
            "Step [420/503], Loss: 6.0686, Time: 0.77 seconds\n",
            "Step [430/503], Loss: 6.9543, Time: 0.65 seconds\n",
            "Step [440/503], Loss: 7.2328, Time: 0.71 seconds\n",
            "Step [450/503], Loss: 6.3039, Time: 0.53 seconds\n",
            "Step [460/503], Loss: 6.0190, Time: 0.61 seconds\n",
            "Step [470/503], Loss: 5.7375, Time: 0.45 seconds\n",
            "Step [480/503], Loss: 5.9775, Time: 0.52 seconds\n",
            "Step [490/503], Loss: 6.2926, Time: 0.57 seconds\n",
            "Step [500/503], Loss: 6.9990, Time: 0.62 seconds\n",
            "Training loss: 7.3351\n",
            "Validation loss: 6.1893\n",
            "\n",
            "------------------------- Epoch 2 -------------------------\n",
            "Step [10/5032], Loss: 6.7012, Average Loss: 7.4137,Time: 1.58 seconds\n",
            "Step [20/5032], Loss: 7.3483, Average Loss: 6.8576,Time: 1.59 seconds\n",
            "Step [30/5032], Loss: 5.8252, Average Loss: 6.5561,Time: 1.52 seconds\n",
            "Step [40/5032], Loss: 6.3705, Average Loss: 6.5088,Time: 1.69 seconds\n",
            "Step [50/5032], Loss: 6.2985, Average Loss: 6.4108,Time: 1.52 seconds\n",
            "Step [60/5032], Loss: 6.8799, Average Loss: 6.4448,Time: 1.50 seconds\n",
            "Step [70/5032], Loss: 5.5694, Average Loss: 6.3980,Time: 1.30 seconds\n",
            "Step [80/5032], Loss: 7.0256, Average Loss: 6.3739,Time: 1.99 seconds\n",
            "Step [90/5032], Loss: 7.0515, Average Loss: 6.3770,Time: 2.27 seconds\n",
            "Step [100/5032], Loss: 6.2910, Average Loss: 6.3721,Time: 1.70 seconds\n",
            "Step [110/5032], Loss: 5.8270, Average Loss: 6.3372,Time: 1.55 seconds\n",
            "Step [120/5032], Loss: 6.4360, Average Loss: 6.3527,Time: 1.55 seconds\n",
            "Step [130/5032], Loss: 8.8589, Average Loss: 6.3904,Time: 1.78 seconds\n",
            "Step [140/5032], Loss: 4.8087, Average Loss: 6.3251,Time: 1.15 seconds\n",
            "Step [150/5032], Loss: 8.0305, Average Loss: 6.3607,Time: 2.29 seconds\n",
            "Step [160/5032], Loss: 6.5645, Average Loss: 6.3680,Time: 1.65 seconds\n",
            "Step [170/5032], Loss: 6.9133, Average Loss: 6.3924,Time: 1.74 seconds\n",
            "Step [180/5032], Loss: 6.4214, Average Loss: 6.3955,Time: 1.73 seconds\n",
            "Step [190/5032], Loss: 6.0623, Average Loss: 6.3720,Time: 1.65 seconds\n",
            "Step [200/5032], Loss: 7.0925, Average Loss: 6.4122,Time: 1.46 seconds\n",
            "Step [210/5032], Loss: 5.6616, Average Loss: 6.4007,Time: 1.30 seconds\n",
            "Step [220/5032], Loss: 5.7581, Average Loss: 6.4027,Time: 1.48 seconds\n",
            "Step [230/5032], Loss: 6.6041, Average Loss: 6.3911,Time: 1.72 seconds\n",
            "Step [240/5032], Loss: 6.9854, Average Loss: 6.3906,Time: 1.56 seconds\n",
            "Step [250/5032], Loss: 6.6597, Average Loss: 6.3944,Time: 1.93 seconds\n",
            "Step [260/5032], Loss: 7.4537, Average Loss: 6.4040,Time: 1.86 seconds\n",
            "Step [270/5032], Loss: 6.9571, Average Loss: 6.3960,Time: 1.60 seconds\n",
            "Step [280/5032], Loss: 6.8177, Average Loss: 6.3945,Time: 1.60 seconds\n",
            "Step [290/5032], Loss: 5.8038, Average Loss: 6.3997,Time: 1.58 seconds\n",
            "Step [300/5032], Loss: 7.0450, Average Loss: 6.3982,Time: 1.59 seconds\n",
            "Step [310/5032], Loss: 6.2718, Average Loss: 6.3975,Time: 1.84 seconds\n",
            "Step [320/5032], Loss: 6.3269, Average Loss: 6.3944,Time: 1.55 seconds\n",
            "Step [330/5032], Loss: 6.3962, Average Loss: 6.3810,Time: 1.19 seconds\n",
            "Step [340/5032], Loss: 6.8608, Average Loss: 6.3900,Time: 1.34 seconds\n",
            "Step [350/5032], Loss: 5.4167, Average Loss: 6.3879,Time: 1.37 seconds\n",
            "Step [360/5032], Loss: 4.8844, Average Loss: 6.3943,Time: 2.07 seconds\n",
            "Step [370/5032], Loss: 7.3980, Average Loss: 6.3903,Time: 1.69 seconds\n",
            "Step [380/5032], Loss: 4.6313, Average Loss: 6.3959,Time: 2.28 seconds\n",
            "Step [390/5032], Loss: 6.6533, Average Loss: 6.3926,Time: 1.45 seconds\n",
            "Step [400/5032], Loss: 4.9348, Average Loss: 6.3917,Time: 1.68 seconds\n",
            "Step [410/5032], Loss: 6.3851, Average Loss: 6.3810,Time: 1.61 seconds\n",
            "Step [420/5032], Loss: 6.1575, Average Loss: 6.3834,Time: 1.74 seconds\n",
            "Step [430/5032], Loss: 6.4519, Average Loss: 6.3798,Time: 1.42 seconds\n",
            "Step [440/5032], Loss: 5.0467, Average Loss: 6.3694,Time: 1.63 seconds\n",
            "Step [450/5032], Loss: 6.8793, Average Loss: 6.3694,Time: 1.59 seconds\n",
            "Step [460/5032], Loss: 5.8334, Average Loss: 6.3650,Time: 1.57 seconds\n",
            "Step [470/5032], Loss: 5.3003, Average Loss: 6.3562,Time: 1.38 seconds\n",
            "Step [480/5032], Loss: 6.4422, Average Loss: 6.3504,Time: 1.76 seconds\n",
            "Step [490/5032], Loss: 5.3114, Average Loss: 6.3449,Time: 1.40 seconds\n",
            "Step [500/5032], Loss: 6.6971, Average Loss: 6.3465,Time: 1.39 seconds\n",
            "Step [510/5032], Loss: 6.9750, Average Loss: 6.3441,Time: 1.84 seconds\n",
            "Step [520/5032], Loss: 6.1520, Average Loss: 6.3395,Time: 1.74 seconds\n",
            "Step [530/5032], Loss: 9.2128, Average Loss: 6.3436,Time: 1.50 seconds\n",
            "Step [540/5032], Loss: 6.2580, Average Loss: 6.3344,Time: 1.59 seconds\n",
            "Step [550/5032], Loss: 5.6225, Average Loss: 6.3349,Time: 1.65 seconds\n",
            "Step [560/5032], Loss: 6.6961, Average Loss: 6.3301,Time: 1.42 seconds\n",
            "Step [570/5032], Loss: 5.5490, Average Loss: 6.3227,Time: 1.31 seconds\n",
            "Step [580/5032], Loss: 7.1948, Average Loss: 6.3139,Time: 1.29 seconds\n",
            "Step [590/5032], Loss: 6.9100, Average Loss: 6.3118,Time: 1.40 seconds\n",
            "Step [600/5032], Loss: 5.8107, Average Loss: 6.3036,Time: 1.83 seconds\n",
            "Step [610/5032], Loss: 6.0365, Average Loss: 6.3055,Time: 1.55 seconds\n",
            "Step [620/5032], Loss: 7.3040, Average Loss: 6.3123,Time: 1.63 seconds\n",
            "Step [630/5032], Loss: 4.9683, Average Loss: 6.3114,Time: 2.19 seconds\n",
            "Step [640/5032], Loss: 7.0260, Average Loss: 6.3075,Time: 1.60 seconds\n",
            "Step [650/5032], Loss: 7.1541, Average Loss: 6.3043,Time: 1.50 seconds\n",
            "Step [660/5032], Loss: 5.6087, Average Loss: 6.3035,Time: 1.49 seconds\n",
            "Step [670/5032], Loss: 6.9711, Average Loss: 6.3047,Time: 1.55 seconds\n",
            "Step [680/5032], Loss: 6.5092, Average Loss: 6.2982,Time: 1.55 seconds\n",
            "Step [690/5032], Loss: 6.3327, Average Loss: 6.2949,Time: 2.08 seconds\n",
            "Step [700/5032], Loss: 5.8311, Average Loss: 6.2977,Time: 1.39 seconds\n",
            "Step [710/5032], Loss: 5.2846, Average Loss: 6.2998,Time: 1.64 seconds\n",
            "Step [720/5032], Loss: 6.5114, Average Loss: 6.3027,Time: 1.47 seconds\n",
            "Step [730/5032], Loss: 6.9117, Average Loss: 6.3000,Time: 1.63 seconds\n",
            "Step [740/5032], Loss: 7.4046, Average Loss: 6.2963,Time: 1.47 seconds\n",
            "Step [750/5032], Loss: 6.4403, Average Loss: 6.2954,Time: 1.76 seconds\n",
            "Step [760/5032], Loss: 6.7792, Average Loss: 6.2922,Time: 1.45 seconds\n",
            "Step [770/5032], Loss: 5.9757, Average Loss: 6.2937,Time: 1.48 seconds\n",
            "Step [780/5032], Loss: 4.9502, Average Loss: 6.2899,Time: 1.34 seconds\n",
            "Step [790/5032], Loss: 6.1596, Average Loss: 6.2928,Time: 1.93 seconds\n",
            "Step [800/5032], Loss: 6.6283, Average Loss: 6.2901,Time: 1.77 seconds\n",
            "Step [810/5032], Loss: 6.2264, Average Loss: 6.2917,Time: 1.48 seconds\n",
            "Step [820/5032], Loss: 6.5907, Average Loss: 6.2921,Time: 1.39 seconds\n",
            "Step [830/5032], Loss: 6.8220, Average Loss: 6.2871,Time: 1.41 seconds\n",
            "Step [840/5032], Loss: 6.8513, Average Loss: 6.2862,Time: 1.73 seconds\n",
            "Step [850/5032], Loss: 5.2396, Average Loss: 6.2844,Time: 1.82 seconds\n",
            "Step [860/5032], Loss: 6.3993, Average Loss: 6.2888,Time: 1.80 seconds\n",
            "Step [870/5032], Loss: 6.7529, Average Loss: 6.2863,Time: 1.55 seconds\n",
            "Step [880/5032], Loss: 6.1726, Average Loss: 6.2802,Time: 1.56 seconds\n",
            "Step [890/5032], Loss: 6.5374, Average Loss: 6.2800,Time: 1.79 seconds\n",
            "Step [900/5032], Loss: 6.9108, Average Loss: 6.2795,Time: 1.68 seconds\n",
            "Step [910/5032], Loss: 4.6595, Average Loss: 6.2770,Time: 1.79 seconds\n",
            "Step [920/5032], Loss: 5.8575, Average Loss: 6.2732,Time: 1.49 seconds\n",
            "Step [930/5032], Loss: 3.9285, Average Loss: 6.2751,Time: 1.85 seconds\n",
            "Step [940/5032], Loss: 6.4678, Average Loss: 6.2757,Time: 2.14 seconds\n",
            "Step [950/5032], Loss: 6.0472, Average Loss: 6.2752,Time: 1.70 seconds\n",
            "Step [960/5032], Loss: 6.5137, Average Loss: 6.2759,Time: 1.52 seconds\n",
            "Step [970/5032], Loss: 5.8979, Average Loss: 6.2730,Time: 1.68 seconds\n",
            "Step [980/5032], Loss: 6.1415, Average Loss: 6.2753,Time: 1.82 seconds\n",
            "Step [990/5032], Loss: 5.4155, Average Loss: 6.2729,Time: 1.47 seconds\n",
            "Step [1000/5032], Loss: 6.9816, Average Loss: 6.2742,Time: 1.87 seconds\n",
            "Step [1010/5032], Loss: 5.5329, Average Loss: 6.2692,Time: 1.61 seconds\n",
            "Step [1020/5032], Loss: 4.7682, Average Loss: 6.2698,Time: 1.66 seconds\n",
            "Step [1030/5032], Loss: 4.8129, Average Loss: 6.2644,Time: 1.14 seconds\n",
            "Step [1040/5032], Loss: 6.9054, Average Loss: 6.2648,Time: 1.66 seconds\n",
            "Step [1050/5032], Loss: 6.1405, Average Loss: 6.2660,Time: 1.81 seconds\n",
            "Step [1060/5032], Loss: 5.6641, Average Loss: 6.2643,Time: 1.83 seconds\n",
            "Step [1070/5032], Loss: 7.2674, Average Loss: 6.2654,Time: 1.48 seconds\n",
            "Step [1080/5032], Loss: 6.3734, Average Loss: 6.2635,Time: 1.36 seconds\n",
            "Step [1090/5032], Loss: 6.2315, Average Loss: 6.2608,Time: 2.11 seconds\n",
            "Step [1100/5032], Loss: 4.5520, Average Loss: 6.2578,Time: 1.46 seconds\n",
            "Step [1110/5032], Loss: 5.6245, Average Loss: 6.2532,Time: 1.46 seconds\n",
            "Step [1120/5032], Loss: 5.9476, Average Loss: 6.2526,Time: 1.61 seconds\n",
            "Step [1130/5032], Loss: 7.7245, Average Loss: 6.2515,Time: 1.62 seconds\n",
            "Step [1140/5032], Loss: 6.6066, Average Loss: 6.2528,Time: 1.70 seconds\n",
            "Step [1150/5032], Loss: 5.8984, Average Loss: 6.2514,Time: 1.66 seconds\n",
            "Step [1160/5032], Loss: 4.0589, Average Loss: 6.2510,Time: 1.72 seconds\n",
            "Step [1170/5032], Loss: 6.2371, Average Loss: 6.2515,Time: 1.59 seconds\n",
            "Step [1180/5032], Loss: 6.8918, Average Loss: 6.2490,Time: 1.55 seconds\n",
            "Step [1190/5032], Loss: 6.1450, Average Loss: 6.2468,Time: 1.58 seconds\n",
            "Step [1200/5032], Loss: 5.4209, Average Loss: 6.2417,Time: 1.82 seconds\n",
            "Step [1210/5032], Loss: 5.9399, Average Loss: 6.2400,Time: 1.55 seconds\n",
            "Step [1220/5032], Loss: 6.3968, Average Loss: 6.2362,Time: 1.47 seconds\n",
            "Step [1230/5032], Loss: 6.3098, Average Loss: 6.2309,Time: 1.61 seconds\n",
            "Step [1240/5032], Loss: 6.1734, Average Loss: 6.2303,Time: 2.15 seconds\n",
            "Step [1250/5032], Loss: 6.3690, Average Loss: 6.2294,Time: 1.53 seconds\n",
            "Step [1260/5032], Loss: 4.9447, Average Loss: 6.2288,Time: 1.73 seconds\n",
            "Step [1270/5032], Loss: 4.7706, Average Loss: 6.2297,Time: 1.48 seconds\n",
            "Step [1280/5032], Loss: 7.0485, Average Loss: 6.2262,Time: 1.38 seconds\n",
            "Step [1290/5032], Loss: 5.6217, Average Loss: 6.2233,Time: 1.36 seconds\n",
            "Step [1300/5032], Loss: 6.1743, Average Loss: 6.2247,Time: 2.55 seconds\n",
            "Step [1310/5032], Loss: 5.3681, Average Loss: 6.2242,Time: 1.72 seconds\n",
            "Step [1320/5032], Loss: 6.0746, Average Loss: 6.2204,Time: 1.66 seconds\n",
            "Step [1330/5032], Loss: 6.1444, Average Loss: 6.2167,Time: 1.32 seconds\n",
            "Step [1340/5032], Loss: 6.0868, Average Loss: 6.2141,Time: 1.72 seconds\n",
            "Step [1350/5032], Loss: 5.3225, Average Loss: 6.2135,Time: 1.88 seconds\n",
            "Step [1360/5032], Loss: 9.5382, Average Loss: 6.2155,Time: 1.64 seconds\n",
            "Step [1370/5032], Loss: 6.5226, Average Loss: 6.2129,Time: 1.54 seconds\n",
            "Step [1380/5032], Loss: 6.0561, Average Loss: 6.2080,Time: 1.41 seconds\n",
            "Step [1390/5032], Loss: 4.9838, Average Loss: 6.2099,Time: 1.33 seconds\n",
            "Step [1400/5032], Loss: 5.6041, Average Loss: 6.2095,Time: 1.57 seconds\n",
            "Step [1410/5032], Loss: 6.8408, Average Loss: 6.2075,Time: 1.62 seconds\n",
            "Step [1420/5032], Loss: 4.8761, Average Loss: 6.2048,Time: 2.00 seconds\n",
            "Step [1430/5032], Loss: 6.3289, Average Loss: 6.2049,Time: 2.20 seconds\n",
            "Step [1440/5032], Loss: 6.4341, Average Loss: 6.2037,Time: 1.27 seconds\n",
            "Step [1450/5032], Loss: 6.8570, Average Loss: 6.2020,Time: 1.50 seconds\n",
            "Step [1460/5032], Loss: 5.6093, Average Loss: 6.2022,Time: 1.46 seconds\n",
            "Step [1470/5032], Loss: 6.4163, Average Loss: 6.2016,Time: 1.23 seconds\n",
            "Step [1480/5032], Loss: 5.9449, Average Loss: 6.1971,Time: 1.61 seconds\n",
            "Step [1490/5032], Loss: 6.1929, Average Loss: 6.1974,Time: 1.58 seconds\n",
            "Step [1500/5032], Loss: 5.6304, Average Loss: 6.1939,Time: 1.37 seconds\n",
            "Step [1510/5032], Loss: 6.5664, Average Loss: 6.1953,Time: 1.84 seconds\n",
            "Step [1520/5032], Loss: 5.4783, Average Loss: 6.1919,Time: 1.29 seconds\n",
            "Step [1530/5032], Loss: 5.3486, Average Loss: 6.1928,Time: 1.90 seconds\n",
            "Step [1540/5032], Loss: 4.4647, Average Loss: 6.1902,Time: 1.68 seconds\n",
            "Step [1550/5032], Loss: 6.8043, Average Loss: 6.1930,Time: 1.86 seconds\n",
            "Step [1560/5032], Loss: 5.8485, Average Loss: 6.1891,Time: 1.36 seconds\n",
            "Step [1570/5032], Loss: 5.3086, Average Loss: 6.1876,Time: 1.80 seconds\n",
            "Step [1580/5032], Loss: 7.3160, Average Loss: 6.1874,Time: 1.62 seconds\n",
            "Step [1590/5032], Loss: 4.6192, Average Loss: 6.1839,Time: 1.20 seconds\n",
            "Step [1600/5032], Loss: 6.4648, Average Loss: 6.1832,Time: 1.38 seconds\n",
            "Step [1610/5032], Loss: 6.8718, Average Loss: 6.1824,Time: 1.74 seconds\n",
            "Step [1620/5032], Loss: 6.4909, Average Loss: 6.1830,Time: 1.54 seconds\n",
            "Step [1630/5032], Loss: 5.9852, Average Loss: 6.1802,Time: 1.92 seconds\n",
            "Step [1640/5032], Loss: 6.2099, Average Loss: 6.1783,Time: 1.76 seconds\n",
            "Step [1650/5032], Loss: 7.1522, Average Loss: 6.1784,Time: 1.23 seconds\n",
            "Step [1660/5032], Loss: 6.3747, Average Loss: 6.1771,Time: 1.14 seconds\n",
            "Step [1670/5032], Loss: 5.8233, Average Loss: 6.1785,Time: 1.61 seconds\n",
            "Step [1680/5032], Loss: 5.5736, Average Loss: 6.1761,Time: 1.42 seconds\n",
            "Step [1690/5032], Loss: 5.6978, Average Loss: 6.1752,Time: 1.31 seconds\n",
            "Step [1700/5032], Loss: 6.4291, Average Loss: 6.1733,Time: 1.36 seconds\n",
            "Step [1710/5032], Loss: 6.6944, Average Loss: 6.1720,Time: 1.81 seconds\n",
            "Step [1720/5032], Loss: 6.8445, Average Loss: 6.1712,Time: 1.41 seconds\n",
            "Step [1730/5032], Loss: 6.1129, Average Loss: 6.1697,Time: 1.57 seconds\n",
            "Step [1740/5032], Loss: 4.9905, Average Loss: 6.1665,Time: 1.41 seconds\n",
            "Step [1750/5032], Loss: 6.1642, Average Loss: 6.1666,Time: 1.82 seconds\n",
            "Step [1760/5032], Loss: 5.6424, Average Loss: 6.1663,Time: 1.87 seconds\n",
            "Step [1770/5032], Loss: 5.8131, Average Loss: 6.1666,Time: 1.29 seconds\n",
            "Step [1780/5032], Loss: 7.5802, Average Loss: 6.1695,Time: 3.26 seconds\n",
            "Step [1790/5032], Loss: 6.6458, Average Loss: 6.1698,Time: 1.83 seconds\n",
            "Step [1800/5032], Loss: 3.6401, Average Loss: 6.1669,Time: 1.31 seconds\n",
            "Step [1810/5032], Loss: 6.6711, Average Loss: 6.1642,Time: 1.70 seconds\n",
            "Step [1820/5032], Loss: 4.8079, Average Loss: 6.1625,Time: 1.40 seconds\n",
            "Step [1830/5032], Loss: 6.2313, Average Loss: 6.1612,Time: 1.56 seconds\n",
            "Step [1840/5032], Loss: 4.9185, Average Loss: 6.1589,Time: 1.53 seconds\n",
            "Step [1850/5032], Loss: 5.9501, Average Loss: 6.1572,Time: 1.43 seconds\n",
            "Step [1860/5032], Loss: 6.3769, Average Loss: 6.1552,Time: 1.54 seconds\n",
            "Step [1870/5032], Loss: 7.3296, Average Loss: 6.1548,Time: 1.43 seconds\n",
            "Step [1880/5032], Loss: 4.9058, Average Loss: 6.1544,Time: 1.35 seconds\n",
            "Step [1890/5032], Loss: 8.1464, Average Loss: 6.1542,Time: 1.50 seconds\n",
            "Step [1900/5032], Loss: 6.8311, Average Loss: 6.1543,Time: 2.06 seconds\n",
            "Step [1910/5032], Loss: 6.1311, Average Loss: 6.1546,Time: 1.49 seconds\n",
            "Step [1920/5032], Loss: 5.8357, Average Loss: 6.1541,Time: 1.82 seconds\n",
            "Step [1930/5032], Loss: 6.0264, Average Loss: 6.1530,Time: 1.84 seconds\n",
            "Step [1940/5032], Loss: 4.8392, Average Loss: 6.1521,Time: 1.58 seconds\n",
            "Step [1950/5032], Loss: 8.4512, Average Loss: 6.1545,Time: 2.13 seconds\n",
            "Step [1960/5032], Loss: 5.0285, Average Loss: 6.1534,Time: 1.42 seconds\n",
            "Step [1970/5032], Loss: 6.3840, Average Loss: 6.1507,Time: 1.88 seconds\n",
            "Step [1980/5032], Loss: 5.4187, Average Loss: 6.1494,Time: 1.45 seconds\n",
            "Step [1990/5032], Loss: 5.4368, Average Loss: 6.1483,Time: 1.53 seconds\n",
            "Step [2000/5032], Loss: 5.5166, Average Loss: 6.1466,Time: 1.80 seconds\n",
            "Step [2010/5032], Loss: 6.2163, Average Loss: 6.1437,Time: 1.22 seconds\n",
            "Step [2020/5032], Loss: 5.7391, Average Loss: 6.1435,Time: 1.39 seconds\n",
            "Step [2030/5032], Loss: 5.3284, Average Loss: 6.1428,Time: 1.88 seconds\n",
            "Step [2040/5032], Loss: 6.0117, Average Loss: 6.1444,Time: 1.69 seconds\n",
            "Step [2050/5032], Loss: 6.7481, Average Loss: 6.1422,Time: 1.62 seconds\n",
            "Step [2060/5032], Loss: 5.8119, Average Loss: 6.1428,Time: 1.78 seconds\n",
            "Step [2070/5032], Loss: 6.6718, Average Loss: 6.1413,Time: 1.30 seconds\n",
            "Step [2080/5032], Loss: 6.2610, Average Loss: 6.1403,Time: 1.75 seconds\n",
            "Step [2090/5032], Loss: 5.7911, Average Loss: 6.1393,Time: 1.59 seconds\n",
            "Step [2100/5032], Loss: 5.7119, Average Loss: 6.1363,Time: 1.72 seconds\n",
            "Step [2110/5032], Loss: 5.5215, Average Loss: 6.1351,Time: 1.40 seconds\n",
            "Step [2120/5032], Loss: 5.8860, Average Loss: 6.1341,Time: 1.56 seconds\n",
            "Step [2130/5032], Loss: 6.0285, Average Loss: 6.1326,Time: 1.69 seconds\n",
            "Step [2140/5032], Loss: 6.4243, Average Loss: 6.1315,Time: 1.60 seconds\n",
            "Step [2150/5032], Loss: 6.2198, Average Loss: 6.1303,Time: 1.44 seconds\n",
            "Step [2160/5032], Loss: 5.9183, Average Loss: 6.1306,Time: 1.70 seconds\n",
            "Step [2170/5032], Loss: 6.6016, Average Loss: 6.1312,Time: 1.74 seconds\n",
            "Step [2180/5032], Loss: 5.5972, Average Loss: 6.1283,Time: 1.69 seconds\n",
            "Step [2190/5032], Loss: 5.2254, Average Loss: 6.1284,Time: 2.13 seconds\n",
            "Step [2200/5032], Loss: 5.5915, Average Loss: 6.1268,Time: 1.65 seconds\n",
            "Step [2210/5032], Loss: 4.8869, Average Loss: 6.1237,Time: 1.50 seconds\n",
            "Step [2220/5032], Loss: 6.2573, Average Loss: 6.1210,Time: 1.49 seconds\n",
            "Step [2230/5032], Loss: 6.1379, Average Loss: 6.1192,Time: 1.59 seconds\n",
            "Step [2240/5032], Loss: 6.6260, Average Loss: 6.1195,Time: 1.75 seconds\n",
            "Step [2250/5032], Loss: 5.6153, Average Loss: 6.1192,Time: 1.66 seconds\n",
            "Step [2260/5032], Loss: 5.5666, Average Loss: 6.1186,Time: 1.66 seconds\n",
            "Step [2270/5032], Loss: 4.7409, Average Loss: 6.1165,Time: 1.76 seconds\n",
            "Step [2280/5032], Loss: 5.2538, Average Loss: 6.1151,Time: 1.65 seconds\n",
            "Step [2290/5032], Loss: 4.8885, Average Loss: 6.1141,Time: 1.83 seconds\n",
            "Step [2300/5032], Loss: 6.4245, Average Loss: 6.1131,Time: 1.51 seconds\n",
            "Step [2310/5032], Loss: 7.4542, Average Loss: 6.1140,Time: 1.48 seconds\n",
            "Step [2320/5032], Loss: 6.5056, Average Loss: 6.1138,Time: 1.45 seconds\n",
            "Step [2330/5032], Loss: 5.8831, Average Loss: 6.1123,Time: 1.52 seconds\n",
            "Step [2340/5032], Loss: 5.2352, Average Loss: 6.1118,Time: 1.56 seconds\n",
            "Step [2350/5032], Loss: 6.4999, Average Loss: 6.1123,Time: 1.30 seconds\n",
            "Step [2360/5032], Loss: 6.5404, Average Loss: 6.1112,Time: 1.78 seconds\n",
            "Step [2370/5032], Loss: 5.5426, Average Loss: 6.1115,Time: 1.49 seconds\n",
            "Step [2380/5032], Loss: 5.1091, Average Loss: 6.1117,Time: 2.02 seconds\n",
            "Step [2390/5032], Loss: 6.5944, Average Loss: 6.1106,Time: 1.68 seconds\n",
            "Step [2400/5032], Loss: 5.0220, Average Loss: 6.1105,Time: 2.07 seconds\n",
            "Step [2410/5032], Loss: 5.3080, Average Loss: 6.1093,Time: 1.75 seconds\n",
            "Step [2420/5032], Loss: 6.2025, Average Loss: 6.1097,Time: 1.98 seconds\n",
            "Step [2430/5032], Loss: 6.5468, Average Loss: 6.1102,Time: 1.23 seconds\n",
            "Step [2440/5032], Loss: 6.5913, Average Loss: 6.1095,Time: 1.61 seconds\n",
            "Step [2450/5032], Loss: 6.5952, Average Loss: 6.1079,Time: 1.47 seconds\n",
            "Step [2460/5032], Loss: 5.5568, Average Loss: 6.1057,Time: 1.34 seconds\n",
            "Step [2470/5032], Loss: 6.0326, Average Loss: 6.1071,Time: 1.62 seconds\n",
            "Step [2480/5032], Loss: 6.6779, Average Loss: 6.1057,Time: 1.60 seconds\n",
            "Step [2490/5032], Loss: 6.6074, Average Loss: 6.1056,Time: 1.48 seconds\n",
            "Step [2500/5032], Loss: 5.4646, Average Loss: 6.1049,Time: 1.52 seconds\n",
            "Step [2510/5032], Loss: 4.6740, Average Loss: 6.1045,Time: 1.50 seconds\n",
            "Step [2520/5032], Loss: 7.5672, Average Loss: 6.1039,Time: 1.91 seconds\n",
            "Step [2530/5032], Loss: 6.5749, Average Loss: 6.1040,Time: 1.98 seconds\n",
            "Step [2540/5032], Loss: 5.9168, Average Loss: 6.1033,Time: 1.72 seconds\n",
            "Step [2550/5032], Loss: 6.7654, Average Loss: 6.1005,Time: 1.95 seconds\n",
            "Step [2560/5032], Loss: 4.6846, Average Loss: 6.0975,Time: 1.70 seconds\n",
            "Step [2570/5032], Loss: 6.9849, Average Loss: 6.0978,Time: 1.59 seconds\n",
            "Step [2580/5032], Loss: 5.4986, Average Loss: 6.0972,Time: 1.61 seconds\n",
            "Step [2590/5032], Loss: 6.2033, Average Loss: 6.0964,Time: 1.72 seconds\n",
            "Step [2600/5032], Loss: 4.4681, Average Loss: 6.0952,Time: 1.43 seconds\n",
            "Step [2610/5032], Loss: 6.1294, Average Loss: 6.0947,Time: 1.88 seconds\n",
            "Step [2620/5032], Loss: 6.4643, Average Loss: 6.0941,Time: 1.58 seconds\n",
            "Step [2630/5032], Loss: 6.6212, Average Loss: 6.0938,Time: 1.49 seconds\n",
            "Step [2640/5032], Loss: 6.0783, Average Loss: 6.0936,Time: 1.49 seconds\n",
            "Step [2650/5032], Loss: 5.6507, Average Loss: 6.0929,Time: 1.29 seconds\n",
            "Step [2660/5032], Loss: 5.2555, Average Loss: 6.0918,Time: 1.43 seconds\n",
            "Step [2670/5032], Loss: 4.6151, Average Loss: 6.0908,Time: 1.25 seconds\n",
            "Step [2680/5032], Loss: 6.2199, Average Loss: 6.0927,Time: 2.91 seconds\n",
            "Step [2690/5032], Loss: 6.2403, Average Loss: 6.0930,Time: 1.64 seconds\n",
            "Step [2700/5032], Loss: 5.9272, Average Loss: 6.0920,Time: 1.55 seconds\n",
            "Step [2710/5032], Loss: 7.6433, Average Loss: 6.0932,Time: 1.45 seconds\n",
            "Step [2720/5032], Loss: 5.7606, Average Loss: 6.0921,Time: 1.54 seconds\n",
            "Step [2730/5032], Loss: 6.2907, Average Loss: 6.0912,Time: 1.62 seconds\n",
            "Step [2740/5032], Loss: 6.0329, Average Loss: 6.0914,Time: 1.72 seconds\n",
            "Step [2750/5032], Loss: 6.8055, Average Loss: 6.0929,Time: 2.40 seconds\n",
            "Step [2760/5032], Loss: 4.5082, Average Loss: 6.0902,Time: 1.44 seconds\n",
            "Step [2770/5032], Loss: 5.6396, Average Loss: 6.0883,Time: 1.86 seconds\n",
            "Step [2780/5032], Loss: 5.5003, Average Loss: 6.0882,Time: 1.48 seconds\n",
            "Step [2790/5032], Loss: 6.6123, Average Loss: 6.0891,Time: 1.26 seconds\n",
            "Step [2800/5032], Loss: 6.2895, Average Loss: 6.0882,Time: 1.51 seconds\n",
            "Step [2810/5032], Loss: 5.8735, Average Loss: 6.0873,Time: 1.85 seconds\n",
            "Step [2820/5032], Loss: 5.4063, Average Loss: 6.0866,Time: 1.27 seconds\n",
            "Step [2830/5032], Loss: 5.6542, Average Loss: 6.0857,Time: 1.30 seconds\n",
            "Step [2840/5032], Loss: 6.2309, Average Loss: 6.0883,Time: 2.54 seconds\n",
            "Step [2850/5032], Loss: 6.5435, Average Loss: 6.0886,Time: 1.77 seconds\n",
            "Step [2860/5032], Loss: 4.8976, Average Loss: 6.0878,Time: 1.33 seconds\n",
            "Step [2870/5032], Loss: 6.1360, Average Loss: 6.0878,Time: 1.31 seconds\n",
            "Step [2880/5032], Loss: 5.8709, Average Loss: 6.0861,Time: 1.72 seconds\n",
            "Step [2890/5032], Loss: 4.8065, Average Loss: 6.0859,Time: 1.73 seconds\n",
            "Step [2900/5032], Loss: 6.2216, Average Loss: 6.0861,Time: 2.14 seconds\n",
            "Step [2910/5032], Loss: 6.6888, Average Loss: 6.0845,Time: 1.37 seconds\n",
            "Step [2920/5032], Loss: 6.2023, Average Loss: 6.0844,Time: 2.03 seconds\n",
            "Step [2930/5032], Loss: 6.0323, Average Loss: 6.0834,Time: 1.42 seconds\n",
            "Step [2940/5032], Loss: 5.1336, Average Loss: 6.0829,Time: 1.77 seconds\n",
            "Step [2950/5032], Loss: 5.8042, Average Loss: 6.0831,Time: 1.78 seconds\n",
            "Step [2960/5032], Loss: 8.8774, Average Loss: 6.0833,Time: 2.18 seconds\n",
            "Step [2970/5032], Loss: 5.8892, Average Loss: 6.0830,Time: 1.49 seconds\n",
            "Step [2980/5032], Loss: 6.8003, Average Loss: 6.0805,Time: 1.34 seconds\n",
            "Step [2990/5032], Loss: 4.7583, Average Loss: 6.0793,Time: 1.45 seconds\n",
            "Step [3000/5032], Loss: 6.0218, Average Loss: 6.0779,Time: 1.58 seconds\n",
            "Step [3010/5032], Loss: 4.9131, Average Loss: 6.0778,Time: 1.81 seconds\n",
            "Step [3020/5032], Loss: 4.0179, Average Loss: 6.0749,Time: 1.72 seconds\n",
            "Step [3030/5032], Loss: 5.7041, Average Loss: 6.0763,Time: 1.73 seconds\n",
            "Step [3040/5032], Loss: 6.9697, Average Loss: 6.0750,Time: 1.13 seconds\n",
            "Step [3050/5032], Loss: 6.0478, Average Loss: 6.0749,Time: 1.80 seconds\n",
            "Step [3060/5032], Loss: 5.4131, Average Loss: 6.0751,Time: 1.69 seconds\n",
            "Step [3070/5032], Loss: 5.9724, Average Loss: 6.0746,Time: 1.36 seconds\n",
            "Step [3080/5032], Loss: 7.1484, Average Loss: 6.0739,Time: 1.32 seconds\n",
            "Step [3090/5032], Loss: 4.8847, Average Loss: 6.0742,Time: 2.36 seconds\n",
            "Step [3100/5032], Loss: 5.3692, Average Loss: 6.0739,Time: 2.04 seconds\n",
            "Step [3110/5032], Loss: 5.8371, Average Loss: 6.0728,Time: 1.63 seconds\n",
            "Step [3120/5032], Loss: 6.7921, Average Loss: 6.0732,Time: 1.85 seconds\n",
            "Step [3130/5032], Loss: 5.1382, Average Loss: 6.0717,Time: 1.48 seconds\n",
            "Step [3140/5032], Loss: 5.5904, Average Loss: 6.0712,Time: 1.57 seconds\n",
            "Step [3150/5032], Loss: 6.1788, Average Loss: 6.0725,Time: 2.42 seconds\n",
            "Step [3160/5032], Loss: 6.1691, Average Loss: 6.0712,Time: 1.64 seconds\n",
            "Step [3170/5032], Loss: 5.2556, Average Loss: 6.0711,Time: 1.49 seconds\n",
            "Step [3180/5032], Loss: 5.5808, Average Loss: 6.0712,Time: 1.53 seconds\n",
            "Step [3190/5032], Loss: 6.1329, Average Loss: 6.0699,Time: 1.54 seconds\n",
            "Step [3200/5032], Loss: 5.6636, Average Loss: 6.0692,Time: 1.76 seconds\n",
            "Step [3210/5032], Loss: 3.3680, Average Loss: 6.0672,Time: 1.66 seconds\n",
            "Step [3220/5032], Loss: 6.4248, Average Loss: 6.0657,Time: 1.65 seconds\n",
            "Step [3230/5032], Loss: 6.4327, Average Loss: 6.0651,Time: 1.73 seconds\n",
            "Step [3240/5032], Loss: 5.5705, Average Loss: 6.0640,Time: 1.83 seconds\n",
            "Step [3250/5032], Loss: 5.5328, Average Loss: 6.0630,Time: 1.56 seconds\n",
            "Step [3260/5032], Loss: 5.9245, Average Loss: 6.0635,Time: 1.76 seconds\n",
            "Step [3270/5032], Loss: 4.4776, Average Loss: 6.0620,Time: 1.51 seconds\n",
            "Step [3280/5032], Loss: 6.5450, Average Loss: 6.0605,Time: 1.44 seconds\n",
            "Step [3290/5032], Loss: 6.4429, Average Loss: 6.0609,Time: 1.44 seconds\n",
            "Step [3300/5032], Loss: 5.7741, Average Loss: 6.0602,Time: 1.70 seconds\n",
            "Step [3310/5032], Loss: 4.9039, Average Loss: 6.0600,Time: 1.15 seconds\n",
            "Step [3320/5032], Loss: 4.7417, Average Loss: 6.0587,Time: 1.50 seconds\n",
            "Step [3330/5032], Loss: 5.5190, Average Loss: 6.0580,Time: 1.39 seconds\n",
            "Step [3340/5032], Loss: 6.2477, Average Loss: 6.0578,Time: 2.60 seconds\n",
            "Step [3350/5032], Loss: 5.5929, Average Loss: 6.0578,Time: 1.66 seconds\n",
            "Step [3360/5032], Loss: 5.9367, Average Loss: 6.0549,Time: 1.42 seconds\n",
            "Step [3370/5032], Loss: 4.5809, Average Loss: 6.0539,Time: 1.37 seconds\n",
            "Step [3380/5032], Loss: 5.4959, Average Loss: 6.0523,Time: 1.63 seconds\n",
            "Step [3390/5032], Loss: 5.8790, Average Loss: 6.0513,Time: 1.30 seconds\n",
            "Step [3400/5032], Loss: 5.8166, Average Loss: 6.0500,Time: 1.47 seconds\n",
            "Step [3410/5032], Loss: 6.8357, Average Loss: 6.0493,Time: 1.72 seconds\n",
            "Step [3420/5032], Loss: 6.9740, Average Loss: 6.0474,Time: 1.39 seconds\n",
            "Step [3430/5032], Loss: 5.7496, Average Loss: 6.0465,Time: 1.50 seconds\n",
            "Step [3440/5032], Loss: 5.7564, Average Loss: 6.0468,Time: 1.75 seconds\n",
            "Step [3450/5032], Loss: 8.4739, Average Loss: 6.0475,Time: 1.80 seconds\n",
            "Step [3460/5032], Loss: 5.6957, Average Loss: 6.0473,Time: 1.21 seconds\n",
            "Step [3470/5032], Loss: 6.2810, Average Loss: 6.0468,Time: 1.83 seconds\n",
            "Step [3480/5032], Loss: 5.5479, Average Loss: 6.0465,Time: 1.86 seconds\n",
            "Step [3490/5032], Loss: 7.3109, Average Loss: 6.0472,Time: 2.11 seconds\n",
            "Step [3500/5032], Loss: 5.9543, Average Loss: 6.0459,Time: 1.76 seconds\n",
            "Step [3510/5032], Loss: 5.3800, Average Loss: 6.0447,Time: 1.61 seconds\n",
            "Step [3520/5032], Loss: 6.2718, Average Loss: 6.0451,Time: 1.25 seconds\n",
            "Step [3530/5032], Loss: 6.0146, Average Loss: 6.0440,Time: 1.38 seconds\n",
            "Step [3540/5032], Loss: 5.4720, Average Loss: 6.0431,Time: 1.52 seconds\n",
            "Step [3550/5032], Loss: 6.1740, Average Loss: 6.0427,Time: 1.95 seconds\n",
            "Step [3560/5032], Loss: 5.5707, Average Loss: 6.0420,Time: 1.66 seconds\n",
            "Step [3570/5032], Loss: 6.9340, Average Loss: 6.0410,Time: 1.47 seconds\n",
            "Step [3580/5032], Loss: 5.2516, Average Loss: 6.0405,Time: 1.16 seconds\n",
            "Step [3590/5032], Loss: 7.1458, Average Loss: 6.0405,Time: 1.47 seconds\n",
            "Step [3600/5032], Loss: 5.2812, Average Loss: 6.0394,Time: 1.63 seconds\n",
            "Step [3610/5032], Loss: 5.9286, Average Loss: 6.0378,Time: 1.71 seconds\n",
            "Step [3620/5032], Loss: 4.5579, Average Loss: 6.0372,Time: 1.87 seconds\n",
            "Step [3630/5032], Loss: 5.6018, Average Loss: 6.0364,Time: 1.54 seconds\n",
            "Step [3640/5032], Loss: 5.2002, Average Loss: 6.0348,Time: 1.58 seconds\n",
            "Step [3650/5032], Loss: 6.1586, Average Loss: 6.0329,Time: 1.48 seconds\n",
            "Step [3660/5032], Loss: 4.1680, Average Loss: 6.0321,Time: 1.79 seconds\n",
            "Step [3670/5032], Loss: 5.4685, Average Loss: 6.0317,Time: 1.38 seconds\n",
            "Step [3680/5032], Loss: 5.6433, Average Loss: 6.0310,Time: 1.66 seconds\n",
            "Step [3690/5032], Loss: 6.5826, Average Loss: 6.0301,Time: 1.54 seconds\n",
            "Step [3700/5032], Loss: 4.8918, Average Loss: 6.0290,Time: 1.35 seconds\n",
            "Step [3710/5032], Loss: 6.0017, Average Loss: 6.0294,Time: 1.50 seconds\n",
            "Step [3720/5032], Loss: 4.8402, Average Loss: 6.0295,Time: 1.44 seconds\n",
            "Step [3730/5032], Loss: 4.9205, Average Loss: 6.0293,Time: 1.51 seconds\n",
            "Step [3740/5032], Loss: 4.9285, Average Loss: 6.0278,Time: 1.80 seconds\n",
            "Step [3750/5032], Loss: 5.6165, Average Loss: 6.0277,Time: 1.46 seconds\n",
            "Step [3760/5032], Loss: 5.3138, Average Loss: 6.0261,Time: 2.03 seconds\n",
            "Step [3770/5032], Loss: 5.4088, Average Loss: 6.0242,Time: 1.74 seconds\n",
            "Step [3780/5032], Loss: 5.0857, Average Loss: 6.0239,Time: 1.34 seconds\n",
            "Step [3790/5032], Loss: 6.3387, Average Loss: 6.0235,Time: 1.51 seconds\n",
            "Step [3800/5032], Loss: 4.6243, Average Loss: 6.0224,Time: 1.51 seconds\n",
            "Step [3810/5032], Loss: 6.4795, Average Loss: 6.0224,Time: 1.72 seconds\n",
            "Step [3820/5032], Loss: 5.2912, Average Loss: 6.0219,Time: 1.56 seconds\n",
            "Step [3830/5032], Loss: 6.4676, Average Loss: 6.0214,Time: 2.03 seconds\n",
            "Step [3840/5032], Loss: 7.2442, Average Loss: 6.0206,Time: 1.75 seconds\n",
            "Step [3850/5032], Loss: 5.9928, Average Loss: 6.0205,Time: 2.24 seconds\n",
            "Step [3860/5032], Loss: 6.8693, Average Loss: 6.0191,Time: 1.50 seconds\n",
            "Step [3870/5032], Loss: 5.4360, Average Loss: 6.0174,Time: 1.32 seconds\n",
            "Step [3880/5032], Loss: 4.7754, Average Loss: 6.0156,Time: 1.31 seconds\n",
            "Step [3890/5032], Loss: 5.7813, Average Loss: 6.0152,Time: 1.52 seconds\n",
            "Step [3900/5032], Loss: 5.8903, Average Loss: 6.0154,Time: 1.76 seconds\n",
            "Step [3910/5032], Loss: 5.7031, Average Loss: 6.0152,Time: 1.64 seconds\n",
            "Step [3920/5032], Loss: 4.8127, Average Loss: 6.0143,Time: 1.97 seconds\n",
            "Step [3930/5032], Loss: 5.8215, Average Loss: 6.0127,Time: 1.43 seconds\n",
            "Step [3940/5032], Loss: 6.1540, Average Loss: 6.0110,Time: 1.54 seconds\n",
            "Step [3950/5032], Loss: 5.8011, Average Loss: 6.0106,Time: 1.40 seconds\n",
            "Step [3960/5032], Loss: 6.6730, Average Loss: 6.0104,Time: 1.60 seconds\n",
            "Step [3970/5032], Loss: 5.2066, Average Loss: 6.0102,Time: 2.13 seconds\n",
            "Step [3980/5032], Loss: 5.9998, Average Loss: 6.0097,Time: 1.69 seconds\n",
            "Step [3990/5032], Loss: 5.7767, Average Loss: 6.0082,Time: 1.79 seconds\n",
            "Step [4000/5032], Loss: 4.8904, Average Loss: 6.0076,Time: 2.62 seconds\n",
            "Step [4010/5032], Loss: 5.2854, Average Loss: 6.0075,Time: 1.42 seconds\n",
            "Step [4020/5032], Loss: 5.4726, Average Loss: 6.0064,Time: 1.58 seconds\n",
            "Step [4030/5032], Loss: 3.3352, Average Loss: 6.0055,Time: 1.45 seconds\n",
            "Step [4040/5032], Loss: 8.4283, Average Loss: 6.0063,Time: 4.81 seconds\n",
            "Step [4050/5032], Loss: 6.7266, Average Loss: 6.0055,Time: 1.86 seconds\n",
            "Step [4060/5032], Loss: 5.6239, Average Loss: 6.0043,Time: 1.49 seconds\n",
            "Step [4070/5032], Loss: 6.7006, Average Loss: 6.0045,Time: 1.90 seconds\n",
            "Step [4080/5032], Loss: 4.9623, Average Loss: 6.0039,Time: 1.81 seconds\n",
            "Step [4090/5032], Loss: 6.1449, Average Loss: 6.0038,Time: 1.43 seconds\n",
            "Step [4100/5032], Loss: 5.9298, Average Loss: 6.0012,Time: 1.68 seconds\n",
            "Step [4110/5032], Loss: 5.7081, Average Loss: 6.0004,Time: 1.35 seconds\n",
            "Step [4120/5032], Loss: 5.8319, Average Loss: 6.0005,Time: 1.53 seconds\n",
            "Step [4130/5032], Loss: 5.8869, Average Loss: 6.0003,Time: 1.21 seconds\n",
            "Step [4140/5032], Loss: 3.9786, Average Loss: 5.9996,Time: 1.92 seconds\n",
            "Step [4150/5032], Loss: 6.3611, Average Loss: 5.9985,Time: 1.61 seconds\n",
            "Step [4160/5032], Loss: 5.6892, Average Loss: 5.9976,Time: 1.85 seconds\n",
            "Step [4170/5032], Loss: 4.9163, Average Loss: 5.9966,Time: 1.60 seconds\n",
            "Step [4180/5032], Loss: 5.5977, Average Loss: 5.9961,Time: 1.44 seconds\n",
            "Step [4190/5032], Loss: 4.5350, Average Loss: 5.9962,Time: 1.73 seconds\n",
            "Step [4200/5032], Loss: 5.4983, Average Loss: 5.9947,Time: 1.48 seconds\n",
            "Step [4210/5032], Loss: 4.2371, Average Loss: 5.9928,Time: 1.61 seconds\n",
            "Step [4220/5032], Loss: 5.5988, Average Loss: 5.9922,Time: 1.99 seconds\n",
            "Step [4230/5032], Loss: 4.4115, Average Loss: 5.9912,Time: 2.21 seconds\n",
            "Step [4240/5032], Loss: 5.5935, Average Loss: 5.9907,Time: 1.60 seconds\n",
            "Step [4250/5032], Loss: 6.4720, Average Loss: 5.9905,Time: 1.68 seconds\n",
            "Step [4260/5032], Loss: 4.7791, Average Loss: 5.9893,Time: 1.66 seconds\n",
            "Step [4270/5032], Loss: 6.0425, Average Loss: 5.9889,Time: 1.75 seconds\n",
            "Step [4280/5032], Loss: 5.8988, Average Loss: 5.9882,Time: 1.54 seconds\n",
            "Step [4290/5032], Loss: 6.4652, Average Loss: 5.9882,Time: 1.54 seconds\n",
            "Step [4300/5032], Loss: 5.3891, Average Loss: 5.9869,Time: 1.68 seconds\n",
            "Step [4310/5032], Loss: 5.3454, Average Loss: 5.9870,Time: 1.47 seconds\n",
            "Step [4320/5032], Loss: 7.7612, Average Loss: 5.9869,Time: 1.53 seconds\n",
            "Step [4330/5032], Loss: 5.7860, Average Loss: 5.9858,Time: 1.39 seconds\n",
            "Step [4340/5032], Loss: 5.6966, Average Loss: 5.9859,Time: 1.77 seconds\n",
            "Step [4350/5032], Loss: 5.0552, Average Loss: 5.9852,Time: 1.37 seconds\n",
            "Step [4360/5032], Loss: 5.9935, Average Loss: 5.9844,Time: 1.76 seconds\n",
            "Step [4370/5032], Loss: 7.2924, Average Loss: 5.9840,Time: 1.69 seconds\n",
            "Step [4380/5032], Loss: 4.8858, Average Loss: 5.9832,Time: 1.10 seconds\n",
            "Step [4390/5032], Loss: 5.8128, Average Loss: 5.9829,Time: 1.37 seconds\n",
            "Step [4400/5032], Loss: 5.6274, Average Loss: 5.9827,Time: 1.66 seconds\n",
            "Step [4410/5032], Loss: 6.7042, Average Loss: 5.9828,Time: 1.55 seconds\n",
            "Step [4420/5032], Loss: 4.8823, Average Loss: 5.9829,Time: 1.49 seconds\n",
            "Step [4430/5032], Loss: 5.9091, Average Loss: 5.9813,Time: 1.67 seconds\n",
            "Step [4440/5032], Loss: 5.7643, Average Loss: 5.9802,Time: 1.65 seconds\n",
            "Step [4450/5032], Loss: 5.5725, Average Loss: 5.9800,Time: 1.77 seconds\n",
            "Step [4460/5032], Loss: 5.6847, Average Loss: 5.9806,Time: 2.13 seconds\n",
            "Step [4470/5032], Loss: 6.5779, Average Loss: 5.9804,Time: 1.66 seconds\n",
            "Step [4480/5032], Loss: 5.5884, Average Loss: 5.9793,Time: 1.46 seconds\n",
            "Step [4490/5032], Loss: 6.6140, Average Loss: 5.9779,Time: 1.32 seconds\n",
            "Step [4500/5032], Loss: 5.4199, Average Loss: 5.9774,Time: 1.81 seconds\n",
            "Step [4510/5032], Loss: 5.9507, Average Loss: 5.9790,Time: 2.55 seconds\n",
            "Step [4520/5032], Loss: 5.5228, Average Loss: 5.9777,Time: 1.36 seconds\n",
            "Step [4530/5032], Loss: 3.8512, Average Loss: 5.9760,Time: 1.28 seconds\n",
            "Step [4540/5032], Loss: 5.9268, Average Loss: 5.9745,Time: 1.40 seconds\n",
            "Step [4550/5032], Loss: 5.3583, Average Loss: 5.9735,Time: 1.80 seconds\n",
            "Step [4560/5032], Loss: 4.8380, Average Loss: 5.9740,Time: 1.58 seconds\n",
            "Step [4570/5032], Loss: 6.6054, Average Loss: 5.9730,Time: 1.46 seconds\n",
            "Step [4580/5032], Loss: 5.4673, Average Loss: 5.9719,Time: 1.45 seconds\n",
            "Step [4590/5032], Loss: 5.6839, Average Loss: 5.9711,Time: 1.50 seconds\n",
            "Step [4600/5032], Loss: 6.6316, Average Loss: 5.9711,Time: 2.00 seconds\n",
            "Step [4610/5032], Loss: 4.2695, Average Loss: 5.9701,Time: 2.30 seconds\n",
            "Step [4620/5032], Loss: 5.9367, Average Loss: 5.9701,Time: 1.80 seconds\n",
            "Step [4630/5032], Loss: 6.2562, Average Loss: 5.9693,Time: 1.34 seconds\n",
            "Step [4640/5032], Loss: 6.1747, Average Loss: 5.9684,Time: 1.48 seconds\n",
            "Step [4650/5032], Loss: 5.4504, Average Loss: 5.9675,Time: 1.74 seconds\n",
            "Step [4660/5032], Loss: 4.6273, Average Loss: 5.9659,Time: 1.55 seconds\n",
            "Step [4670/5032], Loss: 6.1109, Average Loss: 5.9645,Time: 1.78 seconds\n",
            "Step [4680/5032], Loss: 5.4282, Average Loss: 5.9639,Time: 1.29 seconds\n",
            "Step [4690/5032], Loss: 5.1036, Average Loss: 5.9631,Time: 1.82 seconds\n",
            "Step [4700/5032], Loss: 5.3632, Average Loss: 5.9625,Time: 1.60 seconds\n",
            "Step [4710/5032], Loss: 4.5468, Average Loss: 5.9620,Time: 1.71 seconds\n",
            "Step [4720/5032], Loss: 5.6407, Average Loss: 5.9609,Time: 1.97 seconds\n",
            "Step [4730/5032], Loss: 5.8542, Average Loss: 5.9603,Time: 1.31 seconds\n",
            "Step [4740/5032], Loss: 4.8424, Average Loss: 5.9596,Time: 1.69 seconds\n",
            "Step [4750/5032], Loss: 5.3728, Average Loss: 5.9590,Time: 1.16 seconds\n",
            "Step [4760/5032], Loss: 4.4039, Average Loss: 5.9580,Time: 1.67 seconds\n",
            "Step [4770/5032], Loss: 5.6352, Average Loss: 5.9564,Time: 1.55 seconds\n",
            "Step [4780/5032], Loss: 6.1795, Average Loss: 5.9568,Time: 2.17 seconds\n",
            "Step [4790/5032], Loss: 6.7274, Average Loss: 5.9558,Time: 1.63 seconds\n",
            "Step [4800/5032], Loss: 6.0273, Average Loss: 5.9551,Time: 1.56 seconds\n",
            "Step [4810/5032], Loss: 5.6162, Average Loss: 5.9541,Time: 1.38 seconds\n",
            "Step [4820/5032], Loss: 6.4626, Average Loss: 5.9541,Time: 2.14 seconds\n",
            "Step [4830/5032], Loss: 6.0234, Average Loss: 5.9536,Time: 1.30 seconds\n",
            "Step [4840/5032], Loss: 6.8539, Average Loss: 5.9530,Time: 1.72 seconds\n",
            "Step [4850/5032], Loss: 4.9965, Average Loss: 5.9519,Time: 1.42 seconds\n",
            "Step [4860/5032], Loss: 6.5521, Average Loss: 5.9521,Time: 1.54 seconds\n",
            "Step [4870/5032], Loss: 7.2110, Average Loss: 5.9521,Time: 1.37 seconds\n",
            "Step [4880/5032], Loss: 5.8819, Average Loss: 5.9519,Time: 1.75 seconds\n",
            "Step [4890/5032], Loss: 5.4652, Average Loss: 5.9511,Time: 1.68 seconds\n",
            "Step [4900/5032], Loss: 6.0006, Average Loss: 5.9509,Time: 1.77 seconds\n",
            "Step [4910/5032], Loss: 7.2653, Average Loss: 5.9506,Time: 1.77 seconds\n",
            "Step [4920/5032], Loss: 7.5097, Average Loss: 5.9508,Time: 2.15 seconds\n",
            "Step [4930/5032], Loss: 4.8543, Average Loss: 5.9500,Time: 1.98 seconds\n",
            "Step [4940/5032], Loss: 6.3059, Average Loss: 5.9492,Time: 1.79 seconds\n",
            "Step [4950/5032], Loss: 6.7171, Average Loss: 5.9485,Time: 1.45 seconds\n",
            "Step [4960/5032], Loss: 4.5931, Average Loss: 5.9480,Time: 1.69 seconds\n",
            "Step [4970/5032], Loss: 5.7223, Average Loss: 5.9468,Time: 1.88 seconds\n",
            "Step [4980/5032], Loss: 4.3495, Average Loss: 5.9459,Time: 1.42 seconds\n",
            "Step [4990/5032], Loss: 6.5319, Average Loss: 5.9457,Time: 1.32 seconds\n",
            "Step [5000/5032], Loss: 5.4146, Average Loss: 5.9444,Time: 1.41 seconds\n",
            "Step [5010/5032], Loss: 5.2312, Average Loss: 5.9431,Time: 1.67 seconds\n",
            "Step [5020/5032], Loss: 6.1463, Average Loss: 5.9420,Time: 1.25 seconds\n",
            "Step [5030/5032], Loss: 6.4530, Average Loss: 5.9417,Time: 1.45 seconds\n",
            "Step [10/503], Loss: 5.0301, Time: 0.59 seconds\n",
            "Step [20/503], Loss: 5.4463, Time: 0.61 seconds\n",
            "Step [30/503], Loss: 5.6665, Time: 0.55 seconds\n",
            "Step [40/503], Loss: 4.7755, Time: 0.57 seconds\n",
            "Step [50/503], Loss: 6.3413, Time: 0.50 seconds\n",
            "Step [60/503], Loss: 5.3449, Time: 0.64 seconds\n",
            "Step [70/503], Loss: 4.5972, Time: 0.47 seconds\n",
            "Step [80/503], Loss: 4.1254, Time: 0.60 seconds\n",
            "Step [90/503], Loss: 5.8045, Time: 0.58 seconds\n",
            "Step [100/503], Loss: 4.8280, Time: 0.71 seconds\n",
            "Step [110/503], Loss: 6.3237, Time: 0.61 seconds\n",
            "Step [120/503], Loss: 5.9481, Time: 0.54 seconds\n",
            "Step [130/503], Loss: 4.4411, Time: 0.40 seconds\n",
            "Step [140/503], Loss: 5.9722, Time: 0.73 seconds\n",
            "Step [150/503], Loss: 4.0812, Time: 0.61 seconds\n",
            "Step [160/503], Loss: 5.4235, Time: 0.66 seconds\n",
            "Step [170/503], Loss: 4.4429, Time: 0.55 seconds\n",
            "Step [180/503], Loss: 4.9303, Time: 0.44 seconds\n",
            "Step [190/503], Loss: 6.1458, Time: 0.55 seconds\n",
            "Step [200/503], Loss: 5.4313, Time: 0.58 seconds\n",
            "Step [210/503], Loss: 4.0628, Time: 0.53 seconds\n",
            "Step [220/503], Loss: 3.3991, Time: 0.52 seconds\n",
            "Step [230/503], Loss: 4.7931, Time: 0.57 seconds\n",
            "Step [240/503], Loss: 3.3958, Time: 0.51 seconds\n",
            "Step [250/503], Loss: 3.1169, Time: 0.40 seconds\n",
            "Step [260/503], Loss: 5.8174, Time: 0.56 seconds\n",
            "Step [270/503], Loss: 6.3398, Time: 0.53 seconds\n",
            "Step [280/503], Loss: 5.7483, Time: 0.63 seconds\n",
            "Step [290/503], Loss: 5.1173, Time: 0.49 seconds\n",
            "Step [300/503], Loss: 5.4592, Time: 0.48 seconds\n",
            "Step [310/503], Loss: 3.7523, Time: 0.50 seconds\n",
            "Step [320/503], Loss: 5.6246, Time: 0.69 seconds\n",
            "Step [330/503], Loss: 7.0962, Time: 0.65 seconds\n",
            "Step [340/503], Loss: 6.1900, Time: 0.80 seconds\n",
            "Step [350/503], Loss: 5.9842, Time: 0.78 seconds\n",
            "Step [360/503], Loss: 4.9765, Time: 0.45 seconds\n",
            "Step [370/503], Loss: 5.5448, Time: 0.53 seconds\n",
            "Step [380/503], Loss: 6.0956, Time: 0.68 seconds\n",
            "Step [390/503], Loss: 5.5789, Time: 0.56 seconds\n",
            "Step [400/503], Loss: 5.6631, Time: 0.54 seconds\n",
            "Step [410/503], Loss: 6.0778, Time: 0.54 seconds\n",
            "Step [420/503], Loss: 5.4293, Time: 0.76 seconds\n",
            "Step [430/503], Loss: 6.2510, Time: 0.65 seconds\n",
            "Step [440/503], Loss: 6.6467, Time: 0.71 seconds\n",
            "Step [450/503], Loss: 5.6618, Time: 0.53 seconds\n",
            "Step [460/503], Loss: 4.8452, Time: 0.62 seconds\n",
            "Step [470/503], Loss: 4.8823, Time: 0.44 seconds\n",
            "Step [480/503], Loss: 5.2677, Time: 0.52 seconds\n",
            "Step [490/503], Loss: 5.4957, Time: 0.57 seconds\n",
            "Step [500/503], Loss: 6.3142, Time: 0.61 seconds\n",
            "Training loss: 5.9405\n",
            "Validation loss: 5.4659\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'transformer_model.pth'\n",
        "model_path = '/content/drive/MyDrive/transformer/'\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "os.chdir(model_path)\n",
        "\n",
        "torch.save(model.state_dict(), model_name)\n"
      ],
      "metadata": {
        "id": "AAeca8BWhEfg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_saved = Transformer(\n",
        "    num_tokens=enc_voc_size, dim_model=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dropout_p=0.1\n",
        ").to(device)\n",
        "\n",
        "model_location = os.path.join(model_path, model_name)\n",
        "model_saved.load_state_dict(torch.load(model_location, map_location=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjlwCqPbnjdf",
        "outputId": "a5931c64-18e8-4abb-8f02-db27d33cea93"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, input_sequence, max_length=512, SOS_token=trg_sos_idx, EOS_token=trg_eos_idx):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
        "\n",
        "    num_tokens = len(input_sequence[0])\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Get source mask\n",
        "        tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
        "\n",
        "        pred = model(input_sequence, y_input, tgt_mask)\n",
        "\n",
        "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
        "        next_item = torch.tensor([[next_item]], device=device)\n",
        "\n",
        "        # Concatenate previous input with predicted best word\n",
        "        y_input = torch.cat((y_input, next_item), dim=1)\n",
        "\n",
        "        # Stop if model predicts end of sentence\n",
        "        if next_item.view(-1).item() == EOS_token:\n",
        "            break\n",
        "\n",
        "    return y_input.view(-1).tolist()"
      ],
      "metadata": {
        "id": "09QJSOOGUGT3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_encode(text, tokenizer):\n",
        "  input_ids = np.array(tokenizer.encode(text).ids)\n",
        "\n",
        "  SOS_token = np.array([trg_sos_idx])\n",
        "  EOS_token = np.array([trg_eos_idx])\n",
        "  #print(SOS_token, EOS_token)\n",
        "  X = np.concatenate((SOS_token, input_ids, EOS_token))\n",
        "  print(X)\n",
        "  return X"
      ],
      "metadata": {
        "id": "19xSbYUUoPOp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    torch.tensor([text_to_encode(\"sa dus la mare sa se uite la soare.\", tokenizer)], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 0, 1, 3]], dtype=torch.long, device=device)\n",
        "]\n",
        "\n",
        "for idx, example in enumerate(examples):\n",
        "    result = predict(model_saved, example)\n",
        "    print(f\"Example {idx}\")\n",
        "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
        "    print(f\"Continuation: {result}\")\n",
        "    print(f\"Text-OUTPUT:\", tokenizer.decode(result[1:-1]))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc_CBTvSUNdf",
        "outputId": "4384ed5e-3958-40bb-8744-b3b387953e5b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    0   604 13820   324  1781  1244   342 12581   268   324 21768    18\n",
            "     2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-97498473a7b2>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  torch.tensor([text_to_encode(\"sa dus la mare sa se uite la soare.\", tokenizer)], dtype=torch.long, device=device),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 0\n",
            "Input: [604, 13820, 324, 1781, 1244, 342, 12581, 268, 324, 21768, 18]\n",
            "Continuation: [0, 405, 446, 324, 703, 271, 324, 703, 324, 703, 271, 324, 703, 324, 703, 271, 324, 703, 271, 324, 703, 324, 703, 271, 324, 703, 18, 2]\n",
            "Text-OUTPUT: ... - la data de la data la data de la data la data de la data de la data la data de la data.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}