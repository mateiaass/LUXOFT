{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":8099829,"sourceType":"datasetVersion","datasetId":4783137},{"sourceId":8139602,"sourceType":"datasetVersion","datasetId":4812245}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport copy\nimport gc\nimport os\nimport re\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pandas as pd\nfrom spacy.lang.en import English\nfrom transformers.tokenization_utils import PreTrainedTokenizerBase\nfrom transformers.models.deberta_v2 import DebertaV2ForTokenClassification, DebertaV2TokenizerFast\nfrom transformers.trainer import Trainer\nfrom transformers.training_args import TrainingArguments\nfrom transformers.trainer_utils import EvalPrediction\nfrom transformers.data.data_collator import DataCollatorForTokenClassification\nfrom datasets import Dataset, DatasetDict, concatenate_datasets\nimport wandb\n\nTRAINING_MODEL_PATH = \"/kaggle/input/mdeberta-v3-base/mdeberta-v3-base\"\nTRAINING_MAX_LENGTH = 512\nEVAL_MAX_LENGTH = 512\nCONF_THRESH = 0.9\nLR = 2.5e-5\nLR_SCHEDULER_TYPE = \"linear\"\nNUM_EPOCHS = 3\nBATCH_SIZE = 2\nEVAL_BATCH_SIZE = 2\nGRAD_ACCUMULATION_STEPS = 2 // BATCH_SIZE\nWARMUP_RATIO = 0.1\nWEIGHT_DECAY = 0.01\nAMP = True\nFREEZE_EMBEDDING = False\nFREEZE_LAYERS = 6\nN_SPLITS = 3\nNEGATIVE_RATIO = 0.3\nOUTPUT_DIR = \"output\"\nPath(OUTPUT_DIR).mkdir(exist_ok=True)\n\nargs = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    fp16=AMP,\n    learning_rate=LR,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n    report_to=\"none\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    eval_delay=100,\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=1,\n    logging_steps=10,\n    metric_for_best_model=\"f5\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    overwrite_output_dir=True,\n    lr_scheduler_type=LR_SCHEDULER_TYPE,\n    warmup_ratio=WARMUP_RATIO,\n    weight_decay=WEIGHT_DECAY,\n)\n\nwith Path(\"/kaggle/input/vlad-dataset/pii.json\").open(\"r\") as f:\n    original_data = json.load(f)\n\nall_labels = [\n    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O'\n]\nid2label = {i: l for i, l in enumerate(all_labels)}\nlabel2id = {v: k for k, v in id2label.items()}\ntarget = [l for l in all_labels if l != \"O\"]\n\nclass CustomTokenizer:\n    def __init__(self, tokenizer: PreTrainedTokenizerBase, label2id: dict, max_length: int) -> None:\n        self.tokenizer = tokenizer\n        self.label2id = label2id\n        self.max_length = max_length\n\n    def __call__(self, example: dict) -> dict:\n        text, labels, token_map = [], [], []\n\n        for idx, (t, l, ws) in enumerate(\n            zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"])\n        ):\n            text.append(t)\n            labels.extend([l] * len(t))\n            token_map.extend([idx]*len(t))\n\n            if ws:\n                text.append(\" \")\n                labels.append(\"O\")\n                token_map.append(-1)\n\n        text = \"\".join(text)\n        labels = np.array(labels)\n\n        tokenized = self.tokenizer(\n            \"\".join(text),\n            return_offsets_mapping=True,\n            truncation=True,\n            max_length=self.max_length\n        )\n\n        token_labels = []\n\n        for start_idx, end_idx in tokenized.offset_mapping:\n            if start_idx == 0 and end_idx == 0:\n                token_labels.append(self.label2id[\"O\"])\n                continue\n\n            if text[start_idx].isspace():\n                start_idx += 1\n\n            token_labels.append(self.label2id[labels[start_idx]])\n\n        length = len(tokenized.input_ids)\n\n        return {**tokenized, \"labels\": token_labels, \"length\": length, \"token_map\": token_map}\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained(TRAINING_MODEL_PATH)\ntrain_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=TRAINING_MAX_LENGTH)\neval_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=EVAL_MAX_LENGTH)\n\nds = DatasetDict()\n\nfor key, data in zip([\"original\"], [original_data]):\n    ds[key] = Dataset.from_dict({\n        \"full_text\": [x[\"full_text\"] for x in data],\n        \"document\": [str(x[\"document\"]) for x in data],\n        \"tokens\": [x[\"tokens\"] for x in data],\n        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n        \"provided_labels\": [x[\"labels\"] for x in data],\n    })\n\ndef find_span(target: list[str], document: list[str]) -> list[list[int]]:\n    idx = 0\n    spans = []\n    span = []\n\n    for i, token in enumerate(document):\n        if token != target[idx]:\n            idx = 0\n            span = []\n            continue\n        span.append(i)\n        idx += 1\n        if idx == len(target):\n            spans.append(span)\n            span = []\n            idx = 0\n            continue\n\n    return spans\n\n\nclass PRFScore:\n    def __init__(\n        self,\n        *,\n        tp: int = 0,\n        fp: int = 0,\n        fn: int = 0,\n    ) -> None:\n        self.tp = tp\n        self.fp = fp\n        self.fn = fn\n\n    def __len__(self) -> int:\n        return self.tp + self.fp + self.fn\n\n    def __iadd__(self, other):\n        self.tp += other.tp\n        self.fp += other.fp\n        self.fn += other.fn\n        return self\n\n    def __add__(self, other):\n        return PRFScore(\n            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n        )\n\n    def score_set(self, cand: set, gold: set) -> None:\n        self.tp += len(cand.intersection(gold))\n        self.fp += len(cand - gold)\n        self.fn += len(gold - cand)\n\n    @property\n    def precision(self) -> float:\n        return self.tp / (self.tp + self.fp + 1e-100)\n\n    @property\n    def recall(self) -> float:\n        return self.tp / (self.tp + self.fn + 1e-100)\n\n    @property\n    def f1(self) -> float:\n        p = self.precision\n        r = self.recall\n        return 2 * ((p * r) / (p + r + 1e-100))\n\n    @property\n    def f5(self) -> float:\n        beta = 5\n        p = self.precision\n        r = self.recall\n\n        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n        return fbeta\n\n    def to_dict(self) -> dict[str, float]:\n        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n\nclass MetricsComputer:\n    nlp = English()\n\n    def __init__(self, eval_ds: Dataset, label2id: dict, conf_thresh: float = 0.9) -> None:\n        self.ds = eval_ds.remove_columns(\"labels\").rename_columns({\"provided_labels\": \"labels\"})\n        self.gt_df = self.create_gt_df(self.ds)\n        self.label2id = label2id\n        self.confth = conf_thresh\n        self._search_gt()\n\n    def __call__(self, eval_preds: EvalPrediction) -> dict:\n        pred_df = self.create_pred_df(eval_preds.predictions)\n        return self.compute_metrics_from_df(self.gt_df, pred_df)\n\n    def _search_gt(self) -> None:\n        email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n        phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n        self.emails = []\n        self.phone_nums = []\n\n        for _data in self.ds:\n            for token_idx, token in enumerate(_data[\"tokens\"]):\n                if re.fullmatch(email_regex, token) is not None:\n                    self.emails.append(\n                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n                    )\n            matches = phone_num_regex.findall(_data[\"full_text\"])\n            if not matches:\n                continue\n            for match in matches:\n                target = [t.text for t in self.nlp.tokenizer(match)]\n                matched_spans = find_span(target, _data[\"tokens\"])\n            for matched_span in matched_spans:\n                for intermediate, token_idx in enumerate(matched_span):\n                    prefix = \"I\" if intermediate else \"B\"\n                    self.phone_nums.append(\n                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n                    )\n\n    @staticmethod\n    def create_gt_df(ds: Dataset):\n        gt = []\n        for row in ds:\n            for token_idx, (token, label) in enumerate(zip(row[\"tokens\"], row[\"labels\"])):\n                if label == \"O\":\n                    continue\n                gt.append(\n                    {\"document\": row[\"document\"], \"token\": token_idx, \"label\": label, \"token_str\": token}\n                )\n        gt_df = pd.DataFrame(gt)\n        gt_df[\"row_id\"] = gt_df.index\n\n        return gt_df\n\n    def create_pred_df(self, logits: np.ndarray) -> pd.DataFrame:\n        prediction = logits\n        o_index = self.label2id[\"O\"]\n        preds = prediction.argmax(-1)\n        preds_without_o = prediction.copy()\n        preds_without_o[:,:,o_index] = 0\n        preds_without_o = preds_without_o.argmax(-1)\n        o_preds = prediction[:,:,o_index]\n        preds_final = np.where(o_preds < self.confth, preds_without_o , preds)\n\n        pairs = set()\n        processed = []\n\n        for p_doc, token_map, offsets, tokens, doc in zip(\n            preds_final, self.ds[\"token_map\"], self.ds[\"offset_mapping\"], self.ds[\"tokens\"], self.ds[\"document\"]\n        ):\n            for p_token, (start_idx, end_idx) in zip(p_doc, offsets):\n                label_pred = id2label[p_token]\n\n                if start_idx + end_idx == 0:\n                    continue\n\n                if token_map[start_idx] == -1:\n                    start_idx += 1\n\n                while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                    start_idx += 1\n\n                if start_idx >= len(token_map):\n                    break\n\n                token_id = token_map[start_idx]\n                pair = (doc, token_id)\n\n                if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n                    continue\n\n                if pair in pairs:\n                    continue\n\n                processed.append(\n                    {\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]}\n                )\n                pairs.add(pair)\n\n        pred_df = pd.DataFrame(processed + self.emails + self.phone_nums)\n        pred_df[\"row_id\"] = list(range(len(pred_df)))\n\n        return pred_df\n\n    def compute_metrics_from_df(self, gt_df, pred_df):\n        references = {(row.document, row.token, row.label) for row in gt_df.itertuples()}\n        predictions = {(row.document, row.token, row.label) for row in pred_df.itertuples()}\n\n        score_per_type = defaultdict(PRFScore)\n        references = set(references)\n\n        for ex in predictions:\n            pred_type = ex[-1]\n            if pred_type != 'O':\n                pred_type = pred_type[2:]\n\n            if pred_type not in score_per_type:\n                score_per_type[pred_type] = PRFScore()\n\n            if ex in references:\n                score_per_type[pred_type].tp += 1\n                references.remove(ex)\n            else:\n                score_per_type[pred_type].fp += 1\n\n        for doc, tok, ref_type in references:\n            if ref_type != 'O':\n                ref_type = ref_type[2:]\n\n            if ref_type not in score_per_type:\n                score_per_type[ref_type] = PRFScore()\n            score_per_type[ref_type].fn += 1\n\n        totals = PRFScore()\n\n        for prf in score_per_type.values():\n            totals += prf\n\n        return {\n            \"precision\": totals.precision,\n            \"recall\": totals.recall,\n            \"f5\": totals.f5,\n            **{\n                f\"{v_k}-{k}\": v_v\n                for k in set([l[2:] for l in self.label2id.keys() if l!= 'O'])\n                for v_k, v_v in score_per_type[k].to_dict().items()\n            },\n        }\n\nclass ModelInit:\n    def __init__(\n        self,\n        checkpoint: str,\n        id2label: dict,\n        label2id: dict,\n        freeze_embedding: bool,\n        freeze_layers: int,\n    ) -> None:\n        self.model = DebertaV2ForTokenClassification.from_pretrained(\n            checkpoint,\n            num_labels=len(id2label),\n            id2label=id2label,\n            label2id=label2id,\n            ignore_mismatched_sizes=True\n        )\n        for param in self.model.deberta.embeddings.parameters():\n            param.requires_grad = False if freeze_embedding else True\n        for layer in self.model.deberta.encoder.layer[:freeze_layers]:\n            for param in layer.parameters():\n                param.requires_grad = False\n        self.weight = copy.deepcopy(self.model.state_dict())\n\n    def __call__(self) -> DebertaV2ForTokenClassification:\n        self.model.load_state_dict(self.weight)\n        return self.model\n    \n    def save_model(self, path: str) -> None:\n        self.model.save_pretrained(path)\n\nmodel_init = ModelInit(\n    TRAINING_MODEL_PATH,\n    id2label=id2label,\n    label2id=label2id,\n    freeze_embedding=FREEZE_EMBEDDING,\n    freeze_layers=FREEZE_LAYERS,\n)\n\nfolds = [\n    (\n        np.array([i for i, d in enumerate(ds[\"original\"][\"document\"]) if int(d) % N_SPLITS != s]),\n        np.array([i for i, d in enumerate(ds[\"original\"][\"document\"]) if int(d) % N_SPLITS == s])\n    )\n    for s in range(N_SPLITS)\n]\n\nnegative_idxs = [i for i, labels in enumerate(ds[\"original\"][\"provided_labels\"]) if not any(np.array(labels) != \"O\")]\nexclude_indices = negative_idxs[int(len(negative_idxs) * NEGATIVE_RATIO):]\n\nfor fold_idx, (train_idx, eval_idx) in enumerate(folds):\n    args.run_name = f\"fold-{fold_idx}\"\n    args.output_dir = os.path.join(OUTPUT_DIR, f\"fold_{fold_idx}\")\n    original_ds = ds[\"original\"].select([i for i in train_idx if i not in exclude_indices])\n    train_ds = concatenate_datasets([original_ds, ds[\"extra\"]])\n    train_ds = train_ds.map(train_encoder, num_proc=os.cpu_count())\n    eval_ds = ds[\"original\"].select(eval_idx)\n    eval_ds = eval_ds.map(eval_encoder, num_proc=os.cpu_count())\n    trainer = Trainer(\n        args=args,\n        model_init=model_init,\n        train_dataset=train_ds,\n        eval_dataset=eval_ds,\n        tokenizer=tokenizer,\n        compute_metrics=MetricsComputer(eval_ds=eval_ds, label2id=label2id),\n        data_collator=DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16),\n    )\n    trainer.train()\n    \n    eval_res = trainer.evaluate(eval_dataset=eval_ds)\n    with open(os.path.join(args.output_dir, \"eval_result.json\"), \"w\") as f:\n        json.dump(eval_res, f)\n    del trainer\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_uuid":"21267f02-44ce-470b-a818-6b45ae7ab300","_cell_guid":"9392ae9f-b0fc-4855-bcc3-de9712844537","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}