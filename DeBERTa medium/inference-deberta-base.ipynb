{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":8124970,"sourceType":"datasetVersion","datasetId":4801547}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\nimport re\nimport bisect\nfrom pathlib import Path\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom spacy.lang.en import English\nfrom transformers.models.deberta_v2 import DebertaV2ForTokenClassification, DebertaV2TokenizerFast\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\nfrom transformers.trainer import Trainer\nfrom transformers.training_args import TrainingArguments\nfrom transformers.data.data_collator import DataCollatorForTokenClassification\n\nINFERENCE_MAX_LENGTH = 3500\nCONF_THRESH = 0.90\nURL_THRESH = 0.1\nAMP = True\nMODEL_PATH = '/kaggle/input/trained-mdeberta-v3-base/mdeberta-v3-base-tok/fold-3'\nDATA_DIR = '/kaggle/input/pii-detection-removal-from-educational-data/'\n\nnlp = English()\n\ndef find_span(target: list[str], document: list[str]) -> list[list[int]]:\n    idx = 0\n    spans = []\n    span = []\n\n    for i, token in enumerate(document):\n        if token != target[idx]:\n            idx = 0\n            span = []\n            continue\n        span.append(i)\n        idx += 1\n        if idx == len(target):\n            spans.append(span)\n            span = []\n            idx = 0\n            continue\n    \n    return spans\n\ndef spacy_to_hf(data: dict, idx: int) -> slice:\n    str_range = np.where(np.array(data[\"token_map\"]) == idx)[0]\n    start_idx = bisect.bisect_left([off[1] for off in data[\"offset_mapping\"]], str_range.min())\n    end_idx = start_idx\n    while end_idx < len(data[\"offset_mapping\"]):\n        if str_range.max() > data[\"offset_mapping\"][end_idx][1]:\n            end_idx += 1\n            continue\n        break\n    token_range = slice(start_idx, end_idx+1)\n    return token_range\n\nclass CustomTokenizer:\n    def __init__(self, tokenizer: PreTrainedTokenizerBase, max_length: int) -> None:\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __call__(self, example: dict) -> dict:\n        text = []\n        token_map = []\n\n        for idx, (t, ws) in enumerate(zip(example[\"tokens\"], example[\"trailing_whitespace\"])):\n            text.append(t)\n            token_map.extend([idx]*len(t))\n            if ws:\n                text.append(\" \")\n                token_map.append(-1)\n\n        tokenized = self.tokenizer(\n            \"\".join(text),\n            return_offsets_mapping=True,\n            truncation=True,\n            max_length=self.max_length,\n        )\n\n        return {**tokenized,\"token_map\": token_map,}\n\nwith open(str(Path(DATA_DIR).joinpath(\"test.json\")), \"r\") as f:\n    data = json.load(f)\n\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [x[\"document\"] for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n})\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained(MODEL_PATH)\nds = ds.map(CustomTokenizer(tokenizer=tokenizer, max_length=INFERENCE_MAX_LENGTH), num_proc=os.cpu_count())\n\nmodel = DebertaV2ForTokenClassification.from_pretrained(MODEL_PATH)\ncollator = DataCollatorForTokenClassification(tokenizer)\nargs = TrainingArguments(\".\", per_device_eval_batch_size=1, report_to=\"none\", fp16=AMP)\ntrainer = Trainer(\n    model=model, args=args, data_collator=collator, tokenizer=tokenizer,\n)\n\npredictions = trainer.predict(ds).predictions\npred_softmax = torch.softmax(torch.from_numpy(predictions), dim=2).numpy()\nid2label = model.config.id2label\no_index = model.config.label2id[\"O\"]\npreds = predictions.argmax(-1)\npreds_without_o = pred_softmax.copy()\npreds_without_o[:,:,o_index] = 0\npreds_without_o = preds_without_o.argmax(-1)\no_preds = pred_softmax[:,:,o_index]\npreds_final = np.where(o_preds < CONF_THRESH, preds_without_o , preds)\n\nprocessed =[]\npairs = set()\n\nfor p, token_map, offsets, tokens, doc in zip(\n    preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]\n):\n    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n        label_pred = id2label[token_pred]\n\n        if start_idx + end_idx == 0:\n            continue\n\n        if token_map[start_idx] == -1:\n            start_idx += 1\n\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n\n        if start_idx >= len(token_map): \n            break\n\n        token_id = token_map[start_idx]\n        pair = (doc, token_id)\n\n        if label_pred in (\"O\", \"B-EMAIL\", \"B-URL_PERSONAL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n            continue        \n\n        if pair in pairs:\n            continue\n            \n        processed.append(\n            {\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]}\n        )\n        pairs.add(pair)\n\nurl_whitelist = [\n    \"wikipedia.org\",\n    \"coursera.org\",\n    \"google.com\",\n    \".gov\",\n]\nurl_whitelist_regex = re.compile(\"|\".join(url_whitelist))\n\nfor row_idx, _data in enumerate(ds):\n    for token_idx, token in enumerate(_data[\"tokens\"]):\n        if not nlp.tokenizer.url_match(token):\n            continue\n        print(f\"Found URL: {token}\")\n        if url_whitelist_regex.search(token) is not None:\n            print(\"The above is in the whitelist\")\n            continue\n        input_idxs = spacy_to_hf(_data, token_idx)\n        probs = pred_softmax[row_idx, input_idxs, model.config.label2id[\"B-URL_PERSONAL\"]]\n        if probs.mean() > URL_THRESH:\n            print(\"The above is PII\")\n            processed.append(\n                {\n                    \"document\": _data[\"document\"], \n                    \"token\": token_idx, \n                    \"label\": \"B-URL_PERSONAL\", \n                    \"token_str\": token\n                }\n            )\n            pairs.add((_data[\"document\"], token_idx))\n        else:\n            print(\"The above is not PII\")\n\nemail_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\nphone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\nemails = []\nphone_nums = []\n\nfor _data in ds:\n    for token_idx, token in enumerate(_data[\"tokens\"]):\n        if re.fullmatch(email_regex, token) is not None:\n            emails.append(\n                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n            )\n    matches = phone_num_regex.findall(_data[\"full_text\"])\n    if not matches:\n        continue\n    for match in matches:\n        target = [t.text for t in nlp.tokenizer(match)]\n        matched_spans = find_span(target, _data[\"tokens\"])\n    for matched_span in matched_spans:\n        for intermediate, token_idx in enumerate(matched_span):\n            prefix = \"I\" if intermediate else \"B\"\n            phone_nums.append(\n                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n            )\n\ndf = pd.DataFrame(processed + emails + phone_nums)\ndf[\"row_id\"] = list(range(len(df)))\ndf.head(100)\n\ndf[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{"_uuid":"ae34de16-5ea3-4495-a438-9d0ef11e7eb8","_cell_guid":"87ab1c7c-205b-42f1-85a3-624ac45102b2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}