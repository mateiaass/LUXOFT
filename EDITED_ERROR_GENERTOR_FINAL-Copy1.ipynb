{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1EpaF-9x5-t_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: rowordnet in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rowordnet) (3.2.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rowordnet) (5.2.1)\n",
      "Requirement already satisfied: phunspell in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: spylls in c:\\users\\vmatei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from phunspell) (0.1.7)\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "# !pip install rowordnet\n",
    "# !pip install phunspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_12MmtN8_N4v"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3525423193dc4866b32a6f474ae33543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import rowordnet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import phunspell\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "# 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\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v--JtrbxANfb"
   },
   "outputs": [],
   "source": [
    "wn = rowordnet.RoWordNet()\n",
    "pspell = phunspell.Phunspell('ro_RO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7KMgCyhE55L5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730e7a9a764545649c80fdeb73dea6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"mateiaassAI/MarcellP\", streaming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IEZBuJ6OXrgT"
   },
   "outputs": [],
   "source": [
    "dictio_words = {1: {0: 0.0, 1: 0.5, 2: 1.01},\n",
    "          3: {1: 0.0, 2: 0.5, 3: 1.01},\n",
    "          6: {2: 0.0, 3: 0.3, 4: 0.75, 5: 1.01},\n",
    "          9: {3: 0.0, 4: 0.15, 5: 0.4, 6: 0.7, 7: 1.01},\n",
    "          16: {3: 0.0, 4: 0.1, 5: 0.4, 6: 0.7, 7: 0.85, 8: 1.01},\n",
    "          20: {4: 0.0, 5: 0.1, 6: 0.4, 7: 0.7, 8: 0.85, 9: 1.01},\n",
    "          30: {5: 0.0, 6: 0.1, 7: 0.3, 8: 0.5, 9: 0.8, 10: 1.01},\n",
    "          100000000: None}\n",
    "\n",
    "def number_of_words_changed(sentence_length):\n",
    "  prob = random.randint(1, 100) / 100\n",
    "  keys = list(dictio_words.keys())\n",
    "  little_dict = {}\n",
    "  for i in range(len(dictio_words) - 1):\n",
    "      if sentence_length >= keys[i] and sentence_length < keys[i+1]:\n",
    "          little_dict = dictio_words[keys[i]]\n",
    "          values = list(little_dict.values())\n",
    "          for j in range(len(values) - 1):\n",
    "              if prob >= values[j] and prob < values[j+1]:\n",
    "                  return list(little_dict.keys())[list(little_dict.values()).index(values[j])]\n",
    "\n",
    "dictio_spell = {1: {0: 0.0, 1: 1.01},\n",
    "          3: {1: 0.0, 2: 1.01},\n",
    "          5: {1: 0.0, 2: 0.8, 3: 1.01},\n",
    "          10: {1: 0.0, 2: 0.75, 3: 0.9, 4: 1.01},\n",
    "          100000000: None}\n",
    "\n",
    "def number_of_misspellings(word_length):\n",
    "\n",
    "\n",
    "  prob = random.randint(1, 100) / 100\n",
    "\n",
    "  keys = list(dictio_spell.keys())\n",
    "  little_dict = {}\n",
    "  for i in range(len(dictio_spell) - 1):\n",
    "      if word_length >= keys[i] and word_length < keys[i+1]:\n",
    "          little_dict = dictio_spell[keys[i]]\n",
    "          values = list(little_dict.values())\n",
    "          for j in range(len(values) - 1):\n",
    "              if prob >= values[j] and prob < values[j+1]:\n",
    "                  return list(little_dict.keys())[list(little_dict.values()).index(values[j])]\n",
    "\n",
    "mispell_replacement_candidates = {\n",
    "    'a': ['ea', 'ă', 'â', 'au', 'q', 'w', 's', 'z'],\n",
    "    'ă': ['e', 'â', 'a', 'p', 'î', 'ț', 'ș'],\n",
    "    'â': ['î', 'ă', 'a', 'ț'],\n",
    "    'b': ['v', 'd', 'p', 'v', 'g', 'h', 'n'],\n",
    "    'c': ['k', 'g', 'x', 'd', 'f', 'v'],\n",
    "    'd': ['b', 'p', 'e', 'r', 's', 'f', 'x', 'c'],\n",
    "    'e': ['ie', 'i', 'ă', 'ea', 'esc', 'w', 's', 'd', 'r'],\n",
    "    'f': ['r', 'g', 'd', 'c', 'v'],\n",
    "    'g': ['c', 't', 'h', 'f', 'v', 'b'],\n",
    "    'h': ['y', 'g', 'j', 'b', 'n'],\n",
    "    'i': ['ii', 'iii', 'e', 'î', 'y', 'ie', 'l', 'u', 'o', 'j', 'k'],\n",
    "    'î': ['â', 'i', 'ă', 'ț'],\n",
    "    'j': ['ș', 'u', 'h', 'k', 'n', 'm'],\n",
    "    'k': ['c', 'i', 'j', 'l', 'm'],\n",
    "    'l': ['n', 'i', 'o', 'p', 'k'],\n",
    "    'm': ['n', 'j', 'k'],\n",
    "    'n': ['m', 'l', 'j', 'h', 'b'],\n",
    "    'o': ['u', 'or', 'i', 'k', 'l', 'p'],\n",
    "    'p': ['q', 'b', 'd', 'o', 'l'],\n",
    "    'q': ['c', 'p', 'w', 'a'],\n",
    "    'r': ['e', 'd', 't', 'f'],\n",
    "    's': ['ș', 'z', 'w', 'a', 'd', 's', 'z'],\n",
    "    'ș': ['j', 's', 'ă', 'ț', 'p', 'l'],\n",
    "    't': ['ț', 'r', 'f', 'g', 'y'],\n",
    "    'ț': ['t', 'ă', 'â', 'ș', 'î'],\n",
    "    'u': ['v', 'o', 'y', 'h', 'j', 'i'],\n",
    "    'v': ['w', 'u', 'b', 'vr', 'c', 'f', 'g'],\n",
    "    'w': ['v', 'q', 'e', 'a', 's'],\n",
    "    'x': ['cs', 'gz', 'cș', 's', 'd', 'z', 'c'],\n",
    "    'y': ['i', 't', 'u', 'g', 'h'],\n",
    "    'z': ['s', 'a', 'x'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cTfV5ukCbpFx"
   },
   "outputs": [],
   "source": [
    "punctuation_dict = {\n",
    "    ' ': {' ': 0.00, ',': 0.95, '.': 0.975, '-----': 1.01},\n",
    "    ',': {' ': 0.00, ',': 0.41, '.': 0.95, '-----': 1.01},\n",
    "    ';': {' ': 0.00, ',': 0.85, ';': 0.90, '-----': 1.01},\n",
    "    ':': {' ': 0.00, ',': 0.90, ':': 0.92, '-----': 1.01},\n",
    "    '-': {' ': 0.00, '-': 0.80, '-----': 1.01},\n",
    "    '.': {' ': 0.00, ';': 0.20, '.': 0.23, '...': 0.97, '-----': 1.01},\n",
    "    '?': {' ': 0.00, '.': 0.80, '?': 0.84, '!': 0.96, '-----': 1.01},\n",
    "    '!': {' ': 0.00, '.': 0.80, '?': 0.90, '!': 0.97, '-----': 1.01},\n",
    "    '...': {' ': 0.00, '...': 0.80, '-----': 1.01}\n",
    "  }\n",
    "\n",
    "def punctuation_substitution(letter):\n",
    "  prob = random.randint(1, 100) / 100\n",
    "  keys = list(punctuation_dict.keys())\n",
    "  little_dict = punctuation_dict[letter]\n",
    "  values = list(little_dict.values())\n",
    "  for j in range(len(values) - 1):\n",
    "    if prob >= values[j] and prob < values[j+1]:\n",
    "      return list(little_dict.keys())[list(little_dict.values()).index(values[j])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nY0_ga4pbg9U"
   },
   "outputs": [],
   "source": [
    "def copy_word(sentence, word, isSpace = True):\n",
    "  if isSpace:\n",
    "    sentence = sentence + \" \" + word\n",
    "  else:\n",
    "    sentence = sentence + word\n",
    "  return sentence\n",
    "\n",
    "def concatenate(sentence, word1, word2, isSpace = True):\n",
    "  if isSpace:\n",
    "    sentence = sentence + \" \" + word1 + word2\n",
    "  else:\n",
    "    sentence = sentence + word1 + word2\n",
    "  return sentence\n",
    "\n",
    "def transpose(sentence, word1, word2, isSpace = True):\n",
    "  if isSpace:\n",
    "    sentence = sentence + \" \" + word2 + \" \" + word1\n",
    "  else:\n",
    "    sentence = sentence + word2 + \" \" + word1\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pboKH9FYyXSO"
   },
   "outputs": [],
   "source": [
    "def copy_letter(word, letter):\n",
    "  return word + letter\n",
    "\n",
    "def transpose_letters(word, letter1, letter2):\n",
    "  return word + letter2 + letter1\n",
    "\n",
    "def replace_letter(word, letter):\n",
    "  return word + letter\n",
    "\n",
    "def insert_letter(word, letter, new_letter):\n",
    "  return word + letter + new_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "APZfrT8BvA2o"
   },
   "outputs": [],
   "source": [
    "alphabet = \"аăâbcdefghiîjklmnopqrsștțuvwxyz\"\n",
    "alphabet2 = \"аăâbcdefghiîjklmnopqrsștțuvwxyz-\"\n",
    "\n",
    "def misspell(word):\n",
    "  number_of_changed_letters = number_of_misspellings(len(word))\n",
    "  letters_to_be_changed = np.random.choice(np.arange(0, len(word)), size=number_of_changed_letters, replace=False).tolist()\n",
    "  wrong_word = \"\"\n",
    "  skip_flag = False\n",
    "  for i in range(len(word)):\n",
    "    if skip_flag == True:\n",
    "      skip_flag = False\n",
    "      continue\n",
    "    letter = word[i]\n",
    "\n",
    "    # Letter not to be changed\n",
    "    if i not in letters_to_be_changed:\n",
    "      wrong_word = copy_letter(wrong_word, letter)\n",
    "      continue\n",
    "\n",
    "    # Type of misspelling - Distribution of Probability\n",
    "    letter_error_action = random.randint(1, 100) / 100\n",
    "\n",
    "    # DELETION\n",
    "    if letter_error_action <= 0.25:\n",
    "      continue\n",
    "\n",
    "    # INSERTION\n",
    "    if letter_error_action > 0.25 and letter_error_action <= 0.40:\n",
    "      alphabet_letter = alphabet2[random.randint(0, len(alphabet2) - 1)]\n",
    "      wrong_word = insert_letter(wrong_word, letter, alphabet_letter)\n",
    "      continue\n",
    "\n",
    "    # TRANSPOSITION\n",
    "    if letter_error_action > 0.40 and letter_error_action <= 0.65 and i < len(word) - 1:\n",
    "      next_letter = word[i + 1]\n",
    "      wrong_word = transpose_letters(wrong_word, letter, next_letter)\n",
    "      skip_flag = True\n",
    "      continue\n",
    "\n",
    "    # REPLACEMENT\n",
    "    if letter_error_action > 0.65:\n",
    "      if letter.lower() not in alphabet:\n",
    "        alphabet_letter = letter\n",
    "      else:\n",
    "        # if letter_error_action <= 0.68 and i == 0:\n",
    "        #   alphabet_letter = letter.lower() if letter.islower() else letter.upper()\n",
    "\n",
    "        if letter_error_action <= 0.7 and i == len(word) - 1 and letter.lower() in ['a', 'i', 'l']:\n",
    "          continue\n",
    "\n",
    "        if letter_error_action > 0.90:\n",
    "          alphabet_letter = alphabet[random.randint(0, len(alphabet) - 1)]\n",
    "        else:\n",
    "            alphabet_letter = random.choice(mispell_replacement_candidates[letter.lower()])\n",
    "\n",
    "      wrong_word = replace_letter(wrong_word, alphabet_letter)\n",
    "      continue\n",
    "\n",
    "    # DEFAULT\n",
    "    wrong_word = copy_letter(wrong_word, letter)\n",
    "\n",
    "  return wrong_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PcO7ZeN3xlWW"
   },
   "outputs": [],
   "source": [
    "def morphologic_dex(word):\n",
    "  if len(word) <= 3:\n",
    "    return word\n",
    "\n",
    "  url = f'https://dexonline.ro/definitie/{word}/paradigma'\n",
    "  response = requests.get(url)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    para_lexemes = soup.find_all('div', class_='paraLexeme')\n",
    "\n",
    "    for para_lexeme in para_lexemes:\n",
    "      lexeme_name = para_lexeme.find('span', class_='lexemeName').text\n",
    "      if lexeme_name == word:\n",
    "        table = para_lexeme.find('table', class_='lexeme')\n",
    "\n",
    "        if table:\n",
    "          unique_cells = set()\n",
    "          form_cells = table.find_all('td', class_='form')\n",
    "\n",
    "          for cell in form_cells:\n",
    "            if not cell.find(class_='elisionHidden') and not cell.find(class_='notRecommendedHidden'):\n",
    "              text = cell.get_text(strip=True)\n",
    "              if '—' not in text:\n",
    "                text = text.replace(')', ' ').replace('(', '')\n",
    "                unique_cells.add(text)\n",
    "\n",
    "          if unique_cells:\n",
    "            random_word = random.choice(list(unique_cells))\n",
    "            return random_word\n",
    "  return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "c9GUbngn7ttP"
   },
   "outputs": [],
   "source": [
    "lista_conjunctii = ['și', 'dar', 'iar', 'că', 'să', 'ci', 'fie', 'sau', 'ori', 'ci și', 'ca să', 'încât să', 'nici', 'însă', 'ba', 'deci', 'așadar', 'totuși', 'însăși', 'chiar', 'tot', 'încă', 'atât', 'pe când', 'în timp ce',\n",
    "                    'cu toate că', 'întrucât', 'precum', 'totodată', 'care', 'pe care']\n",
    "\n",
    "lista_prepozitii = ['la', 'în', 'către', 'contrar', 'fără', 'după', 'cu', 'lângă', 'asupra', 'de', 'de la', 'despre', 'dimprejurul', 'din', 'dinaintea', 'înspre', 'între', 'înăuntrul', 'împotriva', 'împrejurul', 'înaintea',\n",
    "                    'înapoia', 'întru', 'dedesubtul', 'datorită', 'printre', 'prin', 'primprejur', 'peste', 'pentru', 'pe', 'până', 'via', 'spre', 'sub', 'din cauza', 'din pricina', 'în vederea', 'cu excepția', 'cu tot cu', 'a']\n",
    "\n",
    "lista_articole_posesive = ['a', 'ai', 'ale', 'al', 'alor', 'a', 'ai', 'ale', 'al', 'alor', 'a', 'ai', 'ale', 'al', 'alor', 'a', 'ai', 'ale', 'al', 'alor', 'a', 'ai', 'ale', 'al', 'alor', 'a', 'ai', 'ale', 'al', 'alor']\n",
    "lista_articole_demonstrative = ['cel', 'cea', 'cei', 'cele', 'celui', 'celei', 'celor', 'cel', 'cea', 'cei', 'cele', 'celui', 'celei', 'celor', 'cel', 'cea', 'cei', 'cele', 'celui', 'celei', 'celor', 'cel', 'cea', 'cei', 'cele', 'celui', 'celei', 'celor']\n",
    "lista_articole_nehotarate = ['un', 'o', 'niste', 'unor', 'unui', 'unei', 'un', 'o', 'niste', 'unor', 'unui', 'unei', 'un', 'o', 'niste', 'unor', 'unui', 'unei','un', 'o', 'niste', 'unor', 'unui', 'unei']\n",
    "\n",
    "def substitute(word_metadata):\n",
    "  action = random.randint(1, 100) / 100\n",
    "\n",
    "  if len(word_metadata['FORM']) > 8 and action > 0.6:\n",
    "    action = 0.5\n",
    "\n",
    "  # RoWORDNET\n",
    "  if action <= 0.2:\n",
    "    word = word_metadata['LEMMA'].lower()\n",
    "    syn_list = []\n",
    "    synset_ids = wn.synsets(literal=word)\n",
    "    for s in synset_ids:\n",
    "      if word in wn.synset(s).literals:\n",
    "        for literal in wn.synset(s).literals:\n",
    "          if literal != word and literal not in syn_list:\n",
    "            syn_list.append(literal)\n",
    "    if len(syn_list) > 0:\n",
    "      return random.choice(syn_list)\n",
    "    else:\n",
    "      return word_metadata['FORM']\n",
    "\n",
    "  # DEXONLINE\n",
    "  elif action > 0.2 and action <= 0.6:\n",
    "\n",
    "    if word_metadata['UPOS'] == 'CONJ' and action <= 0.3:\n",
    "      return random.choice(lista_conjunctii)\n",
    "\n",
    "    if word_metadata['UPOS'] == 'ADP' and action <= 0.3:\n",
    "      return random.choice(lista_prepozitii)\n",
    "\n",
    "    if word_metadata['UPOS'] == 'DET' and word_metadata['FORM'] in lista_articole_posesive and action <= 0.3:\n",
    "      return random.choice(lista_articole_posesive)\n",
    "    if word_metadata['UPOS'] == 'DET' and word_metadata['FORM'] in lista_articole_demonstrative and action <= 0.3:\n",
    "      return random.choice(lista_articole_demonstrative)\n",
    "\n",
    "    if word_metadata['UPOS'] == 'DET' and word_metadata['FORM'] in lista_articole_nehotarate and action <= 0.3:\n",
    "      return random.choice(lista_articole_nehotarate)\n",
    "    return morphologic_dex(word_metadata['LEMMA'])\n",
    "\n",
    "  # SPELLCHECKER\n",
    "  elif action > 0.6:\n",
    "    sugestions = list(pspell.suggest(word_metadata['FORM']))\n",
    "    if len(sugestions) == 0:\n",
    "      return morphologic_dex(word_metadata['LEMMA'])\n",
    "    return random.choice(sugestions)\n",
    "\n",
    "  return word_metadata['FORM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4YRo4YmSGLOy"
   },
   "outputs": [],
   "source": [
    "def save_data(data, counter):\n",
    "  output_file = f'MEID/{counter}.jsonl'\n",
    "  with open(output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    for example_parsed in data:\n",
    "      json.dump(example_parsed, json_file, ensure_ascii=False)#, indent=4)\n",
    "      json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pqr52hP3TGBU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n",
      "140000\n",
      "160000\n",
      "180000\n",
      "200000\n",
      "220000\n",
      "240000\n",
      "260000\n",
      "280000\n",
      "300000\n",
      "320000\n",
      "340000\n",
      "360000\n",
      "380000\n",
      "400000\n",
      "420000\n",
      "440000\n",
      "460000\n",
      "480000\n",
      "500000\n",
      "520000\n",
      "540000\n",
      "560000\n",
      "580000\n",
      "600000\n",
      "620000\n",
      "640000\n",
      "660000\n",
      "680000\n",
      "700000\n",
      "720000\n",
      "740000\n",
      "760000\n",
      "780000\n",
      "800000\n",
      "820000\n",
      "840000\n",
      "860000\n",
      "880000\n",
      "900000\n",
      "920000\n",
      "940000\n",
      "960000\n",
      "980000\n",
      "1000000\n",
      "1020000\n",
      "1040000\n",
      "1060000\n",
      "1080000\n",
      "1100000\n",
      "1120000\n",
      "1140000\n",
      "1160000\n",
      "1180000\n",
      "1200000\n",
      "1220000\n",
      "1240000\n",
      "1260000\n",
      "1280000\n",
      "1300000\n",
      "1320000\n",
      "1340000\n",
      "1360000\n",
      "1380000\n",
      "1400000\n",
      "1420000\n",
      "1440000\n",
      "1460000\n",
      "1480000\n",
      "1500000\n",
      "1520000\n",
      "1540000\n",
      "1560000\n",
      "1580000\n",
      "1600000\n",
      "1620000\n",
      "1640000\n",
      "1660000\n",
      "1680000\n",
      "1700000\n",
      "1720000\n",
      "1740000\n",
      "1760000\n",
      "1780000\n",
      "1800000\n",
      "1820000\n",
      "1840000\n",
      "1860000\n",
      "1880000\n",
      "1900000\n",
      "1920000\n",
      "1940000\n",
      "1960000\n",
      "1980000\n",
      "2000000\n",
      "2020000\n",
      "2040000\n",
      "2060000\n",
      "2080000\n",
      "2100000\n",
      "2120000\n",
      "2140000\n",
      "2160000\n",
      "2180000\n",
      "2200000\n",
      "2220000\n",
      "2240000\n",
      "2260000\n",
      "2280000\n",
      "2300000\n",
      "2320000\n",
      "2340000\n",
      "2360000\n",
      "2380000\n",
      "2400000\n",
      "2420000\n",
      "2440000\n",
      "2460000\n",
      "2480000\n",
      "2500000\n",
      "2520000\n",
      "2540000\n",
      "2560000\n",
      "2580000\n",
      "2600000\n",
      "2620000\n",
      "2640000\n",
      "2660000\n",
      "2680000\n",
      "2700000\n",
      "2720000\n",
      "2740000\n",
      "2760000\n",
      "2780000\n",
      "2800000\n",
      "2820000\n",
      "2840000\n",
      "2860000\n",
      "2880000\n",
      "2900000\n",
      "2920000\n",
      "2940000\n",
      "2960000\n",
      "2980000\n",
      "3000000\n",
      "3020000\n",
      "3040000\n",
      "3060000\n",
      "3080000\n",
      "3100000\n",
      "3120000\n",
      "3140000\n",
      "3160000\n",
      "3180000\n",
      "3200000\n",
      "3201000 2024-05-19 09:52:21.774152\n",
      "3202000 2024-05-19 09:55:57.983215\n",
      "3203000 2024-05-19 09:59:38.099210\n",
      "3204000 2024-05-19 10:02:30.611200\n",
      "3205000 2024-05-19 10:06:10.472070\n",
      "3206000 2024-05-19 10:09:28.516757\n",
      "3207000 2024-05-19 10:13:04.842851\n",
      "3208000 2024-05-19 10:17:54.780912\n",
      "3209000 2024-05-19 10:22:02.037600\n",
      "3210000 2024-05-19 10:27:05.052693\n",
      "3211000 2024-05-19 10:31:45.424467\n",
      "3212000 2024-05-19 10:32:59.037521\n",
      "3213000 2024-05-19 10:37:27.058518\n",
      "3214000 2024-05-19 10:42:13.656938\n",
      "3215000 2024-05-19 10:47:38.017209\n",
      "3216000 2024-05-19 10:50:14.682893\n",
      "3217000 2024-05-19 10:52:07.606850\n",
      "3218000 2024-05-19 10:56:17.263708\n",
      "3219000 2024-05-19 11:00:51.575422\n",
      "3220000 2024-05-19 11:05:41.549656\n",
      "3220000 2024-05-19 11:05:41.549656 91\n",
      "3221000 2024-05-19 11:10:58.242238\n",
      "3222000 2024-05-19 11:15:59.059967\n",
      "3223000 2024-05-19 11:21:30.218537\n",
      "3224000 2024-05-19 11:26:41.681499\n",
      "3225000 2024-05-19 11:32:08.702813\n",
      "3226000 2024-05-19 11:37:31.179730\n",
      "3227000 2024-05-19 11:41:39.896580\n",
      "3228000 2024-05-19 11:44:37.859197\n",
      "3229000 2024-05-19 11:47:35.921873\n",
      "3230000 2024-05-19 11:51:57.452845\n",
      "3231000 2024-05-19 11:56:44.104719\n",
      "3232000 2024-05-19 12:00:42.510460\n",
      "3233000 2024-05-19 12:04:44.110034\n",
      "3234000 2024-05-19 12:08:28.457655\n",
      "3235000 2024-05-19 12:12:13.465769\n",
      "3236000 2024-05-19 12:16:28.350970\n",
      "3237000 2024-05-19 12:18:57.113290\n",
      "3238000 2024-05-19 12:24:12.255479\n",
      "3239000 2024-05-19 12:26:37.857446\n",
      "3240000 2024-05-19 12:29:47.611001\n",
      "3240000 2024-05-19 12:29:48.190119 92\n",
      "3241000 2024-05-19 12:34:02.960157\n",
      "3242000 2024-05-19 12:38:49.039581\n",
      "3243000 2024-05-19 12:43:50.236928\n",
      "3244000 2024-05-19 12:48:21.637951\n",
      "3245000 2024-05-19 12:52:40.814845\n",
      "3246000 2024-05-19 12:54:49.763165\n",
      "3247000 2024-05-19 12:58:40.762411\n",
      "3248000 2024-05-19 13:03:35.329413\n",
      "3249000 2024-05-19 13:05:48.357093\n",
      "3250000 2024-05-19 13:06:51.114631\n",
      "3251000 2024-05-19 13:10:40.587081\n",
      "3252000 2024-05-19 13:13:48.675658\n",
      "3253000 2024-05-19 13:18:02.886183\n",
      "3254000 2024-05-19 13:19:47.269033\n",
      "3255000 2024-05-19 13:22:02.740009\n",
      "3256000 2024-05-19 13:26:26.520099\n",
      "3257000 2024-05-19 13:30:40.174868\n",
      "3258000 2024-05-19 13:35:09.683711\n",
      "3259000 2024-05-19 13:40:26.504888\n",
      "3260000 2024-05-19 13:44:42.387655\n",
      "3260000 2024-05-19 13:44:42.521126 93\n",
      "3261000 2024-05-19 13:49:39.678454\n",
      "3262000 2024-05-19 13:53:01.699406\n",
      "3263000 2024-05-19 13:55:42.056883\n",
      "3264000 2024-05-19 13:59:25.830081\n",
      "3265000 2024-05-19 14:02:38.702232\n",
      "3266000 2024-05-19 14:07:53.904888\n",
      "3267000 2024-05-19 14:12:42.089289\n",
      "3268000 2024-05-19 14:16:57.146752\n",
      "3269000 2024-05-19 14:20:13.157055\n",
      "3270000 2024-05-19 14:23:55.306297\n",
      "3271000 2024-05-19 14:26:51.249422\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m     wrong_sentence \u001b[38;5;241m=\u001b[39m copy_word(wrong_sentence, word[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFORM\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     wrong_sentence \u001b[38;5;241m=\u001b[39m copy_word(wrong_sentence, \u001b[43msubstitute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# DEFAULT\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m, in \u001b[0;36msubstitute\u001b[1;34m(word_metadata)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# SPELLCHECKER\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.6\u001b[39m:\n\u001b[1;32m---> 52\u001b[0m   sugestions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFORM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sugestions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m morphologic_dex(word_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEMMA\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spylls\\hunspell\\dictionary.py:227\u001b[0m, in \u001b[0;36mDictionary.suggest\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuggest\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m    Suggests corrections for the misspelled word (in order of probability/similarity, best\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    suggestions first), returns lazy generator of suggestions.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m        word: Misspelled word\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggester(word)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spylls\\hunspell\\algo\\suggest.py:159\u001b[0m, in \u001b[0;36mSuggest.__call__\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    Outer \"public\" interface: returns a list of all valid suggestions, as strings.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        word: Word to check\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m (suggestion\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m suggestion \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggestions(word))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spylls\\hunspell\\algo\\suggest.py:159\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    Outer \"public\" interface: returns a list of all valid suggestions, as strings.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        word: Word to check\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43msuggestion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msuggestion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggestions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spylls\\hunspell\\algo\\suggest.py:347\u001b[0m, in \u001b[0;36mSuggest.suggestions\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# If there was no good edits that were valid words, we might try\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# ngram-based suggestion algorithm: it is slower, but able to correct severely misspelled words\u001b[39;00m\n\u001b[0;32m    346\u001b[0m ngrams_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msug\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngram_suggestions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandled\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhandle_found\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSuggestion\u001b[49m\u001b[43m(\u001b[49m\u001b[43msug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mngram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_inclusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mngrams_seen\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spylls\\hunspell\\algo\\suggest.py:513\u001b[0m, in \u001b[0;36mSuggest.ngram_suggestions\u001b[1;34m(self, word, handled)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maff\u001b[38;5;241m.\u001b[39mMAXNGRAMSUGS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m ngram_suggest\u001b[38;5;241m.\u001b[39mngram_suggest(\n\u001b[0;32m    514\u001b[0m             word\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    515\u001b[0m             dictionary_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords_for_ngram,\n\u001b[0;32m    516\u001b[0m             prefixes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maff\u001b[38;5;241m.\u001b[39mPFX, suffixes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maff\u001b[38;5;241m.\u001b[39mSFX,\n\u001b[0;32m    517\u001b[0m             known\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m(word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m handled)},\n\u001b[0;32m    518\u001b[0m             maxdiff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maff\u001b[38;5;241m.\u001b[39mMAXDIFF,\n\u001b[0;32m    519\u001b[0m             onlymaxdiff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maff\u001b[38;5;241m.\u001b[39mONLYMAXDIFF,\n\u001b[0;32m    520\u001b[0m             has_phonetic\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maff\u001b[38;5;241m.\u001b[39mPHONE \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spylls\\hunspell\\algo\\ngram_suggest.py:74\u001b[0m, in \u001b[0;36mngram_suggest\u001b[1;34m(misspelling, dictionary_words, prefixes, suffixes, known, maxdiff, onlymaxdiff, has_phonetic)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# TODO: hunspell has more exceptions/flag checks here (part of it we cover later in suggest,\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# deciding, for example, if the suggestion is forbidden)\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mroot_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmisspelling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# If dictionary word have alternative spellings provided via `pp:` data tag, calculate\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# score against them, too. Note that only simple ph:spelling are listed in alt_spellings,\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# more complicated tags like ph:spellin* or ph:spellng->spelling are ignored in ngrams\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39malt_spellings:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spylls\\hunspell\\algo\\ngram_suggest.py:153\u001b[0m, in \u001b[0;36mroot_score\u001b[1;34m(word1, word2)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroot_score\u001b[39m(word1: \u001b[38;5;28mstr\u001b[39m, word2: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    Scoring, stage 1: Simple score for first dictionary words choosing: 3-gram score + longest start\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    substring.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m        word2: possible suggestion\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 153\u001b[0m         \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngram\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlonger_worse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    154\u001b[0m         sm\u001b[38;5;241m.\u001b[39mleftcommonsubstring(word1, word2\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m    155\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spylls\\hunspell\\algo\\string_metrics.py:52\u001b[0m, in \u001b[0;36mngram\u001b[1;34m(max_ngram_size, s1, s2, weighted, any_mismatch, longer_worse)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(l1 \u001b[38;5;241m-\u001b[39m ngram_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# ...and if the ngram of current size in this position is present in ANY place in second string\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s1[pos:pos\u001b[38;5;241m+\u001b[39mngram_size] \u001b[38;5;129;01min\u001b[39;00m s2:\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;66;03m# increase score\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m         ns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m weighted:\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# For \"weighted\" ngrams, decrease score if ngram is not found,\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         ns \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset[\"train\"])\n",
    "count = 0\n",
    "data = []\n",
    "\n",
    "punctuation_list = [' ', ',', ';', ':', '-', '.', '?', '!', '...']\n",
    "\n",
    "contor_file = 90\n",
    "breakline = 20000\n",
    "deadline = 3400000\n",
    "after_limit = 3200000\n",
    "\n",
    "for example in iterator:\n",
    "    count += 1\n",
    "    if count <= after_limit:\n",
    "        if count % breakline == 0:\n",
    "            print(count)\n",
    "        continue\n",
    "\n",
    "    if count % 1000 == 0:\n",
    "        print(count, datetime.datetime.now())\n",
    "    \n",
    "    correct_sentence = example['text']\n",
    "    metadata = example['metadata_text']\n",
    "\n",
    "    list_of_words = [word['FORM'] for word in metadata if word['FEATS'] != '_']\n",
    "\n",
    "    word_count = len(list_of_words)\n",
    "    if word_count <= 3:\n",
    "        continue\n",
    "        \n",
    "    nr_errs = number_of_words_changed(word_count)\n",
    "    numbers_to_be_changed = np.random.choice(np.arange(0, word_count), size=nr_errs, replace=False).tolist()\n",
    "\n",
    "    skip_flag = False\n",
    "    wrong_sentence = \"\"\n",
    "    nr_word = -1\n",
    "    for i in range(len(metadata)):\n",
    "      if skip_flag == True:\n",
    "        skip_flag = False\n",
    "        nr_word += 1\n",
    "        continue\n",
    "\n",
    "      word = metadata[i]\n",
    "      if word['FEATS'] == '_':\n",
    "        if word['FORM'] in punctuation_list:\n",
    "          new_punctuation = punctuation_substitution(word['FORM'])\n",
    "          if new_punctuation in ['-', ' ']:\n",
    "            wrong_sentence = copy_word(wrong_sentence, new_punctuation, False)\n",
    "        else:\n",
    "          wrong_sentence = copy_word(wrong_sentence, word['FORM'])\n",
    "\n",
    "        continue\n",
    "\n",
    "      nr_word += 1\n",
    "\n",
    "      # Token not to be changed\n",
    "      if nr_word not in numbers_to_be_changed:\n",
    "        wrong_sentence = copy_word(wrong_sentence, word['FORM'])\n",
    "        continue\n",
    "\n",
    "      # Type of error - Distribution of Probability\n",
    "      token_error_action = random.randint(1, 100) / 100\n",
    "\n",
    "      # CONCATENATION\n",
    "      if token_error_action <= 0.12 and i < len(metadata) - 1:\n",
    "        next_word = metadata[i + 1]\n",
    "        if next_word['FEATS'] != '_':\n",
    "          wrong_sentence = concatenate(wrong_sentence, word['FORM'], next_word['FORM'])\n",
    "          skip_flag = True\n",
    "          continue\n",
    "\n",
    "      # TRANSPOSITION\n",
    "      if token_error_action > 0.12 and token_error_action <= 0.2 and i < len(metadata) - 1:\n",
    "        next_word = metadata[i + 1]\n",
    "        if next_word['FEATS'] != '_':\n",
    "          wrong_sentence = transpose(wrong_sentence, word['FORM'], next_word['FORM'])\n",
    "          skip_flag = True\n",
    "          continue\n",
    "\n",
    "      # DELETION\n",
    "      if token_error_action > 0.2 and token_error_action <= 0.25:\n",
    "        continue\n",
    "\n",
    "      # MISSPELLING\n",
    "      if token_error_action > 0.25 and token_error_action <= 0.7:\n",
    "        wrong_sentence = copy_word(wrong_sentence, misspell(word['FORM']))\n",
    "        continue\n",
    "\n",
    "      # SUBSTITUTION\n",
    "      if token_error_action > 0.7:\n",
    "        if word['UPOS'] in ['PROPN', 'NUM', 'SYM']:\n",
    "          wrong_sentence = copy_word(wrong_sentence, word['FORM'])\n",
    "        else:\n",
    "          wrong_sentence = copy_word(wrong_sentence, substitute(word))\n",
    "        continue\n",
    "\n",
    "      # DEFAULT\n",
    "      wrong_sentence = copy_word(wrong_sentence, word['FORM'])\n",
    "\n",
    "    data.append({\"wrong\": wrong_sentence[1:], \"right\": correct_sentence})\n",
    "\n",
    "    if count % breakline == 0:\n",
    "      contor_file += 1\n",
    "      print(count, datetime.datetime.now(), contor_file)\n",
    "      save_data(data, contor_file)\n",
    "      data = []\n",
    "\n",
    "    if count == deadline:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
