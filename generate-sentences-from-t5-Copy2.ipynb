{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(64101, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(64101, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(64101, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=64101, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"512-encoder-1000K/kaggle/working/results/checkpoint-125000/\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config\n",
    "start_token = \"<unk>\"\n",
    "end_token = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['wrong', 'right'],\n",
       "     num_rows: 1967695\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['wrong', 'right'],\n",
       "     num_rows: 60857\n",
       " })]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mateiaassAI/MEID_v2\", split=['train[:97%]', 'train[97%:100%]'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['wrong', 'right'],\n",
       "    num_rows: 60857\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = dataset[0]\n",
    "ds_test = dataset[1]\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_marks = ['.', '?', '!', ';', '...']\n",
    "\n",
    "def filter_sentences(sentences):\n",
    "    text = sentences['right']\n",
    "    if any(text.endswith(punc) for punc in punctuation_marks):\n",
    "      words = text.split()\n",
    "      if len(words) >= 10:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds_test = ds_test.filter(filter_sentences, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31457\n"
     ]
    }
   ],
   "source": [
    "fds_test[0]\n",
    "print(len(fds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10001\n",
      "10002\n",
      "10003\n",
      "10004\n",
      "10005\n",
      "10006\n",
      "10007\n",
      "10008\n",
      "10009\n",
      "10010\n",
      "10011\n",
      "10012\n",
      "10013\n",
      "10014\n",
      "10015\n",
      "10016\n",
      "10017\n",
      "10018\n",
      "10019\n",
      "10020\n",
      "10021\n",
      "10022\n",
      "10023\n",
      "10024\n",
      "10025\n",
      "10026\n",
      "10027\n",
      "10028\n",
      "10029\n",
      "10030\n",
      "10031\n",
      "10032\n",
      "10033\n",
      "10034\n",
      "10035\n",
      "10036\n",
      "10037\n",
      "10038\n",
      "10039\n",
      "10040\n",
      "10041\n",
      "10042\n",
      "10043\n",
      "10044\n",
      "10045\n",
      "10046\n",
      "10047\n",
      "10048\n",
      "10049\n",
      "10050\n",
      "10051\n",
      "10052\n",
      "10053\n",
      "10054\n",
      "10055\n",
      "10056\n",
      "10057\n",
      "10058\n",
      "10059\n",
      "10060\n",
      "10061\n",
      "10062\n",
      "10063\n",
      "10064\n",
      "10065\n",
      "10066\n",
      "10067\n",
      "10068\n",
      "10069\n",
      "10070\n",
      "10071\n",
      "10072\n",
      "10073\n",
      "10074\n",
      "10075\n",
      "10076\n",
      "10077\n",
      "10078\n",
      "10079\n",
      "10080\n",
      "10081\n",
      "10082\n",
      "10083\n",
      "10084\n",
      "10085\n",
      "10086\n",
      "10087\n",
      "10088\n",
      "10089\n",
      "10090\n",
      "10091\n",
      "10092\n",
      "10093\n",
      "10094\n",
      "10095\n",
      "10096\n",
      "10097\n",
      "10098\n",
      "10099\n",
      "10100\n",
      "10101\n",
      "10102\n",
      "10103\n",
      "10104\n",
      "10105\n",
      "10106\n",
      "10107\n",
      "10108\n",
      "10109\n",
      "10110\n",
      "10111\n",
      "10112\n",
      "10113\n",
      "10114\n",
      "10115\n",
      "10116\n",
      "10117\n",
      "10118\n",
      "10119\n",
      "10120\n",
      "10121\n",
      "10122\n",
      "10123\n",
      "10124\n",
      "10125\n",
      "10126\n",
      "10127\n",
      "10128\n",
      "10129\n",
      "10130\n",
      "10131\n",
      "10132\n",
      "10133\n",
      "10134\n",
      "10135\n",
      "10136\n",
      "10137\n",
      "10138\n",
      "10139\n",
      "10140\n",
      "10141\n",
      "10142\n",
      "10143\n",
      "10144\n",
      "10145\n",
      "10146\n",
      "10147\n",
      "10148\n",
      "10149\n",
      "10150\n",
      "10151\n",
      "10152\n",
      "10153\n",
      "10154\n",
      "10155\n",
      "10156\n",
      "10157\n",
      "10158\n",
      "10159\n",
      "10160\n",
      "10161\n",
      "10162\n",
      "10163\n",
      "10164\n",
      "10165\n",
      "10166\n",
      "10167\n",
      "10168\n",
      "10169\n",
      "10170\n",
      "10171\n",
      "10172\n",
      "10173\n",
      "10174\n",
      "10175\n",
      "10176\n",
      "10177\n",
      "10178\n",
      "10179\n",
      "10180\n",
      "10181\n",
      "10182\n",
      "10183\n",
      "10184\n",
      "10185\n",
      "10186\n",
      "10187\n",
      "10188\n",
      "10189\n",
      "10190\n",
      "10191\n",
      "10192\n",
      "10193\n",
      "10194\n",
      "10195\n",
      "10196\n",
      "10197\n",
      "10198\n",
      "10199\n",
      "10200\n",
      "10201\n",
      "10202\n",
      "10203\n",
      "10204\n",
      "10205\n",
      "10206\n",
      "10207\n",
      "10208\n",
      "10209\n",
      "10210\n",
      "10211\n",
      "10212\n",
      "10213\n",
      "10214\n",
      "10215\n",
      "10216\n",
      "10217\n",
      "10218\n",
      "10219\n",
      "10220\n",
      "10221\n",
      "10222\n",
      "10223\n",
      "10224\n",
      "10225\n",
      "10226\n",
      "10227\n",
      "10228\n",
      "10229\n",
      "10230\n",
      "10231\n",
      "10232\n",
      "10233\n",
      "10234\n",
      "10235\n",
      "10236\n",
      "10237\n",
      "10238\n",
      "10239\n",
      "10240\n",
      "10241\n",
      "10242\n",
      "10243\n",
      "10244\n",
      "10245\n",
      "10246\n",
      "10247\n",
      "10248\n",
      "10249\n",
      "10250\n",
      "10251\n",
      "10252\n",
      "10253\n",
      "10254\n",
      "10255\n",
      "10256\n",
      "10257\n",
      "10258\n",
      "10259\n",
      "10260\n",
      "10261\n",
      "10262\n",
      "10263\n",
      "10264\n",
      "10265\n",
      "10266\n",
      "10267\n",
      "10268\n",
      "10269\n",
      "10270\n",
      "10271\n",
      "10272\n",
      "10273\n",
      "10274\n",
      "10275\n",
      "10276\n",
      "10277\n",
      "10278\n",
      "10279\n",
      "10280\n",
      "10281\n",
      "10282\n",
      "10283\n",
      "10284\n",
      "10285\n",
      "10286\n",
      "10287\n",
      "10288\n",
      "10289\n",
      "10290\n",
      "10291\n",
      "10292\n",
      "10293\n",
      "10294\n",
      "10295\n",
      "10296\n",
      "10297\n",
      "10298\n",
      "10299\n",
      "10300\n",
      "10301\n",
      "10302\n",
      "10303\n",
      "10304\n",
      "10305\n",
      "10306\n",
      "10307\n",
      "10308\n",
      "10309\n",
      "10310\n",
      "10311\n",
      "10312\n",
      "10313\n",
      "10314\n",
      "10315\n",
      "10316\n",
      "10317\n",
      "10318\n",
      "10319\n",
      "10320\n",
      "10321\n",
      "10322\n",
      "10323\n",
      "10324\n",
      "10325\n",
      "10326\n",
      "10327\n",
      "10328\n",
      "10329\n",
      "10330\n",
      "10331\n",
      "10332\n",
      "10333\n",
      "10334\n",
      "10335\n",
      "10336\n",
      "10337\n",
      "10338\n",
      "10339\n",
      "10340\n",
      "10341\n",
      "10342\n",
      "10343\n",
      "10344\n",
      "10345\n",
      "10346\n",
      "10347\n",
      "10348\n",
      "10349\n",
      "10350\n",
      "10351\n",
      "10352\n",
      "10353\n",
      "10354\n",
      "10355\n",
      "10356\n",
      "10357\n",
      "10358\n",
      "10359\n",
      "10360\n",
      "10361\n",
      "10362\n",
      "10363\n",
      "10364\n",
      "10365\n",
      "10366\n",
      "10367\n",
      "10368\n",
      "10369\n",
      "10370\n",
      "10371\n",
      "10372\n",
      "10373\n",
      "10374\n",
      "10375\n",
      "10376\n",
      "10377\n",
      "10378\n",
      "10379\n",
      "10380\n",
      "10381\n",
      "10382\n",
      "10383\n",
      "10384\n",
      "10385\n",
      "10386\n",
      "10387\n",
      "10388\n",
      "10389\n",
      "10390\n",
      "10391\n",
      "10392\n",
      "10393\n",
      "10394\n",
      "10395\n",
      "10396\n",
      "10397\n",
      "10398\n",
      "10399\n",
      "10400\n",
      "10401\n",
      "10402\n",
      "10403\n",
      "10404\n",
      "10405\n",
      "10406\n",
      "10407\n",
      "10408\n",
      "10409\n",
      "10410\n",
      "10411\n",
      "10412\n",
      "10413\n",
      "10414\n",
      "10415\n",
      "10416\n",
      "10417\n",
      "10418\n",
      "10419\n",
      "10420\n",
      "10421\n",
      "10422\n",
      "10423\n",
      "10424\n",
      "10425\n",
      "10426\n",
      "10427\n",
      "10428\n",
      "10429\n",
      "10430\n",
      "10431\n",
      "10432\n",
      "10433\n",
      "10434\n",
      "10435\n",
      "10436\n",
      "10437\n",
      "10438\n",
      "10439\n",
      "10440\n",
      "10441\n",
      "10442\n",
      "10443\n",
      "10444\n",
      "10445\n",
      "10446\n",
      "10447\n",
      "10448\n",
      "10449\n",
      "10450\n",
      "10451\n",
      "10452\n",
      "10453\n",
      "10454\n",
      "10455\n",
      "10456\n",
      "10457\n",
      "10458\n",
      "10459\n",
      "10460\n",
      "10461\n",
      "10462\n",
      "10463\n",
      "10464\n",
      "10465\n",
      "10466\n",
      "10467\n",
      "10468\n",
      "10469\n",
      "10470\n",
      "10471\n",
      "10472\n",
      "10473\n",
      "10474\n",
      "10475\n",
      "10476\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_text(generated_text, start_token=\"<unk>\", end_token=\"</s>\"):\n",
    "    start_index = generated_text.find(start_token)\n",
    "    end_index = generated_text.find(end_token, start_index)\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        return generated_text[start_index + len(start_token):end_index].strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "sentence_pairs = []\n",
    "batch_index = 10\n",
    "\n",
    "for idx, data in enumerate(fds_test):\n",
    "    if idx < 10000:\n",
    "        continue;\n",
    "        \n",
    "    print(idx)\n",
    "    correct_sentence = data['right']\n",
    "    input_sentence = data['wrong']\n",
    "\n",
    "    input_tensor = tokenizer.encode(input_sentence, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(input_tensor)\n",
    "    generated_text = tokenizer.decode(outputs[0].cpu(), skip_special_tokens=False)\n",
    "    predicted_sentence = extract_text(generated_text)\n",
    "\n",
    "    if predicted_sentence is not None:\n",
    "        sentence_pairs.append(\n",
    "            (correct_sentence, input_sentence, predicted_sentence))\n",
    "\n",
    "    if len(sentence_pairs) >= 1000:\n",
    "        df = pd.DataFrame(sentence_pairs)\n",
    "        file_name = f'ress/sentence_pairs_batch_{batch_index}.jsonl'\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            for pair in sentence_pairs:\n",
    "                json.dump(pair, f, ensure_ascii=False)\n",
    "                f.write('\\n')\n",
    "        print(f\"Saved 1000 sentence pairs to {file_name}\")\n",
    "        sentence_pairs = []  # Clear the list after saving\n",
    "        batch_index += 1  # Increment the batch index\n",
    "\n",
    "    if batch_index == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pentru perioada în care un pacient are deschisă o fișă de spitalizare de zi pe<unk> parcursul unei singure zile sau pe<unk> parcursul mai multor zile, acesta poate beneficia și de servicii medicale în ambulatoriul de specialitate, altele decât cele necesare acordării serviciilor medicale din spitalizare de zi, cu respectarea condițiilor de acordare a serviciilor medicale în ambulatoriu.\n"
     ]
    }
   ],
   "source": [
    "print(sentence_pairs[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maria sa dus la mare ca sa manance incicheta cu vanilie.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Maria sa dus la mare ca sa mannce incchetata cu vanilie \", return_tensors=\"pt\").input_ids\n",
    "generated_text = tokenizer.decode(model.generate(input_ids)[0], skip_special_tokens=False)\n",
    "\n",
    "start_index = generated_text.find(start_token)\n",
    "end_index = generated_text.find(end_token, start_index)\n",
    "\n",
    "if start_index != -1 and end_index != -1:\n",
    "    extracted_text = generated_text[start_index + len(start_token):end_index].strip()\n",
    "else:\n",
    "    extracted_text = None\n",
    "\n",
    "print(extracted_text)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5060173,
     "sourceId": 8483397,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
