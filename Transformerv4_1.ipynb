{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e19d14bfe3b433eb0b8edaadcaf6741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec9f574d726d436eaa131dc52d712c20",
              "IPY_MODEL_b2dc937c48bb4cbb9845da8b3e9a61df",
              "IPY_MODEL_e4502e00e0714540a3c1798e690fde98",
              "IPY_MODEL_b3c24410f9ed414aa70c90efcb4fefbe",
              "IPY_MODEL_3294fcf31c184a199d0fdd78bc456c02"
            ],
            "layout": "IPY_MODEL_269849a09cc5473a9495b36b17fc7355"
          }
        },
        "ec9f574d726d436eaa131dc52d712c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc1df1a418274872b7e6ae5d08d02d43",
            "placeholder": "​",
            "style": "IPY_MODEL_c91df640c96d4d5ca2212062d4efaec4",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b2dc937c48bb4cbb9845da8b3e9a61df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5f9f246477d74562bf6bfd3f8e734a3a",
            "placeholder": "​",
            "style": "IPY_MODEL_5c10e51847684e1981975247acc76db4",
            "value": ""
          }
        },
        "e4502e00e0714540a3c1798e690fde98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_765dea327e6f4fe584dd30b97518ef59",
            "style": "IPY_MODEL_dc122c656a394daf9e08b73401995249",
            "value": true
          }
        },
        "b3c24410f9ed414aa70c90efcb4fefbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8dc1d2657fe94a66b61726131d0ea3dc",
            "style": "IPY_MODEL_f4cfac122a4943c1aae6b4b1819a196c",
            "tooltip": ""
          }
        },
        "3294fcf31c184a199d0fdd78bc456c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51fbc3cb622a470b8d49f7c51d80b85e",
            "placeholder": "​",
            "style": "IPY_MODEL_eb303e77ba9644e6b3758f372d666dd7",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "269849a09cc5473a9495b36b17fc7355": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "dc1df1a418274872b7e6ae5d08d02d43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c91df640c96d4d5ca2212062d4efaec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f9f246477d74562bf6bfd3f8e734a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c10e51847684e1981975247acc76db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "765dea327e6f4fe584dd30b97518ef59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc122c656a394daf9e08b73401995249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dc1d2657fe94a66b61726131d0ea3dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4cfac122a4943c1aae6b4b1819a196c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "51fbc3cb622a470b8d49f7c51d80b85e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb303e77ba9644e6b3758f372d666dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "Q70reR67PfNp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "OMdGxf6Gifhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a6f10a-dd90-4eb7-ad26-fd52874917b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_model, dropout_p, max_len):\n",
        "        super().__init__()\n",
        "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "        # max_len determines how far the position can have an effect on a token (window)\n",
        "\n",
        "        # Info\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Encoding - From formula\n",
        "        pos_encoding = torch.zeros(max_len, dim_model)\n",
        "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
        "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
        "\n",
        "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
        "\n",
        "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
        "\n",
        "        # Saving buffer (same as parameter without gradients needed)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
        "        # Residual connection + pos encoding\n",
        "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
      ],
      "metadata": {
        "id": "X7dEc_6HRR3b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    # Constructor\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_tokens,\n",
        "        dim_model,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dropout_p,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # INFO\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.dim_model = dim_model\n",
        "\n",
        "        # LAYERS\n",
        "        self.positional_encoder = PositionalEncoding(\n",
        "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
        "        )\n",
        "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=dim_model,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dropout=dropout_p,\n",
        "        )\n",
        "        self.out = nn.Linear(dim_model, num_tokens)\n",
        "\n",
        "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
        "        # Src size must be (batch_size, src sequence length)\n",
        "        # Tgt size must be (batch_size, tgt sequence length)\n",
        "\n",
        "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
        "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
        "        src = self.positional_encoder(src)\n",
        "        tgt = self.positional_encoder(tgt)\n",
        "\n",
        "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
        "        # to obtain size (sequence length, batch_size, dim_model),\n",
        "        src = src.permute(1,0,2)\n",
        "        tgt = tgt.permute(1,0,2)\n",
        "\n",
        "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
        "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
        "        out = self.out(transformer_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def get_tgt_mask(self, size) -> torch.tensor:\n",
        "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
        "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
        "        mask = mask.float()\n",
        "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
        "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
        "\n",
        "        # EX for size=5:\n",
        "        # [[0., -inf, -inf, -inf, -inf],\n",
        "        #  [0.,   0., -inf, -inf, -inf],\n",
        "        #  [0.,   0.,   0., -inf, -inf],\n",
        "        #  [0.,   0.,   0.,   0., -inf],\n",
        "        #  [0.,   0.,   0.,   0.,   0.]]\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
        "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
        "        # [False, False, False, True, True, True]\n",
        "        return (matrix == pad_token)"
      ],
      "metadata": {
        "id": "fzOm3mDZQTP_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 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\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "JtAKnwd9iNIi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387,
          "referenced_widgets": [
            "9e19d14bfe3b433eb0b8edaadcaf6741",
            "ec9f574d726d436eaa131dc52d712c20",
            "b2dc937c48bb4cbb9845da8b3e9a61df",
            "e4502e00e0714540a3c1798e690fde98",
            "b3c24410f9ed414aa70c90efcb4fefbe",
            "3294fcf31c184a199d0fdd78bc456c02",
            "269849a09cc5473a9495b36b17fc7355",
            "dc1df1a418274872b7e6ae5d08d02d43",
            "c91df640c96d4d5ca2212062d4efaec4",
            "5f9f246477d74562bf6bfd3f8e734a3a",
            "5c10e51847684e1981975247acc76db4",
            "765dea327e6f4fe584dd30b97518ef59",
            "dc122c656a394daf9e08b73401995249",
            "8dc1d2657fe94a66b61726131d0ea3dc",
            "f4cfac122a4943c1aae6b4b1819a196c",
            "51fbc3cb622a470b8d49f7c51d80b85e",
            "eb303e77ba9644e6b3758f372d666dd7"
          ]
        },
        "outputId": "fae19731-4857-499a-cf29-77b08576159e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e19d14bfe3b433eb0b8edaadcaf6741"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNsZ7A1znxCl",
        "outputId": "a229c4a6-5559-444f-87e6-6bafd174ef70"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset(\"mateiaassAI/MEID\", split=['train[:5%]', 'train[5%:7%]', 'train[7%:10%]'])\n",
        "dataset = load_dataset(\"mateiaassAI/MEID\", split=['train[:75%]', 'train[50%:55%]', 'train[80%:81%]'])\n",
        "train_dataset = dataset[0]\n",
        "test_dataset = dataset[1]\n",
        "valid_dataset = dataset[2]"
      ],
      "metadata": {
        "id": "fJ0hb3wki33I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80beece2-a5c3-4465-e175-ff863fbaf63a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import pandas as pd\n",
        "\n",
        "dataset_tok = load_dataset(\"mateiaassAI/MEID\")\n",
        "dataset_tok = dataset_tok[\"train\"]\n",
        "dataset_tok = dataset_tok['right']\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
        "tokenizer.train_from_iterator(dataset_tok, vocab_size=100000, min_frequency=1, show_progress=True,special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n",
        "\n",
        "src_pad_idx = tokenizer.token_to_id('<pad>')\n",
        "trg_pad_idx = src_pad_idx\n",
        "trg_sos_idx = tokenizer.token_to_id('<s>')\n",
        "trg_eos_idx = tokenizer.token_to_id('</s>')\n",
        "\n",
        "enc_voc_size = tokenizer.get_vocab_size()\n",
        "dec_voc_size = enc_voc_size\n",
        "\n",
        "print(trg_sos_idx, trg_eos_idx, src_pad_idx, enc_voc_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkb6axTWkYsr",
        "outputId": "c48f9620-dc6d-4038-db39-3f81a5bea179"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2 1 100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_data(dataframe, tokenizer):\n",
        "    data = []\n",
        "    for example in dataframe:\n",
        "      input_text = example[\"wrong\"]\n",
        "      target_text = example[\"right\"]\n",
        "\n",
        "      input_ids = np.array(tokenizer.encode(input_text).ids)\n",
        "      target_ids = np.array(tokenizer.encode(target_text).ids)\n",
        "\n",
        "      SOS_token = np.array([trg_sos_idx])\n",
        "      EOS_token = np.array([trg_eos_idx])\n",
        "      X = np.concatenate((SOS_token, input_ids, EOS_token))\n",
        "      y = np.concatenate((SOS_token, target_ids, EOS_token))\n",
        "\n",
        "      data.append([X.tolist(), y.tolist()])\n",
        "\n",
        "    np.random.shuffle(data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "Za__KeXoSt0A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = generate_data(train_dataset, tokenizer)\n",
        "val_data = generate_data(valid_dataset, tokenizer)"
      ],
      "metadata": {
        "id": "R2xYKC9gSvwO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(train_data)\n",
        "\n",
        "# Check for NaN values\n",
        "nan_values = df.isnull().sum().sum()\n",
        "\n",
        "\n",
        "if nan_values == 0:\n",
        "    print(\"No NaN values found in the dataset.\")\n",
        "else:\n",
        "    print(f\"Total NaN values found: {nan_values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUcHmzPrwYjv",
        "outputId": "b8f31858-dff6-440b-f0af-737faf517bca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No NaN values found in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_numeric(val):\n",
        "    try:\n",
        "        pd.to_numeric(val)\n",
        "        return True\n",
        "    except (ValueError, TypeError):\n",
        "        return False\n",
        "\n",
        "# Find non-numeric elements in each column\n",
        "non_numeric_elements = {}\n",
        "for col in df.columns:\n",
        "    non_numeric_elements[col] = df[~df[col].apply(is_numeric)][col]\n",
        "\n",
        "# Print non-numeric elements\n",
        "for col, values in non_numeric_elements.items():\n",
        "    if not values.empty:\n",
        "        print(f\"Non-numeric elements in column '{col}':\")\n",
        "        print(values)"
      ],
      "metadata": {
        "id": "PJkz-6aH6SmX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
        "    batches = []\n",
        "    for idx in range(0, len(data), batch_size):\n",
        "        # We make sure we dont get the last bit if its not batch_size size\n",
        "        if idx + batch_size < len(data):\n",
        "            # Here you would need to get the max length of the batch,\n",
        "            # and normalize the length with the PAD token.\n",
        "            if padding:\n",
        "                max_batch_length = 0\n",
        "\n",
        "                # Get longest sentence in batch\n",
        "                for seq in data[idx : idx + batch_size]:\n",
        "                    if len(seq[0]) > max_batch_length:\n",
        "                        max_batch_length = len(seq[0])\n",
        "                    if len(seq[1]) > max_batch_length:\n",
        "                        max_batch_length = len(seq[1])\n",
        "\n",
        "                # Append X padding tokens until it reaches the max length\n",
        "                for seq_idx in range(batch_size):\n",
        "                    remaining_length = max_batch_length - len(data[idx + seq_idx][0])\n",
        "                    data[idx + seq_idx][0] += [padding_token] * remaining_length\n",
        "                    # padding_array = np.array([padding_token] * remaining_length)\n",
        "                    # data[idx + seq_idx][0] = np.concatenate((data[idx + seq_idx][0], padding_array))\n",
        "\n",
        "                    remaining_length = max_batch_length - len(data[idx + seq_idx][1])\n",
        "                    # padding_array = np.array([padding_token] * remaining_length)\n",
        "                    data[idx + seq_idx][1] += [padding_token] * remaining_length\n",
        "                    # data[idx + seq_idx][1] = np.concatenate((data[idx + seq_idx][1], padding_array))\n",
        "\n",
        "            # batches.append(data[idx : idx + batch_size])\n",
        "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
        "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
        "\n",
        "    return batches"
      ],
      "metadata": {
        "id": "K91y1xqSQ3pp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = batchify_data(train_data, batch_size=2, padding = True, padding_token = src_pad_idx)\n",
        "val_dataloader = batchify_data(val_data, batch_size=2, padding = True, padding_token = src_pad_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0Q26x5to8Tk",
        "outputId": "2e8b97db-9256-44cb-a077-7c39f5b6c52a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75490 batches of size 2\n",
            "1006 batches of size 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = Transformer(\n",
        "    num_tokens=enc_voc_size, dim_model=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dropout_p=0.1\n",
        ").to(device)\n",
        "# opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "# loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Wxm1IHVxS8EL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0def372c-8c13-4698-9d3c-8491c9ab4e45"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.kaiming_uniform_(m.weight.data)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "model.apply(initialize_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1o_Y8uJoPK3",
        "outputId": "c9bcd0e8-2bab-45f0-f01e-9d84b7f5afa4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 146,640,544 trainable parameters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (positional_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (embedding): Embedding(100000, 512)\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=512, out_features=100000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "init_lr = 1e-6\n",
        "factor = 0.9\n",
        "adam_eps = 5e-9\n",
        "patience = 10\n",
        "# warmup = 100\n",
        "clip = 1.0\n",
        "weight_decay = 5e-4\n",
        "\n",
        "# optimizer = Adam(params=model.parameters(),\n",
        "#                  lr=init_lr,\n",
        "#                  weight_decay=weight_decay,\n",
        "#                  eps=adam_eps)\n",
        "\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "#                                                  verbose=True,\n",
        "#                                                  factor=factor,\n",
        "#                                                  patience=patience)\n",
        "\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
        "optimizer = AdamW(model.parameters(), lr=0.000007, betas=(0.9, 0.99), weight_decay = 0.1)\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters(), lr=init_lr, weight_decay=0.01)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), 1e-02, weight_decay=0.01)\n",
        "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "# scheduler = optim.lr_scheduler.CyclicLR(optimizer,base_lr=init_lr,max_lr=0.001,mode='triangular2', cycle_momentum=True)\n"
      ],
      "metadata": {
        "id": "_dz-bYFjoRku"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_loop(model, opt, loss_fn, dataloader):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        # batch = np.array(batch)\n",
        "        # print(batch[0])\n",
        "        X, y = batch[:, 0], batch[:, 1]\n",
        "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
        "\n",
        "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "        y_input = y[:,:-1]\n",
        "        y_expected = y[:,1:]\n",
        "\n",
        "        # Get mask to mask out the next words\n",
        "        sequence_length = y_input.size(1)\n",
        "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "        # Standard training except we pass in y_input and tgt_mask\n",
        "        pred = model(X, y_input, tgt_mask)\n",
        "\n",
        "        # Permute pred to have batch size first again\n",
        "        pred = pred.permute(1, 2, 0)\n",
        "        # print(pred[0,:,:], y_expected[0])\n",
        "        # break\n",
        "        loss = loss_fn(pred, y_expected)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.detach().item()\n",
        "        # scheduler.step()\n",
        "        # if torch.isnan(loss).any():\n",
        "        #       print(\"NaN loss detected! Debugging information:\")\n",
        "        #       print(\"Input Data (X):\", X)\n",
        "        #       print(\"Expected Output (y_expected):\", y_expected)\n",
        "        #       print(\"Expected Output (y_input):\", y_input)\n",
        "        #       print(\"Model Predictions (pred):\", pred)\n",
        "        #       # print(\"Gradients:\", [param.grad for param in model.parameters() if param.grad is not None])\n",
        "\n",
        "        # Print learning rate\n",
        "        learning_rate = opt.param_groups[0]['lr']\n",
        "        if (step + 1) % 10 == 0:  # Adjust frequency as needed\n",
        "            elapsed_time = time.time() - start_time\n",
        "            avg_loss = total_loss / step\n",
        "            print(f\"Step [{step+1}/{len(dataloader)}], Loss: {loss.item():.4f}, Time: {elapsed_time:.2f} seconds, Average Loss: {avg_loss:.4f}, Learning Rate: {learning_rate}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "RMdU626HTCez"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def validation_loop(model, loss_fn, dataloader):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            X, y = batch[:, 0], batch[:, 1]\n",
        "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
        "\n",
        "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "            y_input = y[:,:-1]\n",
        "            y_expected = y[:,1:]\n",
        "\n",
        "            # Get mask to mask out the next words\n",
        "            sequence_length = y_input.size(1)\n",
        "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "            # Standard training except we pass in y_input and src_mask\n",
        "            pred = model(X, y_input, tgt_mask)\n",
        "\n",
        "            # Permute pred to have batch size first again\n",
        "            pred = pred.permute(1, 2, 0)\n",
        "            loss = loss_fn(pred, y_expected)\n",
        "            total_loss += loss.detach().item()\n",
        "\n",
        "            # if torch.isnan(loss).any():\n",
        "            #   print(\"NaN loss detected! Debugging information:\")\n",
        "            #   print(\"Input Data (X):\", X)\n",
        "            #   print(\"Expected Output (y_expected):\", y_expected)\n",
        "            #   print(\"Expected Output (y_input):\", y_input)\n",
        "            #   print(\"Model Predictions (pred):\", pred)\n",
        "            #   print(\"Gradients:\", [param.grad for param in model.parameters() if param.grad is not None])\n",
        "\n",
        "            #   # Additional information\n",
        "            #   print(\"Model Parameters:\")\n",
        "            #   for name, param in model.named_parameters():\n",
        "            #       print(name, param.data)\n",
        "\n",
        "            #   print(\"Optimizer State:\")\n",
        "            #   for param_group in opt.param_groups:\n",
        "            #       print(\"Learning Rate:\", param_group['lr'])\n",
        "\n",
        "            #   # Add more debugging information as needed\n",
        "\n",
        "            #   print(\"Loss Function Parameters:\")\n",
        "            #   for name, param in loss_fn.named_parameters():\n",
        "            #       print(name, param.data)\n",
        "            #   break\n",
        "\n",
        "            if (step + 1) % 10 == 0:  # Adjust frequency as needed\n",
        "              elapsed_time = time.time() - start_time\n",
        "              print(f\"Step [{step+1}/{len(dataloader)}], Loss: {loss.item():.4f}, Time: {elapsed_time:.2f} seconds\")\n",
        "              start_time = time.time()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "TZQBz5OiT55I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "\n",
        "    # Used for plotting later on\n",
        "    train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "    print(\"Training and validating model\")\n",
        "    for epoch in range(epochs):\n",
        "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
        "\n",
        "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
        "        # break\n",
        "        train_loss_list += [train_loss]\n",
        "\n",
        "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
        "        validation_loss_list += [validation_loss]\n",
        "\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
        "        print()\n",
        "\n",
        "    return train_loss_list, validation_loss_list"
      ],
      "metadata": {
        "id": "B-IKivf4T8yQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, validation_loss_list = fit(model, optimizer, loss_fn, train_dataloader, val_dataloader, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mfyzQgF0T-6q",
        "outputId": "1a2c0e2d-ddb6-49c6-cdd5-f7c07b72331c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and validating model\n",
            "------------------------- Epoch 1 -------------------------\n",
            "Step [10/75490], Loss: 11.8338, Time: 2.25 seconds, Average Loss: 13.6187, Learning Rate: 7e-06\n",
            "Step [20/75490], Loss: 11.3741, Time: 1.37 seconds, Average Loss: 12.5173, Learning Rate: 7e-06\n",
            "Step [30/75490], Loss: 11.5637, Time: 1.38 seconds, Average Loss: 12.1180, Learning Rate: 7e-06\n",
            "Step [40/75490], Loss: 10.5496, Time: 2.12 seconds, Average Loss: 11.8339, Learning Rate: 7e-06\n",
            "Step [50/75490], Loss: 10.8405, Time: 1.54 seconds, Average Loss: 11.5993, Learning Rate: 7e-06\n",
            "Step [60/75490], Loss: 10.4227, Time: 1.99 seconds, Average Loss: 11.4419, Learning Rate: 7e-06\n",
            "Step [70/75490], Loss: 10.4999, Time: 1.90 seconds, Average Loss: 11.3216, Learning Rate: 7e-06\n",
            "Step [80/75490], Loss: 10.6138, Time: 1.63 seconds, Average Loss: 11.2181, Learning Rate: 7e-06\n",
            "Step [90/75490], Loss: 9.9632, Time: 1.87 seconds, Average Loss: 11.1034, Learning Rate: 7e-06\n",
            "Step [100/75490], Loss: 10.2039, Time: 1.50 seconds, Average Loss: 10.9884, Learning Rate: 7e-06\n",
            "Step [110/75490], Loss: 10.3426, Time: 1.69 seconds, Average Loss: 10.8931, Learning Rate: 7e-06\n",
            "Step [120/75490], Loss: 10.7500, Time: 1.66 seconds, Average Loss: 10.8360, Learning Rate: 7e-06\n",
            "Step [130/75490], Loss: 10.5661, Time: 1.90 seconds, Average Loss: 10.7850, Learning Rate: 7e-06\n",
            "Step [140/75490], Loss: 9.4014, Time: 2.12 seconds, Average Loss: 10.7237, Learning Rate: 7e-06\n",
            "Step [150/75490], Loss: 10.0305, Time: 1.71 seconds, Average Loss: 10.7001, Learning Rate: 7e-06\n",
            "Step [160/75490], Loss: 9.8724, Time: 1.62 seconds, Average Loss: 10.6547, Learning Rate: 7e-06\n",
            "Step [170/75490], Loss: 9.6098, Time: 1.55 seconds, Average Loss: 10.6061, Learning Rate: 7e-06\n",
            "Step [180/75490], Loss: 10.5990, Time: 1.54 seconds, Average Loss: 10.5713, Learning Rate: 7e-06\n",
            "Step [190/75490], Loss: 9.6892, Time: 2.13 seconds, Average Loss: 10.5340, Learning Rate: 7e-06\n",
            "Step [200/75490], Loss: 10.0140, Time: 1.88 seconds, Average Loss: 10.5039, Learning Rate: 7e-06\n",
            "Step [210/75490], Loss: 9.2945, Time: 1.86 seconds, Average Loss: 10.4646, Learning Rate: 7e-06\n",
            "Step [220/75490], Loss: 8.8943, Time: 1.82 seconds, Average Loss: 10.4330, Learning Rate: 7e-06\n",
            "Step [230/75490], Loss: 9.8535, Time: 1.40 seconds, Average Loss: 10.4016, Learning Rate: 7e-06\n",
            "Step [240/75490], Loss: 9.6635, Time: 1.64 seconds, Average Loss: 10.3778, Learning Rate: 7e-06\n",
            "Step [250/75490], Loss: 10.0098, Time: 1.79 seconds, Average Loss: 10.3548, Learning Rate: 7e-06\n",
            "Step [260/75490], Loss: 8.9423, Time: 1.69 seconds, Average Loss: 10.3162, Learning Rate: 7e-06\n",
            "Step [270/75490], Loss: 9.3206, Time: 1.96 seconds, Average Loss: 10.2812, Learning Rate: 7e-06\n",
            "Step [280/75490], Loss: 10.0647, Time: 1.38 seconds, Average Loss: 10.2600, Learning Rate: 7e-06\n",
            "Step [290/75490], Loss: 9.3633, Time: 1.41 seconds, Average Loss: 10.2325, Learning Rate: 7e-06\n",
            "Step [300/75490], Loss: 9.8157, Time: 1.66 seconds, Average Loss: 10.2068, Learning Rate: 7e-06\n",
            "Step [310/75490], Loss: 8.9608, Time: 1.46 seconds, Average Loss: 10.1847, Learning Rate: 7e-06\n",
            "Step [320/75490], Loss: 10.0799, Time: 1.92 seconds, Average Loss: 10.1607, Learning Rate: 7e-06\n",
            "Step [330/75490], Loss: 10.2553, Time: 1.66 seconds, Average Loss: 10.1434, Learning Rate: 7e-06\n",
            "Step [340/75490], Loss: 9.6214, Time: 1.91 seconds, Average Loss: 10.1318, Learning Rate: 7e-06\n",
            "Step [350/75490], Loss: 9.3055, Time: 1.49 seconds, Average Loss: 10.1083, Learning Rate: 7e-06\n",
            "Step [360/75490], Loss: 9.5938, Time: 1.61 seconds, Average Loss: 10.0960, Learning Rate: 7e-06\n",
            "Step [370/75490], Loss: 9.1276, Time: 1.20 seconds, Average Loss: 10.0746, Learning Rate: 7e-06\n",
            "Step [380/75490], Loss: 8.7730, Time: 1.74 seconds, Average Loss: 10.0593, Learning Rate: 7e-06\n",
            "Step [390/75490], Loss: 9.5369, Time: 1.72 seconds, Average Loss: 10.0343, Learning Rate: 7e-06\n",
            "Step [400/75490], Loss: 9.4624, Time: 1.22 seconds, Average Loss: 10.0235, Learning Rate: 7e-06\n",
            "Step [410/75490], Loss: 8.7221, Time: 1.23 seconds, Average Loss: 10.0065, Learning Rate: 7e-06\n",
            "Step [420/75490], Loss: 9.2633, Time: 1.86 seconds, Average Loss: 9.9899, Learning Rate: 7e-06\n",
            "Step [430/75490], Loss: 9.2667, Time: 1.58 seconds, Average Loss: 9.9764, Learning Rate: 7e-06\n",
            "Step [440/75490], Loss: 8.6331, Time: 1.59 seconds, Average Loss: 9.9581, Learning Rate: 7e-06\n",
            "Step [450/75490], Loss: 8.7328, Time: 1.81 seconds, Average Loss: 9.9394, Learning Rate: 7e-06\n",
            "Step [460/75490], Loss: 8.6237, Time: 1.82 seconds, Average Loss: 9.9202, Learning Rate: 7e-06\n",
            "Step [470/75490], Loss: 9.2471, Time: 1.67 seconds, Average Loss: 9.9056, Learning Rate: 7e-06\n",
            "Step [480/75490], Loss: 9.8169, Time: 2.18 seconds, Average Loss: 9.8867, Learning Rate: 7e-06\n",
            "Step [490/75490], Loss: 9.1688, Time: 1.82 seconds, Average Loss: 9.8691, Learning Rate: 7e-06\n",
            "Step [500/75490], Loss: 9.4460, Time: 1.29 seconds, Average Loss: 9.8534, Learning Rate: 7e-06\n",
            "Step [510/75490], Loss: 9.0659, Time: 2.08 seconds, Average Loss: 9.8421, Learning Rate: 7e-06\n",
            "Step [520/75490], Loss: 8.8081, Time: 1.26 seconds, Average Loss: 9.8241, Learning Rate: 7e-06\n",
            "Step [530/75490], Loss: 9.4125, Time: 1.70 seconds, Average Loss: 9.8085, Learning Rate: 7e-06\n",
            "Step [540/75490], Loss: 8.7359, Time: 1.49 seconds, Average Loss: 9.7922, Learning Rate: 7e-06\n",
            "Step [550/75490], Loss: 9.7909, Time: 1.73 seconds, Average Loss: 9.7737, Learning Rate: 7e-06\n",
            "Step [560/75490], Loss: 8.8960, Time: 1.98 seconds, Average Loss: 9.7597, Learning Rate: 7e-06\n",
            "Step [570/75490], Loss: 8.9711, Time: 1.22 seconds, Average Loss: 9.7465, Learning Rate: 7e-06\n",
            "Step [580/75490], Loss: 8.8254, Time: 1.94 seconds, Average Loss: 9.7303, Learning Rate: 7e-06\n",
            "Step [590/75490], Loss: 8.7195, Time: 2.55 seconds, Average Loss: 9.7158, Learning Rate: 7e-06\n",
            "Step [600/75490], Loss: 8.2807, Time: 1.86 seconds, Average Loss: 9.6958, Learning Rate: 7e-06\n",
            "Step [610/75490], Loss: 9.7589, Time: 1.81 seconds, Average Loss: 9.6824, Learning Rate: 7e-06\n",
            "Step [620/75490], Loss: 8.5543, Time: 1.24 seconds, Average Loss: 9.6715, Learning Rate: 7e-06\n",
            "Step [630/75490], Loss: 8.9510, Time: 1.54 seconds, Average Loss: 9.6612, Learning Rate: 7e-06\n",
            "Step [640/75490], Loss: 8.1030, Time: 1.58 seconds, Average Loss: 9.6494, Learning Rate: 7e-06\n",
            "Step [650/75490], Loss: 9.3462, Time: 1.37 seconds, Average Loss: 9.6389, Learning Rate: 7e-06\n",
            "Step [660/75490], Loss: 9.3357, Time: 1.80 seconds, Average Loss: 9.6301, Learning Rate: 7e-06\n",
            "Step [670/75490], Loss: 8.8009, Time: 1.78 seconds, Average Loss: 9.6189, Learning Rate: 7e-06\n",
            "Step [680/75490], Loss: 8.1128, Time: 2.20 seconds, Average Loss: 9.6041, Learning Rate: 7e-06\n",
            "Step [690/75490], Loss: 8.8134, Time: 1.60 seconds, Average Loss: 9.5906, Learning Rate: 7e-06\n",
            "Step [700/75490], Loss: 8.6437, Time: 1.49 seconds, Average Loss: 9.5780, Learning Rate: 7e-06\n",
            "Step [710/75490], Loss: 9.5160, Time: 1.36 seconds, Average Loss: 9.5658, Learning Rate: 7e-06\n",
            "Step [720/75490], Loss: 7.9236, Time: 1.68 seconds, Average Loss: 9.5494, Learning Rate: 7e-06\n",
            "Step [730/75490], Loss: 9.3079, Time: 1.20 seconds, Average Loss: 9.5406, Learning Rate: 7e-06\n",
            "Step [740/75490], Loss: 8.3763, Time: 1.86 seconds, Average Loss: 9.5279, Learning Rate: 7e-06\n",
            "Step [750/75490], Loss: 7.2123, Time: 2.53 seconds, Average Loss: 9.5137, Learning Rate: 7e-06\n",
            "Step [760/75490], Loss: 8.4833, Time: 2.07 seconds, Average Loss: 9.5034, Learning Rate: 7e-06\n",
            "Step [770/75490], Loss: 9.4825, Time: 1.75 seconds, Average Loss: 9.4936, Learning Rate: 7e-06\n",
            "Step [780/75490], Loss: 8.7478, Time: 1.38 seconds, Average Loss: 9.4815, Learning Rate: 7e-06\n",
            "Step [790/75490], Loss: 8.9169, Time: 1.67 seconds, Average Loss: 9.4692, Learning Rate: 7e-06\n",
            "Step [800/75490], Loss: 8.8634, Time: 1.70 seconds, Average Loss: 9.4601, Learning Rate: 7e-06\n",
            "Step [810/75490], Loss: 9.1656, Time: 1.83 seconds, Average Loss: 9.4505, Learning Rate: 7e-06\n",
            "Step [820/75490], Loss: 7.6630, Time: 1.79 seconds, Average Loss: 9.4388, Learning Rate: 7e-06\n",
            "Step [830/75490], Loss: 6.9672, Time: 1.45 seconds, Average Loss: 9.4221, Learning Rate: 7e-06\n",
            "Step [840/75490], Loss: 8.7035, Time: 2.29 seconds, Average Loss: 9.4128, Learning Rate: 7e-06\n",
            "Step [850/75490], Loss: 9.6946, Time: 1.68 seconds, Average Loss: 9.4041, Learning Rate: 7e-06\n",
            "Step [860/75490], Loss: 8.9147, Time: 1.52 seconds, Average Loss: 9.3927, Learning Rate: 7e-06\n",
            "Step [870/75490], Loss: 7.6655, Time: 1.38 seconds, Average Loss: 9.3794, Learning Rate: 7e-06\n",
            "Step [880/75490], Loss: 9.4686, Time: 1.70 seconds, Average Loss: 9.3654, Learning Rate: 7e-06\n",
            "Step [890/75490], Loss: 8.8795, Time: 1.53 seconds, Average Loss: 9.3535, Learning Rate: 7e-06\n",
            "Step [900/75490], Loss: 8.7147, Time: 1.47 seconds, Average Loss: 9.3398, Learning Rate: 7e-06\n",
            "Step [910/75490], Loss: 8.8120, Time: 1.93 seconds, Average Loss: 9.3331, Learning Rate: 7e-06\n",
            "Step [920/75490], Loss: 8.3476, Time: 1.51 seconds, Average Loss: 9.3219, Learning Rate: 7e-06\n",
            "Step [930/75490], Loss: 8.2978, Time: 1.71 seconds, Average Loss: 9.3118, Learning Rate: 7e-06\n",
            "Step [940/75490], Loss: 8.3152, Time: 1.64 seconds, Average Loss: 9.3035, Learning Rate: 7e-06\n",
            "Step [950/75490], Loss: 9.5443, Time: 1.71 seconds, Average Loss: 9.2940, Learning Rate: 7e-06\n",
            "Step [960/75490], Loss: 7.8836, Time: 1.69 seconds, Average Loss: 9.2801, Learning Rate: 7e-06\n",
            "Step [970/75490], Loss: 7.5905, Time: 2.28 seconds, Average Loss: 9.2711, Learning Rate: 7e-06\n",
            "Step [980/75490], Loss: 8.0857, Time: 1.72 seconds, Average Loss: 9.2586, Learning Rate: 7e-06\n",
            "Step [990/75490], Loss: 8.0771, Time: 2.13 seconds, Average Loss: 9.2464, Learning Rate: 7e-06\n",
            "Step [1000/75490], Loss: 8.5519, Time: 1.86 seconds, Average Loss: 9.2350, Learning Rate: 7e-06\n",
            "Step [1010/75490], Loss: 8.9309, Time: 1.69 seconds, Average Loss: 9.2248, Learning Rate: 7e-06\n",
            "Step [1020/75490], Loss: 9.8151, Time: 1.41 seconds, Average Loss: 9.2181, Learning Rate: 7e-06\n",
            "Step [1030/75490], Loss: 6.7162, Time: 1.83 seconds, Average Loss: 9.2075, Learning Rate: 7e-06\n",
            "Step [1040/75490], Loss: 8.4096, Time: 1.70 seconds, Average Loss: 9.1961, Learning Rate: 7e-06\n",
            "Step [1050/75490], Loss: 7.2296, Time: 1.53 seconds, Average Loss: 9.1839, Learning Rate: 7e-06\n",
            "Step [1060/75490], Loss: 8.0027, Time: 1.70 seconds, Average Loss: 9.1761, Learning Rate: 7e-06\n",
            "Step [1070/75490], Loss: 7.7838, Time: 1.80 seconds, Average Loss: 9.1628, Learning Rate: 7e-06\n",
            "Step [1080/75490], Loss: 8.0721, Time: 1.76 seconds, Average Loss: 9.1565, Learning Rate: 7e-06\n",
            "Step [1090/75490], Loss: 7.8900, Time: 1.97 seconds, Average Loss: 9.1458, Learning Rate: 7e-06\n",
            "Step [1100/75490], Loss: 7.3807, Time: 2.00 seconds, Average Loss: 9.1357, Learning Rate: 7e-06\n",
            "Step [1110/75490], Loss: 7.8839, Time: 1.04 seconds, Average Loss: 9.1268, Learning Rate: 7e-06\n",
            "Step [1120/75490], Loss: 8.3048, Time: 1.86 seconds, Average Loss: 9.1161, Learning Rate: 7e-06\n",
            "Step [1130/75490], Loss: 8.1979, Time: 1.41 seconds, Average Loss: 9.1051, Learning Rate: 7e-06\n",
            "Step [1140/75490], Loss: 7.8662, Time: 1.60 seconds, Average Loss: 9.0943, Learning Rate: 7e-06\n",
            "Step [1150/75490], Loss: 8.7490, Time: 1.60 seconds, Average Loss: 9.0844, Learning Rate: 7e-06\n",
            "Step [1160/75490], Loss: 8.0189, Time: 1.88 seconds, Average Loss: 9.0752, Learning Rate: 7e-06\n",
            "Step [1170/75490], Loss: 8.5238, Time: 1.73 seconds, Average Loss: 9.0672, Learning Rate: 7e-06\n",
            "Step [1180/75490], Loss: 9.6230, Time: 2.25 seconds, Average Loss: 9.0578, Learning Rate: 7e-06\n",
            "Step [1190/75490], Loss: 8.0353, Time: 1.81 seconds, Average Loss: 9.0486, Learning Rate: 7e-06\n",
            "Step [1200/75490], Loss: 7.4518, Time: 1.83 seconds, Average Loss: 9.0391, Learning Rate: 7e-06\n",
            "Step [1210/75490], Loss: 9.0985, Time: 1.83 seconds, Average Loss: 9.0307, Learning Rate: 7e-06\n",
            "Step [1220/75490], Loss: 8.3291, Time: 1.58 seconds, Average Loss: 9.0219, Learning Rate: 7e-06\n",
            "Step [1230/75490], Loss: 7.6917, Time: 1.62 seconds, Average Loss: 9.0109, Learning Rate: 7e-06\n",
            "Step [1240/75490], Loss: 8.4306, Time: 1.17 seconds, Average Loss: 9.0027, Learning Rate: 7e-06\n",
            "Step [1250/75490], Loss: 8.0166, Time: 1.55 seconds, Average Loss: 8.9944, Learning Rate: 7e-06\n",
            "Step [1260/75490], Loss: 7.7880, Time: 1.52 seconds, Average Loss: 8.9857, Learning Rate: 7e-06\n",
            "Step [1270/75490], Loss: 7.2331, Time: 1.36 seconds, Average Loss: 8.9784, Learning Rate: 7e-06\n",
            "Step [1280/75490], Loss: 8.2245, Time: 1.84 seconds, Average Loss: 8.9709, Learning Rate: 7e-06\n",
            "Step [1290/75490], Loss: 6.7493, Time: 1.55 seconds, Average Loss: 8.9631, Learning Rate: 7e-06\n",
            "Step [1300/75490], Loss: 6.4693, Time: 1.97 seconds, Average Loss: 8.9519, Learning Rate: 7e-06\n",
            "Step [1310/75490], Loss: 8.9517, Time: 2.34 seconds, Average Loss: 8.9414, Learning Rate: 7e-06\n",
            "Step [1320/75490], Loss: 6.6264, Time: 2.32 seconds, Average Loss: 8.9329, Learning Rate: 7e-06\n",
            "Step [1330/75490], Loss: 7.8406, Time: 1.56 seconds, Average Loss: 8.9243, Learning Rate: 7e-06\n",
            "Step [1340/75490], Loss: 7.9249, Time: 1.76 seconds, Average Loss: 8.9176, Learning Rate: 7e-06\n",
            "Step [1350/75490], Loss: 6.7534, Time: 1.57 seconds, Average Loss: 8.9085, Learning Rate: 7e-06\n",
            "Step [1360/75490], Loss: 8.2606, Time: 1.43 seconds, Average Loss: 8.9012, Learning Rate: 7e-06\n",
            "Step [1370/75490], Loss: 7.5140, Time: 1.57 seconds, Average Loss: 8.8930, Learning Rate: 7e-06\n",
            "Step [1380/75490], Loss: 6.8841, Time: 1.72 seconds, Average Loss: 8.8839, Learning Rate: 7e-06\n",
            "Step [1390/75490], Loss: 8.1073, Time: 1.57 seconds, Average Loss: 8.8748, Learning Rate: 7e-06\n",
            "Step [1400/75490], Loss: 9.4004, Time: 1.97 seconds, Average Loss: 8.8690, Learning Rate: 7e-06\n",
            "Step [1410/75490], Loss: 7.7436, Time: 1.75 seconds, Average Loss: 8.8635, Learning Rate: 7e-06\n",
            "Step [1420/75490], Loss: 8.2664, Time: 1.40 seconds, Average Loss: 8.8562, Learning Rate: 7e-06\n",
            "Step [1430/75490], Loss: 6.7256, Time: 1.85 seconds, Average Loss: 8.8484, Learning Rate: 7e-06\n",
            "Step [1440/75490], Loss: 7.6601, Time: 1.52 seconds, Average Loss: 8.8382, Learning Rate: 7e-06\n",
            "Step [1450/75490], Loss: 6.6815, Time: 1.93 seconds, Average Loss: 8.8354, Learning Rate: 7e-06\n",
            "Step [1460/75490], Loss: 8.2848, Time: 1.39 seconds, Average Loss: 8.8288, Learning Rate: 7e-06\n",
            "Step [1470/75490], Loss: 7.8804, Time: 1.87 seconds, Average Loss: 8.8202, Learning Rate: 7e-06\n",
            "Step [1480/75490], Loss: 7.7997, Time: 2.17 seconds, Average Loss: 8.8112, Learning Rate: 7e-06\n",
            "Step [1490/75490], Loss: 7.8425, Time: 1.63 seconds, Average Loss: 8.8049, Learning Rate: 7e-06\n",
            "Step [1500/75490], Loss: 7.8024, Time: 1.53 seconds, Average Loss: 8.7982, Learning Rate: 7e-06\n",
            "Step [1510/75490], Loss: 6.4740, Time: 1.83 seconds, Average Loss: 8.7893, Learning Rate: 7e-06\n",
            "Step [1520/75490], Loss: 8.0380, Time: 1.90 seconds, Average Loss: 8.7845, Learning Rate: 7e-06\n",
            "Step [1530/75490], Loss: 7.7119, Time: 2.00 seconds, Average Loss: 8.7783, Learning Rate: 7e-06\n",
            "Step [1540/75490], Loss: 7.8557, Time: 1.55 seconds, Average Loss: 8.7711, Learning Rate: 7e-06\n",
            "Step [1550/75490], Loss: 8.5822, Time: 1.61 seconds, Average Loss: 8.7658, Learning Rate: 7e-06\n",
            "Step [1560/75490], Loss: 7.3725, Time: 1.99 seconds, Average Loss: 8.7572, Learning Rate: 7e-06\n",
            "Step [1570/75490], Loss: 7.3475, Time: 2.04 seconds, Average Loss: 8.7518, Learning Rate: 7e-06\n",
            "Step [1580/75490], Loss: 8.4706, Time: 2.67 seconds, Average Loss: 8.7467, Learning Rate: 7e-06\n",
            "Step [1590/75490], Loss: 6.9918, Time: 1.74 seconds, Average Loss: 8.7402, Learning Rate: 7e-06\n",
            "Step [1600/75490], Loss: 6.4341, Time: 2.01 seconds, Average Loss: 8.7335, Learning Rate: 7e-06\n",
            "Step [1610/75490], Loss: 8.2417, Time: 1.56 seconds, Average Loss: 8.7264, Learning Rate: 7e-06\n",
            "Step [1620/75490], Loss: 7.6273, Time: 1.94 seconds, Average Loss: 8.7190, Learning Rate: 7e-06\n",
            "Step [1630/75490], Loss: 8.1755, Time: 1.62 seconds, Average Loss: 8.7136, Learning Rate: 7e-06\n",
            "Step [1640/75490], Loss: 7.7734, Time: 1.20 seconds, Average Loss: 8.7071, Learning Rate: 7e-06\n",
            "Step [1650/75490], Loss: 6.9447, Time: 1.78 seconds, Average Loss: 8.7014, Learning Rate: 7e-06\n",
            "Step [1660/75490], Loss: 7.8819, Time: 1.73 seconds, Average Loss: 8.6944, Learning Rate: 7e-06\n",
            "Step [1670/75490], Loss: 7.8024, Time: 1.58 seconds, Average Loss: 8.6891, Learning Rate: 7e-06\n",
            "Step [1680/75490], Loss: 7.5229, Time: 2.32 seconds, Average Loss: 8.6845, Learning Rate: 7e-06\n",
            "Step [1690/75490], Loss: 7.8767, Time: 1.68 seconds, Average Loss: 8.6772, Learning Rate: 7e-06\n",
            "Step [1700/75490], Loss: 7.7152, Time: 1.89 seconds, Average Loss: 8.6731, Learning Rate: 7e-06\n",
            "Step [1710/75490], Loss: 7.5075, Time: 1.59 seconds, Average Loss: 8.6672, Learning Rate: 7e-06\n",
            "Step [1720/75490], Loss: 7.0949, Time: 2.19 seconds, Average Loss: 8.6644, Learning Rate: 7e-06\n",
            "Step [1730/75490], Loss: 6.9731, Time: 1.65 seconds, Average Loss: 8.6583, Learning Rate: 7e-06\n",
            "Step [1740/75490], Loss: 7.7615, Time: 1.98 seconds, Average Loss: 8.6511, Learning Rate: 7e-06\n",
            "Step [1750/75490], Loss: 8.2559, Time: 1.42 seconds, Average Loss: 8.6451, Learning Rate: 7e-06\n",
            "Step [1760/75490], Loss: 7.9851, Time: 1.89 seconds, Average Loss: 8.6395, Learning Rate: 7e-06\n",
            "Step [1770/75490], Loss: 7.7539, Time: 1.78 seconds, Average Loss: 8.6334, Learning Rate: 7e-06\n",
            "Step [1780/75490], Loss: 7.4750, Time: 1.45 seconds, Average Loss: 8.6255, Learning Rate: 7e-06\n",
            "Step [1790/75490], Loss: 7.5605, Time: 1.30 seconds, Average Loss: 8.6216, Learning Rate: 7e-06\n",
            "Step [1800/75490], Loss: 5.9317, Time: 1.48 seconds, Average Loss: 8.6152, Learning Rate: 7e-06\n",
            "Step [1810/75490], Loss: 7.3614, Time: 1.63 seconds, Average Loss: 8.6093, Learning Rate: 7e-06\n",
            "Step [1820/75490], Loss: 7.3914, Time: 1.39 seconds, Average Loss: 8.6017, Learning Rate: 7e-06\n",
            "Step [1830/75490], Loss: 8.5319, Time: 1.53 seconds, Average Loss: 8.5970, Learning Rate: 7e-06\n",
            "Step [1840/75490], Loss: 8.1540, Time: 1.81 seconds, Average Loss: 8.5891, Learning Rate: 7e-06\n",
            "Step [1850/75490], Loss: 7.7280, Time: 1.71 seconds, Average Loss: 8.5835, Learning Rate: 7e-06\n",
            "Step [1860/75490], Loss: 8.5557, Time: 1.67 seconds, Average Loss: 8.5785, Learning Rate: 7e-06\n",
            "Step [1870/75490], Loss: 8.2030, Time: 1.83 seconds, Average Loss: 8.5736, Learning Rate: 7e-06\n",
            "Step [1880/75490], Loss: 8.4884, Time: 1.49 seconds, Average Loss: 8.5679, Learning Rate: 7e-06\n",
            "Step [1890/75490], Loss: 7.1575, Time: 2.33 seconds, Average Loss: 8.5606, Learning Rate: 7e-06\n",
            "Step [1900/75490], Loss: 7.3082, Time: 1.38 seconds, Average Loss: 8.5555, Learning Rate: 7e-06\n",
            "Step [1910/75490], Loss: 7.4131, Time: 1.42 seconds, Average Loss: 8.5495, Learning Rate: 7e-06\n",
            "Step [1920/75490], Loss: 7.4118, Time: 1.83 seconds, Average Loss: 8.5443, Learning Rate: 7e-06\n",
            "Step [1930/75490], Loss: 7.9466, Time: 1.96 seconds, Average Loss: 8.5385, Learning Rate: 7e-06\n",
            "Step [1940/75490], Loss: 8.5038, Time: 1.51 seconds, Average Loss: 8.5321, Learning Rate: 7e-06\n",
            "Step [1950/75490], Loss: 6.3776, Time: 1.96 seconds, Average Loss: 8.5284, Learning Rate: 7e-06\n",
            "Step [1960/75490], Loss: 7.2358, Time: 1.41 seconds, Average Loss: 8.5213, Learning Rate: 7e-06\n",
            "Step [1970/75490], Loss: 7.1192, Time: 1.96 seconds, Average Loss: 8.5150, Learning Rate: 7e-06\n",
            "Step [1980/75490], Loss: 6.7086, Time: 2.28 seconds, Average Loss: 8.5103, Learning Rate: 7e-06\n",
            "Step [1990/75490], Loss: 8.0566, Time: 1.82 seconds, Average Loss: 8.5064, Learning Rate: 7e-06\n",
            "Step [2000/75490], Loss: 7.6751, Time: 1.54 seconds, Average Loss: 8.5004, Learning Rate: 7e-06\n",
            "Step [2010/75490], Loss: 7.7588, Time: 2.57 seconds, Average Loss: 8.4958, Learning Rate: 7e-06\n",
            "Step [2020/75490], Loss: 7.5318, Time: 1.71 seconds, Average Loss: 8.4897, Learning Rate: 7e-06\n",
            "Step [2030/75490], Loss: 6.2960, Time: 1.82 seconds, Average Loss: 8.4849, Learning Rate: 7e-06\n",
            "Step [2040/75490], Loss: 6.4120, Time: 1.47 seconds, Average Loss: 8.4798, Learning Rate: 7e-06\n",
            "Step [2050/75490], Loss: 6.8419, Time: 1.64 seconds, Average Loss: 8.4746, Learning Rate: 7e-06\n",
            "Step [2060/75490], Loss: 6.9485, Time: 2.04 seconds, Average Loss: 8.4696, Learning Rate: 7e-06\n",
            "Step [2070/75490], Loss: 8.2241, Time: 1.43 seconds, Average Loss: 8.4646, Learning Rate: 7e-06\n",
            "Step [2080/75490], Loss: 6.4508, Time: 1.53 seconds, Average Loss: 8.4599, Learning Rate: 7e-06\n",
            "Step [2090/75490], Loss: 6.4259, Time: 1.76 seconds, Average Loss: 8.4553, Learning Rate: 7e-06\n",
            "Step [2100/75490], Loss: 7.5280, Time: 1.66 seconds, Average Loss: 8.4512, Learning Rate: 7e-06\n",
            "Step [2110/75490], Loss: 7.1705, Time: 1.82 seconds, Average Loss: 8.4459, Learning Rate: 7e-06\n",
            "Step [2120/75490], Loss: 8.0132, Time: 1.97 seconds, Average Loss: 8.4419, Learning Rate: 7e-06\n",
            "Step [2130/75490], Loss: 8.2740, Time: 1.32 seconds, Average Loss: 8.4378, Learning Rate: 7e-06\n",
            "Step [2140/75490], Loss: 6.7268, Time: 2.04 seconds, Average Loss: 8.4337, Learning Rate: 7e-06\n",
            "Step [2150/75490], Loss: 6.7088, Time: 1.41 seconds, Average Loss: 8.4295, Learning Rate: 7e-06\n",
            "Step [2160/75490], Loss: 8.0341, Time: 1.65 seconds, Average Loss: 8.4256, Learning Rate: 7e-06\n",
            "Step [2170/75490], Loss: 9.8128, Time: 1.43 seconds, Average Loss: 8.4230, Learning Rate: 7e-06\n",
            "Step [2180/75490], Loss: 7.6047, Time: 1.73 seconds, Average Loss: 8.4192, Learning Rate: 7e-06\n",
            "Step [2190/75490], Loss: 7.3732, Time: 1.77 seconds, Average Loss: 8.4147, Learning Rate: 7e-06\n",
            "Step [2200/75490], Loss: 7.7941, Time: 1.89 seconds, Average Loss: 8.4092, Learning Rate: 7e-06\n",
            "Step [2210/75490], Loss: 9.0438, Time: 1.80 seconds, Average Loss: 8.4046, Learning Rate: 7e-06\n",
            "Step [2220/75490], Loss: 6.5404, Time: 1.43 seconds, Average Loss: 8.4005, Learning Rate: 7e-06\n",
            "Step [2230/75490], Loss: 6.5620, Time: 1.85 seconds, Average Loss: 8.3950, Learning Rate: 7e-06\n",
            "Step [2240/75490], Loss: 6.6673, Time: 1.75 seconds, Average Loss: 8.3898, Learning Rate: 7e-06\n",
            "Step [2250/75490], Loss: 8.0599, Time: 1.56 seconds, Average Loss: 8.3873, Learning Rate: 7e-06\n",
            "Step [2260/75490], Loss: 6.9669, Time: 1.92 seconds, Average Loss: 8.3819, Learning Rate: 7e-06\n",
            "Step [2270/75490], Loss: 6.4696, Time: 1.68 seconds, Average Loss: 8.3779, Learning Rate: 7e-06\n",
            "Step [2280/75490], Loss: 7.1321, Time: 1.77 seconds, Average Loss: 8.3726, Learning Rate: 7e-06\n",
            "Step [2290/75490], Loss: 7.1710, Time: 1.44 seconds, Average Loss: 8.3696, Learning Rate: 7e-06\n",
            "Step [2300/75490], Loss: 7.3049, Time: 2.08 seconds, Average Loss: 8.3638, Learning Rate: 7e-06\n",
            "Step [2310/75490], Loss: 7.5555, Time: 1.69 seconds, Average Loss: 8.3593, Learning Rate: 7e-06\n",
            "Step [2320/75490], Loss: 8.5547, Time: 1.35 seconds, Average Loss: 8.3555, Learning Rate: 7e-06\n",
            "Step [2330/75490], Loss: 6.8150, Time: 1.70 seconds, Average Loss: 8.3502, Learning Rate: 7e-06\n",
            "Step [2340/75490], Loss: 7.4667, Time: 1.88 seconds, Average Loss: 8.3460, Learning Rate: 7e-06\n",
            "Step [2350/75490], Loss: 7.8767, Time: 1.96 seconds, Average Loss: 8.3407, Learning Rate: 7e-06\n",
            "Step [2360/75490], Loss: 6.5248, Time: 1.73 seconds, Average Loss: 8.3375, Learning Rate: 7e-06\n",
            "Step [2370/75490], Loss: 7.9281, Time: 1.60 seconds, Average Loss: 8.3336, Learning Rate: 7e-06\n",
            "Step [2380/75490], Loss: 5.6173, Time: 1.58 seconds, Average Loss: 8.3282, Learning Rate: 7e-06\n",
            "Step [2390/75490], Loss: 7.4340, Time: 2.07 seconds, Average Loss: 8.3252, Learning Rate: 7e-06\n",
            "Step [2400/75490], Loss: 6.8714, Time: 1.54 seconds, Average Loss: 8.3209, Learning Rate: 7e-06\n",
            "Step [2410/75490], Loss: 7.6055, Time: 1.53 seconds, Average Loss: 8.3172, Learning Rate: 7e-06\n",
            "Step [2420/75490], Loss: 7.1882, Time: 1.53 seconds, Average Loss: 8.3131, Learning Rate: 7e-06\n",
            "Step [2430/75490], Loss: 7.0743, Time: 2.01 seconds, Average Loss: 8.3089, Learning Rate: 7e-06\n",
            "Step [2440/75490], Loss: 7.6518, Time: 2.01 seconds, Average Loss: 8.3056, Learning Rate: 7e-06\n",
            "Step [2450/75490], Loss: 6.5827, Time: 2.13 seconds, Average Loss: 8.3001, Learning Rate: 7e-06\n",
            "Step [2460/75490], Loss: 7.7058, Time: 1.23 seconds, Average Loss: 8.2970, Learning Rate: 7e-06\n",
            "Step [2470/75490], Loss: 7.2232, Time: 1.75 seconds, Average Loss: 8.2933, Learning Rate: 7e-06\n",
            "Step [2480/75490], Loss: 7.0233, Time: 1.76 seconds, Average Loss: 8.2886, Learning Rate: 7e-06\n",
            "Step [2490/75490], Loss: 6.9528, Time: 1.38 seconds, Average Loss: 8.2853, Learning Rate: 7e-06\n",
            "Step [2500/75490], Loss: 7.4106, Time: 1.70 seconds, Average Loss: 8.2812, Learning Rate: 7e-06\n",
            "Step [2510/75490], Loss: 6.8371, Time: 1.59 seconds, Average Loss: 8.2754, Learning Rate: 7e-06\n",
            "Step [2520/75490], Loss: 6.4263, Time: 1.66 seconds, Average Loss: 8.2715, Learning Rate: 7e-06\n",
            "Step [2530/75490], Loss: 8.0354, Time: 1.55 seconds, Average Loss: 8.2695, Learning Rate: 7e-06\n",
            "Step [2540/75490], Loss: 6.9559, Time: 2.16 seconds, Average Loss: 8.2656, Learning Rate: 7e-06\n",
            "Step [2550/75490], Loss: 7.0814, Time: 1.69 seconds, Average Loss: 8.2624, Learning Rate: 7e-06\n",
            "Step [2560/75490], Loss: 8.4341, Time: 2.25 seconds, Average Loss: 8.2594, Learning Rate: 7e-06\n",
            "Step [2570/75490], Loss: 7.3922, Time: 1.99 seconds, Average Loss: 8.2551, Learning Rate: 7e-06\n",
            "Step [2580/75490], Loss: 7.1183, Time: 2.01 seconds, Average Loss: 8.2527, Learning Rate: 7e-06\n",
            "Step [2590/75490], Loss: 7.2491, Time: 1.55 seconds, Average Loss: 8.2478, Learning Rate: 7e-06\n",
            "Step [2600/75490], Loss: 8.0216, Time: 1.58 seconds, Average Loss: 8.2450, Learning Rate: 7e-06\n",
            "Step [2610/75490], Loss: 6.8915, Time: 1.64 seconds, Average Loss: 8.2424, Learning Rate: 7e-06\n",
            "Step [2620/75490], Loss: 7.8464, Time: 1.54 seconds, Average Loss: 8.2401, Learning Rate: 7e-06\n",
            "Step [2630/75490], Loss: 6.7407, Time: 2.45 seconds, Average Loss: 8.2379, Learning Rate: 7e-06\n",
            "Step [2640/75490], Loss: 7.1316, Time: 1.37 seconds, Average Loss: 8.2342, Learning Rate: 7e-06\n",
            "Step [2650/75490], Loss: 7.8069, Time: 2.48 seconds, Average Loss: 8.2309, Learning Rate: 7e-06\n",
            "Step [2660/75490], Loss: 7.0134, Time: 1.53 seconds, Average Loss: 8.2268, Learning Rate: 7e-06\n",
            "Step [2670/75490], Loss: 6.3858, Time: 1.54 seconds, Average Loss: 8.2224, Learning Rate: 7e-06\n",
            "Step [2680/75490], Loss: 9.0414, Time: 2.28 seconds, Average Loss: 8.2193, Learning Rate: 7e-06\n",
            "Step [2690/75490], Loss: 8.0289, Time: 1.73 seconds, Average Loss: 8.2162, Learning Rate: 7e-06\n",
            "Step [2700/75490], Loss: 8.2793, Time: 1.65 seconds, Average Loss: 8.2139, Learning Rate: 7e-06\n",
            "Step [2710/75490], Loss: 7.3463, Time: 1.59 seconds, Average Loss: 8.2104, Learning Rate: 7e-06\n",
            "Step [2720/75490], Loss: 7.1720, Time: 2.77 seconds, Average Loss: 8.2095, Learning Rate: 7e-06\n",
            "Step [2730/75490], Loss: 7.7381, Time: 1.40 seconds, Average Loss: 8.2059, Learning Rate: 7e-06\n",
            "Step [2740/75490], Loss: 7.3635, Time: 1.75 seconds, Average Loss: 8.2028, Learning Rate: 7e-06\n",
            "Step [2750/75490], Loss: 5.2451, Time: 1.49 seconds, Average Loss: 8.1996, Learning Rate: 7e-06\n",
            "Step [2760/75490], Loss: 7.2889, Time: 1.98 seconds, Average Loss: 8.1964, Learning Rate: 7e-06\n",
            "Step [2770/75490], Loss: 7.5529, Time: 1.94 seconds, Average Loss: 8.1933, Learning Rate: 7e-06\n",
            "Step [2780/75490], Loss: 8.0072, Time: 1.56 seconds, Average Loss: 8.1900, Learning Rate: 7e-06\n",
            "Step [2790/75490], Loss: 6.7300, Time: 1.52 seconds, Average Loss: 8.1866, Learning Rate: 7e-06\n",
            "Step [2800/75490], Loss: 7.5890, Time: 1.88 seconds, Average Loss: 8.1846, Learning Rate: 7e-06\n",
            "Step [2810/75490], Loss: 6.6237, Time: 1.46 seconds, Average Loss: 8.1823, Learning Rate: 7e-06\n",
            "Step [2820/75490], Loss: 5.9862, Time: 1.71 seconds, Average Loss: 8.1784, Learning Rate: 7e-06\n",
            "Step [2830/75490], Loss: 9.0228, Time: 1.62 seconds, Average Loss: 8.1765, Learning Rate: 7e-06\n",
            "Step [2840/75490], Loss: 6.9562, Time: 1.79 seconds, Average Loss: 8.1738, Learning Rate: 7e-06\n",
            "Step [2850/75490], Loss: 6.8190, Time: 1.73 seconds, Average Loss: 8.1692, Learning Rate: 7e-06\n",
            "Step [2860/75490], Loss: 6.1279, Time: 1.80 seconds, Average Loss: 8.1651, Learning Rate: 7e-06\n",
            "Step [2870/75490], Loss: 5.7713, Time: 1.89 seconds, Average Loss: 8.1620, Learning Rate: 7e-06\n",
            "Step [2880/75490], Loss: 6.5110, Time: 1.57 seconds, Average Loss: 8.1587, Learning Rate: 7e-06\n",
            "Step [2890/75490], Loss: 7.5740, Time: 1.56 seconds, Average Loss: 8.1552, Learning Rate: 7e-06\n",
            "Step [2900/75490], Loss: 7.0867, Time: 1.98 seconds, Average Loss: 8.1527, Learning Rate: 7e-06\n",
            "Step [2910/75490], Loss: 7.4610, Time: 1.53 seconds, Average Loss: 8.1506, Learning Rate: 7e-06\n",
            "Step [2920/75490], Loss: 7.1152, Time: 1.71 seconds, Average Loss: 8.1481, Learning Rate: 7e-06\n",
            "Step [2930/75490], Loss: 7.4713, Time: 1.57 seconds, Average Loss: 8.1447, Learning Rate: 7e-06\n",
            "Step [2940/75490], Loss: 6.2652, Time: 1.56 seconds, Average Loss: 8.1430, Learning Rate: 7e-06\n",
            "Step [2950/75490], Loss: 7.4826, Time: 1.71 seconds, Average Loss: 8.1391, Learning Rate: 7e-06\n",
            "Step [2960/75490], Loss: 7.5371, Time: 1.49 seconds, Average Loss: 8.1355, Learning Rate: 7e-06\n",
            "Step [2970/75490], Loss: 7.0804, Time: 1.87 seconds, Average Loss: 8.1330, Learning Rate: 7e-06\n",
            "Step [2980/75490], Loss: 6.4172, Time: 1.45 seconds, Average Loss: 8.1293, Learning Rate: 7e-06\n",
            "Step [2990/75490], Loss: 7.5310, Time: 1.54 seconds, Average Loss: 8.1258, Learning Rate: 7e-06\n",
            "Step [3000/75490], Loss: 6.8626, Time: 1.55 seconds, Average Loss: 8.1226, Learning Rate: 7e-06\n",
            "Step [3010/75490], Loss: 6.2928, Time: 1.78 seconds, Average Loss: 8.1205, Learning Rate: 7e-06\n",
            "Step [3020/75490], Loss: 7.4407, Time: 1.42 seconds, Average Loss: 8.1183, Learning Rate: 7e-06\n",
            "Step [3030/75490], Loss: 8.4639, Time: 1.75 seconds, Average Loss: 8.1150, Learning Rate: 7e-06\n",
            "Step [3040/75490], Loss: 7.3271, Time: 1.42 seconds, Average Loss: 8.1119, Learning Rate: 7e-06\n",
            "Step [3050/75490], Loss: 8.4740, Time: 1.80 seconds, Average Loss: 8.1096, Learning Rate: 7e-06\n",
            "Step [3060/75490], Loss: 7.6815, Time: 1.71 seconds, Average Loss: 8.1073, Learning Rate: 7e-06\n",
            "Step [3070/75490], Loss: 7.0377, Time: 1.66 seconds, Average Loss: 8.1047, Learning Rate: 7e-06\n",
            "Step [3080/75490], Loss: 7.2812, Time: 1.79 seconds, Average Loss: 8.1028, Learning Rate: 7e-06\n",
            "Step [3090/75490], Loss: 6.9115, Time: 2.35 seconds, Average Loss: 8.0990, Learning Rate: 7e-06\n",
            "Step [3100/75490], Loss: 6.4935, Time: 2.16 seconds, Average Loss: 8.0948, Learning Rate: 7e-06\n",
            "Step [3110/75490], Loss: 6.8693, Time: 1.61 seconds, Average Loss: 8.0925, Learning Rate: 7e-06\n",
            "Step [3120/75490], Loss: 6.1125, Time: 1.75 seconds, Average Loss: 8.0894, Learning Rate: 7e-06\n",
            "Step [3130/75490], Loss: 7.3381, Time: 2.08 seconds, Average Loss: 8.0877, Learning Rate: 7e-06\n",
            "Step [3140/75490], Loss: 8.0018, Time: 1.68 seconds, Average Loss: 8.0847, Learning Rate: 7e-06\n",
            "Step [3150/75490], Loss: 6.8024, Time: 1.77 seconds, Average Loss: 8.0830, Learning Rate: 7e-06\n",
            "Step [3160/75490], Loss: 6.5337, Time: 1.76 seconds, Average Loss: 8.0805, Learning Rate: 7e-06\n",
            "Step [3170/75490], Loss: 7.1986, Time: 1.59 seconds, Average Loss: 8.0785, Learning Rate: 7e-06\n",
            "Step [3180/75490], Loss: 7.5150, Time: 1.54 seconds, Average Loss: 8.0760, Learning Rate: 7e-06\n",
            "Step [3190/75490], Loss: 9.0545, Time: 1.56 seconds, Average Loss: 8.0734, Learning Rate: 7e-06\n",
            "Step [3200/75490], Loss: 6.5032, Time: 1.25 seconds, Average Loss: 8.0706, Learning Rate: 7e-06\n",
            "Step [3210/75490], Loss: 7.9679, Time: 1.85 seconds, Average Loss: 8.0687, Learning Rate: 7e-06\n",
            "Step [3220/75490], Loss: 7.1766, Time: 1.06 seconds, Average Loss: 8.0670, Learning Rate: 7e-06\n",
            "Step [3230/75490], Loss: 7.1704, Time: 1.75 seconds, Average Loss: 8.0639, Learning Rate: 7e-06\n",
            "Step [3240/75490], Loss: 7.6391, Time: 2.09 seconds, Average Loss: 8.0635, Learning Rate: 7e-06\n",
            "Step [3250/75490], Loss: 7.9282, Time: 1.79 seconds, Average Loss: 8.0623, Learning Rate: 7e-06\n",
            "Step [3260/75490], Loss: 6.4784, Time: 1.44 seconds, Average Loss: 8.0595, Learning Rate: 7e-06\n",
            "Step [3270/75490], Loss: 6.7760, Time: 1.44 seconds, Average Loss: 8.0565, Learning Rate: 7e-06\n",
            "Step [3280/75490], Loss: 7.8356, Time: 1.24 seconds, Average Loss: 8.0547, Learning Rate: 7e-06\n",
            "Step [3290/75490], Loss: 7.8085, Time: 1.45 seconds, Average Loss: 8.0533, Learning Rate: 7e-06\n",
            "Step [3300/75490], Loss: 7.5988, Time: 2.36 seconds, Average Loss: 8.0510, Learning Rate: 7e-06\n",
            "Step [3310/75490], Loss: 6.8970, Time: 1.42 seconds, Average Loss: 8.0483, Learning Rate: 7e-06\n",
            "Step [3320/75490], Loss: 8.2063, Time: 1.76 seconds, Average Loss: 8.0464, Learning Rate: 7e-06\n",
            "Step [3330/75490], Loss: 7.8876, Time: 1.65 seconds, Average Loss: 8.0443, Learning Rate: 7e-06\n",
            "Step [3340/75490], Loss: 7.0669, Time: 2.11 seconds, Average Loss: 8.0419, Learning Rate: 7e-06\n",
            "Step [3350/75490], Loss: 7.1627, Time: 1.84 seconds, Average Loss: 8.0404, Learning Rate: 7e-06\n",
            "Step [3360/75490], Loss: 6.4873, Time: 1.83 seconds, Average Loss: 8.0385, Learning Rate: 7e-06\n",
            "Step [3370/75490], Loss: 7.1278, Time: 1.83 seconds, Average Loss: 8.0364, Learning Rate: 7e-06\n",
            "Step [3380/75490], Loss: 6.5601, Time: 1.67 seconds, Average Loss: 8.0333, Learning Rate: 7e-06\n",
            "Step [3390/75490], Loss: 7.4635, Time: 1.68 seconds, Average Loss: 8.0309, Learning Rate: 7e-06\n",
            "Step [3400/75490], Loss: 7.2684, Time: 1.93 seconds, Average Loss: 8.0287, Learning Rate: 7e-06\n",
            "Step [3410/75490], Loss: 7.4729, Time: 1.33 seconds, Average Loss: 8.0264, Learning Rate: 7e-06\n",
            "Step [3420/75490], Loss: 6.7985, Time: 1.74 seconds, Average Loss: 8.0228, Learning Rate: 7e-06\n",
            "Step [3430/75490], Loss: 6.8423, Time: 1.62 seconds, Average Loss: 8.0199, Learning Rate: 7e-06\n",
            "Step [3440/75490], Loss: 6.7245, Time: 1.36 seconds, Average Loss: 8.0180, Learning Rate: 7e-06\n",
            "Step [3450/75490], Loss: 6.7117, Time: 1.91 seconds, Average Loss: 8.0148, Learning Rate: 7e-06\n",
            "Step [3460/75490], Loss: 7.6739, Time: 1.56 seconds, Average Loss: 8.0141, Learning Rate: 7e-06\n",
            "Step [3470/75490], Loss: 6.9729, Time: 1.71 seconds, Average Loss: 8.0118, Learning Rate: 7e-06\n",
            "Step [3480/75490], Loss: 6.9842, Time: 1.69 seconds, Average Loss: 8.0088, Learning Rate: 7e-06\n",
            "Step [3490/75490], Loss: 7.9196, Time: 1.52 seconds, Average Loss: 8.0072, Learning Rate: 7e-06\n",
            "Step [3500/75490], Loss: 7.1281, Time: 1.77 seconds, Average Loss: 8.0056, Learning Rate: 7e-06\n",
            "Step [3510/75490], Loss: 7.0653, Time: 1.32 seconds, Average Loss: 8.0058, Learning Rate: 7e-06\n",
            "Step [3520/75490], Loss: 6.0140, Time: 1.92 seconds, Average Loss: 8.0031, Learning Rate: 7e-06\n",
            "Step [3530/75490], Loss: 6.2785, Time: 1.76 seconds, Average Loss: 8.0006, Learning Rate: 7e-06\n",
            "Step [3540/75490], Loss: 7.3738, Time: 1.65 seconds, Average Loss: 8.0001, Learning Rate: 7e-06\n",
            "Step [3550/75490], Loss: 7.4340, Time: 1.68 seconds, Average Loss: 7.9983, Learning Rate: 7e-06\n",
            "Step [3560/75490], Loss: 6.8193, Time: 1.36 seconds, Average Loss: 7.9955, Learning Rate: 7e-06\n",
            "Step [3570/75490], Loss: 8.0947, Time: 2.04 seconds, Average Loss: 7.9921, Learning Rate: 7e-06\n",
            "Step [3580/75490], Loss: 5.5065, Time: 1.80 seconds, Average Loss: 7.9906, Learning Rate: 7e-06\n",
            "Step [3590/75490], Loss: 5.4297, Time: 1.87 seconds, Average Loss: 7.9868, Learning Rate: 7e-06\n",
            "Step [3600/75490], Loss: 7.6133, Time: 1.45 seconds, Average Loss: 7.9838, Learning Rate: 7e-06\n",
            "Step [3610/75490], Loss: 7.3140, Time: 2.02 seconds, Average Loss: 7.9812, Learning Rate: 7e-06\n",
            "Step [3620/75490], Loss: 6.9855, Time: 2.17 seconds, Average Loss: 7.9794, Learning Rate: 7e-06\n",
            "Step [3630/75490], Loss: 8.8225, Time: 1.42 seconds, Average Loss: 7.9763, Learning Rate: 7e-06\n",
            "Step [3640/75490], Loss: 8.1940, Time: 1.44 seconds, Average Loss: 7.9741, Learning Rate: 7e-06\n",
            "Step [3650/75490], Loss: 7.3956, Time: 1.88 seconds, Average Loss: 7.9728, Learning Rate: 7e-06\n",
            "Step [3660/75490], Loss: 8.2121, Time: 1.57 seconds, Average Loss: 7.9716, Learning Rate: 7e-06\n",
            "Step [3670/75490], Loss: 6.7778, Time: 1.59 seconds, Average Loss: 7.9691, Learning Rate: 7e-06\n",
            "Step [3680/75490], Loss: 7.1039, Time: 1.57 seconds, Average Loss: 7.9673, Learning Rate: 7e-06\n",
            "Step [3690/75490], Loss: 7.0058, Time: 1.41 seconds, Average Loss: 7.9646, Learning Rate: 7e-06\n",
            "Step [3700/75490], Loss: 7.6605, Time: 1.37 seconds, Average Loss: 7.9630, Learning Rate: 7e-06\n",
            "Step [3710/75490], Loss: 8.0311, Time: 1.70 seconds, Average Loss: 7.9616, Learning Rate: 7e-06\n",
            "Step [3720/75490], Loss: 6.6854, Time: 1.39 seconds, Average Loss: 7.9592, Learning Rate: 7e-06\n",
            "Step [3730/75490], Loss: 6.8547, Time: 1.68 seconds, Average Loss: 7.9571, Learning Rate: 7e-06\n",
            "Step [3740/75490], Loss: 8.7034, Time: 1.56 seconds, Average Loss: 7.9553, Learning Rate: 7e-06\n",
            "Step [3750/75490], Loss: 6.6222, Time: 2.31 seconds, Average Loss: 7.9535, Learning Rate: 7e-06\n",
            "Step [3760/75490], Loss: 7.1183, Time: 1.93 seconds, Average Loss: 7.9499, Learning Rate: 7e-06\n",
            "Step [3770/75490], Loss: 5.1665, Time: 1.34 seconds, Average Loss: 7.9471, Learning Rate: 7e-06\n",
            "Step [3780/75490], Loss: 7.1702, Time: 1.76 seconds, Average Loss: 7.9465, Learning Rate: 7e-06\n",
            "Step [3790/75490], Loss: 7.0296, Time: 1.53 seconds, Average Loss: 7.9438, Learning Rate: 7e-06\n",
            "Step [3800/75490], Loss: 6.6551, Time: 1.72 seconds, Average Loss: 7.9408, Learning Rate: 7e-06\n",
            "Step [3810/75490], Loss: 7.3134, Time: 1.55 seconds, Average Loss: 7.9380, Learning Rate: 7e-06\n",
            "Step [3820/75490], Loss: 7.0970, Time: 2.00 seconds, Average Loss: 7.9361, Learning Rate: 7e-06\n",
            "Step [3830/75490], Loss: 8.2858, Time: 2.03 seconds, Average Loss: 7.9345, Learning Rate: 7e-06\n",
            "Step [3840/75490], Loss: 7.1985, Time: 1.89 seconds, Average Loss: 7.9333, Learning Rate: 7e-06\n",
            "Step [3850/75490], Loss: 6.5660, Time: 1.64 seconds, Average Loss: 7.9330, Learning Rate: 7e-06\n",
            "Step [3860/75490], Loss: 8.0076, Time: 1.85 seconds, Average Loss: 7.9321, Learning Rate: 7e-06\n",
            "Step [3870/75490], Loss: 6.6345, Time: 1.81 seconds, Average Loss: 7.9297, Learning Rate: 7e-06\n",
            "Step [3880/75490], Loss: 7.5083, Time: 1.88 seconds, Average Loss: 7.9285, Learning Rate: 7e-06\n",
            "Step [3890/75490], Loss: 7.5464, Time: 2.00 seconds, Average Loss: 7.9259, Learning Rate: 7e-06\n",
            "Step [3900/75490], Loss: 8.2176, Time: 1.72 seconds, Average Loss: 7.9246, Learning Rate: 7e-06\n",
            "Step [3910/75490], Loss: 6.4336, Time: 1.84 seconds, Average Loss: 7.9233, Learning Rate: 7e-06\n",
            "Step [3920/75490], Loss: 7.3307, Time: 1.60 seconds, Average Loss: 7.9222, Learning Rate: 7e-06\n",
            "Step [3930/75490], Loss: 7.6060, Time: 1.71 seconds, Average Loss: 7.9206, Learning Rate: 7e-06\n",
            "Step [3940/75490], Loss: 7.5404, Time: 2.22 seconds, Average Loss: 7.9192, Learning Rate: 7e-06\n",
            "Step [3950/75490], Loss: 7.0436, Time: 1.92 seconds, Average Loss: 7.9180, Learning Rate: 7e-06\n",
            "Step [3960/75490], Loss: 7.1308, Time: 1.84 seconds, Average Loss: 7.9152, Learning Rate: 7e-06\n",
            "Step [3970/75490], Loss: 7.2151, Time: 1.62 seconds, Average Loss: 7.9130, Learning Rate: 7e-06\n",
            "Step [3980/75490], Loss: 6.4930, Time: 1.87 seconds, Average Loss: 7.9112, Learning Rate: 7e-06\n",
            "Step [3990/75490], Loss: 7.2017, Time: 1.59 seconds, Average Loss: 7.9091, Learning Rate: 7e-06\n",
            "Step [4000/75490], Loss: 7.3022, Time: 1.27 seconds, Average Loss: 7.9078, Learning Rate: 7e-06\n",
            "Step [4010/75490], Loss: 6.7990, Time: 1.62 seconds, Average Loss: 7.9054, Learning Rate: 7e-06\n",
            "Step [4020/75490], Loss: 5.7684, Time: 1.53 seconds, Average Loss: 7.9033, Learning Rate: 7e-06\n",
            "Step [4030/75490], Loss: 7.5009, Time: 1.41 seconds, Average Loss: 7.9018, Learning Rate: 7e-06\n",
            "Step [4040/75490], Loss: 8.3331, Time: 1.65 seconds, Average Loss: 7.9005, Learning Rate: 7e-06\n",
            "Step [4050/75490], Loss: 6.5607, Time: 1.72 seconds, Average Loss: 7.8985, Learning Rate: 7e-06\n",
            "Step [4060/75490], Loss: 7.6258, Time: 2.08 seconds, Average Loss: 7.8967, Learning Rate: 7e-06\n",
            "Step [4070/75490], Loss: 7.5779, Time: 1.74 seconds, Average Loss: 7.8959, Learning Rate: 7e-06\n",
            "Step [4080/75490], Loss: 7.2261, Time: 1.88 seconds, Average Loss: 7.8931, Learning Rate: 7e-06\n",
            "Step [4090/75490], Loss: 6.8492, Time: 1.55 seconds, Average Loss: 7.8913, Learning Rate: 7e-06\n",
            "Step [4100/75490], Loss: 7.2856, Time: 1.83 seconds, Average Loss: 7.8889, Learning Rate: 7e-06\n",
            "Step [4110/75490], Loss: 6.0982, Time: 1.70 seconds, Average Loss: 7.8876, Learning Rate: 7e-06\n",
            "Step [4120/75490], Loss: 7.9379, Time: 1.62 seconds, Average Loss: 7.8858, Learning Rate: 7e-06\n",
            "Step [4130/75490], Loss: 6.7763, Time: 1.63 seconds, Average Loss: 7.8837, Learning Rate: 7e-06\n",
            "Step [4140/75490], Loss: 5.5414, Time: 1.45 seconds, Average Loss: 7.8819, Learning Rate: 7e-06\n",
            "Step [4150/75490], Loss: 7.4814, Time: 1.60 seconds, Average Loss: 7.8806, Learning Rate: 7e-06\n",
            "Step [4160/75490], Loss: 7.6720, Time: 1.75 seconds, Average Loss: 7.8790, Learning Rate: 7e-06\n",
            "Step [4170/75490], Loss: 6.8768, Time: 1.80 seconds, Average Loss: 7.8772, Learning Rate: 7e-06\n",
            "Step [4180/75490], Loss: 7.0850, Time: 1.70 seconds, Average Loss: 7.8748, Learning Rate: 7e-06\n",
            "Step [4190/75490], Loss: 6.9883, Time: 1.90 seconds, Average Loss: 7.8737, Learning Rate: 7e-06\n",
            "Step [4200/75490], Loss: 7.3509, Time: 1.39 seconds, Average Loss: 7.8718, Learning Rate: 7e-06\n",
            "Step [4210/75490], Loss: 7.0725, Time: 1.55 seconds, Average Loss: 7.8703, Learning Rate: 7e-06\n",
            "Step [4220/75490], Loss: 8.0723, Time: 1.82 seconds, Average Loss: 7.8675, Learning Rate: 7e-06\n",
            "Step [4230/75490], Loss: 7.8179, Time: 2.31 seconds, Average Loss: 7.8667, Learning Rate: 7e-06\n",
            "Step [4240/75490], Loss: 7.1142, Time: 1.55 seconds, Average Loss: 7.8650, Learning Rate: 7e-06\n",
            "Step [4250/75490], Loss: 5.8183, Time: 1.35 seconds, Average Loss: 7.8631, Learning Rate: 7e-06\n",
            "Step [4260/75490], Loss: 7.4981, Time: 1.55 seconds, Average Loss: 7.8609, Learning Rate: 7e-06\n",
            "Step [4270/75490], Loss: 8.2543, Time: 1.70 seconds, Average Loss: 7.8591, Learning Rate: 7e-06\n",
            "Step [4280/75490], Loss: 8.0512, Time: 1.53 seconds, Average Loss: 7.8573, Learning Rate: 7e-06\n",
            "Step [4290/75490], Loss: 6.7417, Time: 1.56 seconds, Average Loss: 7.8554, Learning Rate: 7e-06\n",
            "Step [4300/75490], Loss: 5.8148, Time: 1.83 seconds, Average Loss: 7.8534, Learning Rate: 7e-06\n",
            "Step [4310/75490], Loss: 6.8990, Time: 1.75 seconds, Average Loss: 7.8521, Learning Rate: 7e-06\n",
            "Step [4320/75490], Loss: 7.1017, Time: 1.30 seconds, Average Loss: 7.8518, Learning Rate: 7e-06\n",
            "Step [4330/75490], Loss: 7.3714, Time: 2.08 seconds, Average Loss: 7.8504, Learning Rate: 7e-06\n",
            "Step [4340/75490], Loss: 7.8529, Time: 1.22 seconds, Average Loss: 7.8494, Learning Rate: 7e-06\n",
            "Step [4350/75490], Loss: 8.3124, Time: 1.70 seconds, Average Loss: 7.8481, Learning Rate: 7e-06\n",
            "Step [4360/75490], Loss: 6.9674, Time: 1.37 seconds, Average Loss: 7.8464, Learning Rate: 7e-06\n",
            "Step [4370/75490], Loss: 7.7406, Time: 1.87 seconds, Average Loss: 7.8445, Learning Rate: 7e-06\n",
            "Step [4380/75490], Loss: 7.4770, Time: 1.95 seconds, Average Loss: 7.8437, Learning Rate: 7e-06\n",
            "Step [4390/75490], Loss: 7.8118, Time: 1.87 seconds, Average Loss: 7.8424, Learning Rate: 7e-06\n",
            "Step [4400/75490], Loss: 8.3308, Time: 1.87 seconds, Average Loss: 7.8412, Learning Rate: 7e-06\n",
            "Step [4410/75490], Loss: 8.2066, Time: 1.76 seconds, Average Loss: 7.8397, Learning Rate: 7e-06\n",
            "Step [4420/75490], Loss: 8.0036, Time: 1.69 seconds, Average Loss: 7.8380, Learning Rate: 7e-06\n",
            "Step [4430/75490], Loss: 6.6787, Time: 1.67 seconds, Average Loss: 7.8362, Learning Rate: 7e-06\n",
            "Step [4440/75490], Loss: 6.5707, Time: 1.70 seconds, Average Loss: 7.8335, Learning Rate: 7e-06\n",
            "Step [4450/75490], Loss: 7.6230, Time: 1.57 seconds, Average Loss: 7.8319, Learning Rate: 7e-06\n",
            "Step [4460/75490], Loss: 6.6224, Time: 2.12 seconds, Average Loss: 7.8299, Learning Rate: 7e-06\n",
            "Step [4470/75490], Loss: 8.4823, Time: 1.83 seconds, Average Loss: 7.8274, Learning Rate: 7e-06\n",
            "Step [4480/75490], Loss: 7.3575, Time: 1.60 seconds, Average Loss: 7.8260, Learning Rate: 7e-06\n",
            "Step [4490/75490], Loss: 7.3386, Time: 2.16 seconds, Average Loss: 7.8244, Learning Rate: 7e-06\n",
            "Step [4500/75490], Loss: 8.1685, Time: 1.52 seconds, Average Loss: 7.8234, Learning Rate: 7e-06\n",
            "Step [4510/75490], Loss: 7.7905, Time: 1.58 seconds, Average Loss: 7.8216, Learning Rate: 7e-06\n",
            "Step [4520/75490], Loss: 10.7342, Time: 2.14 seconds, Average Loss: 7.8209, Learning Rate: 7e-06\n",
            "Step [4530/75490], Loss: 6.9835, Time: 1.35 seconds, Average Loss: 7.8187, Learning Rate: 7e-06\n",
            "Step [4540/75490], Loss: 7.7271, Time: 1.67 seconds, Average Loss: 7.8167, Learning Rate: 7e-06\n",
            "Step [4550/75490], Loss: 7.1396, Time: 1.66 seconds, Average Loss: 7.8146, Learning Rate: 7e-06\n",
            "Step [4560/75490], Loss: 7.9601, Time: 1.62 seconds, Average Loss: 7.8136, Learning Rate: 7e-06\n",
            "Step [4570/75490], Loss: 7.2425, Time: 1.90 seconds, Average Loss: 7.8118, Learning Rate: 7e-06\n",
            "Step [4580/75490], Loss: 7.1632, Time: 1.54 seconds, Average Loss: 7.8099, Learning Rate: 7e-06\n",
            "Step [4590/75490], Loss: 7.6421, Time: 1.43 seconds, Average Loss: 7.8092, Learning Rate: 7e-06\n",
            "Step [4600/75490], Loss: 6.9418, Time: 1.72 seconds, Average Loss: 7.8077, Learning Rate: 7e-06\n",
            "Step [4610/75490], Loss: 6.4375, Time: 1.78 seconds, Average Loss: 7.8057, Learning Rate: 7e-06\n",
            "Step [4620/75490], Loss: 7.3164, Time: 1.41 seconds, Average Loss: 7.8046, Learning Rate: 7e-06\n",
            "Step [4630/75490], Loss: 6.5650, Time: 1.71 seconds, Average Loss: 7.8033, Learning Rate: 7e-06\n",
            "Step [4640/75490], Loss: 7.1687, Time: 1.84 seconds, Average Loss: 7.8020, Learning Rate: 7e-06\n",
            "Step [4650/75490], Loss: 6.3872, Time: 1.61 seconds, Average Loss: 7.7994, Learning Rate: 7e-06\n",
            "Step [4660/75490], Loss: 7.5100, Time: 1.89 seconds, Average Loss: 7.7980, Learning Rate: 7e-06\n",
            "Step [4670/75490], Loss: 9.0657, Time: 1.73 seconds, Average Loss: 7.7977, Learning Rate: 7e-06\n",
            "Step [4680/75490], Loss: 8.6831, Time: 1.70 seconds, Average Loss: 7.7972, Learning Rate: 7e-06\n",
            "Step [4690/75490], Loss: 7.2584, Time: 1.68 seconds, Average Loss: 7.7956, Learning Rate: 7e-06\n",
            "Step [4700/75490], Loss: 5.7619, Time: 1.92 seconds, Average Loss: 7.7937, Learning Rate: 7e-06\n",
            "Step [4710/75490], Loss: 6.8302, Time: 2.02 seconds, Average Loss: 7.7925, Learning Rate: 7e-06\n",
            "Step [4720/75490], Loss: 7.0498, Time: 2.08 seconds, Average Loss: 7.7903, Learning Rate: 7e-06\n",
            "Step [4730/75490], Loss: 8.0561, Time: 2.40 seconds, Average Loss: 7.7886, Learning Rate: 7e-06\n",
            "Step [4740/75490], Loss: 5.8141, Time: 1.53 seconds, Average Loss: 7.7865, Learning Rate: 7e-06\n",
            "Step [4750/75490], Loss: 7.5781, Time: 1.25 seconds, Average Loss: 7.7855, Learning Rate: 7e-06\n",
            "Step [4760/75490], Loss: 7.0648, Time: 1.82 seconds, Average Loss: 7.7845, Learning Rate: 7e-06\n",
            "Step [4770/75490], Loss: 6.7737, Time: 2.32 seconds, Average Loss: 7.7829, Learning Rate: 7e-06\n",
            "Step [4780/75490], Loss: 7.3068, Time: 1.09 seconds, Average Loss: 7.7816, Learning Rate: 7e-06\n",
            "Step [4790/75490], Loss: 7.4522, Time: 1.78 seconds, Average Loss: 7.7805, Learning Rate: 7e-06\n",
            "Step [4800/75490], Loss: 8.2638, Time: 1.78 seconds, Average Loss: 7.7802, Learning Rate: 7e-06\n",
            "Step [4810/75490], Loss: 7.6387, Time: 1.80 seconds, Average Loss: 7.7791, Learning Rate: 7e-06\n",
            "Step [4820/75490], Loss: 6.8298, Time: 1.90 seconds, Average Loss: 7.7784, Learning Rate: 7e-06\n",
            "Step [4830/75490], Loss: 7.2656, Time: 1.54 seconds, Average Loss: 7.7770, Learning Rate: 7e-06\n",
            "Step [4840/75490], Loss: 6.6508, Time: 1.54 seconds, Average Loss: 7.7753, Learning Rate: 7e-06\n",
            "Step [4850/75490], Loss: 6.9349, Time: 1.85 seconds, Average Loss: 7.7740, Learning Rate: 7e-06\n",
            "Step [4860/75490], Loss: 6.2700, Time: 1.82 seconds, Average Loss: 7.7721, Learning Rate: 7e-06\n",
            "Step [4870/75490], Loss: 8.0837, Time: 1.51 seconds, Average Loss: 7.7705, Learning Rate: 7e-06\n",
            "Step [4880/75490], Loss: 7.4551, Time: 1.57 seconds, Average Loss: 7.7696, Learning Rate: 7e-06\n",
            "Step [4890/75490], Loss: 7.2138, Time: 1.43 seconds, Average Loss: 7.7676, Learning Rate: 7e-06\n",
            "Step [4900/75490], Loss: 6.2685, Time: 1.83 seconds, Average Loss: 7.7658, Learning Rate: 7e-06\n",
            "Step [4910/75490], Loss: 8.7577, Time: 1.55 seconds, Average Loss: 7.7650, Learning Rate: 7e-06\n",
            "Step [4920/75490], Loss: 6.6630, Time: 2.18 seconds, Average Loss: 7.7628, Learning Rate: 7e-06\n",
            "Step [4930/75490], Loss: 7.0553, Time: 1.89 seconds, Average Loss: 7.7617, Learning Rate: 7e-06\n",
            "Step [4940/75490], Loss: 7.0775, Time: 1.56 seconds, Average Loss: 7.7598, Learning Rate: 7e-06\n",
            "Step [4950/75490], Loss: 7.3811, Time: 1.70 seconds, Average Loss: 7.7589, Learning Rate: 7e-06\n",
            "Step [4960/75490], Loss: 5.8877, Time: 1.47 seconds, Average Loss: 7.7574, Learning Rate: 7e-06\n",
            "Step [4970/75490], Loss: 7.7570, Time: 1.92 seconds, Average Loss: 7.7561, Learning Rate: 7e-06\n",
            "Step [4980/75490], Loss: 6.1536, Time: 1.39 seconds, Average Loss: 7.7547, Learning Rate: 7e-06\n",
            "Step [4990/75490], Loss: 7.1936, Time: 1.51 seconds, Average Loss: 7.7535, Learning Rate: 7e-06\n",
            "Step [5000/75490], Loss: 4.7677, Time: 1.40 seconds, Average Loss: 7.7517, Learning Rate: 7e-06\n",
            "Step [5010/75490], Loss: 7.5714, Time: 1.42 seconds, Average Loss: 7.7506, Learning Rate: 7e-06\n",
            "Step [5020/75490], Loss: 7.3330, Time: 1.66 seconds, Average Loss: 7.7486, Learning Rate: 7e-06\n",
            "Step [5030/75490], Loss: 6.8374, Time: 1.78 seconds, Average Loss: 7.7477, Learning Rate: 7e-06\n",
            "Step [5040/75490], Loss: 7.0387, Time: 1.96 seconds, Average Loss: 7.7458, Learning Rate: 7e-06\n",
            "Step [5050/75490], Loss: 6.9326, Time: 1.77 seconds, Average Loss: 7.7445, Learning Rate: 7e-06\n",
            "Step [5060/75490], Loss: 4.5958, Time: 1.33 seconds, Average Loss: 7.7429, Learning Rate: 7e-06\n",
            "Step [5070/75490], Loss: 7.0843, Time: 1.96 seconds, Average Loss: 7.7419, Learning Rate: 7e-06\n",
            "Step [5080/75490], Loss: 6.7694, Time: 1.73 seconds, Average Loss: 7.7409, Learning Rate: 7e-06\n",
            "Step [5090/75490], Loss: 8.3302, Time: 1.74 seconds, Average Loss: 7.7408, Learning Rate: 7e-06\n",
            "Step [5100/75490], Loss: 7.7550, Time: 1.99 seconds, Average Loss: 7.7391, Learning Rate: 7e-06\n",
            "Step [5110/75490], Loss: 7.7939, Time: 1.52 seconds, Average Loss: 7.7374, Learning Rate: 7e-06\n",
            "Step [5120/75490], Loss: 6.3437, Time: 1.74 seconds, Average Loss: 7.7356, Learning Rate: 7e-06\n",
            "Step [5130/75490], Loss: 5.5112, Time: 1.93 seconds, Average Loss: 7.7343, Learning Rate: 7e-06\n",
            "Step [5140/75490], Loss: 7.4489, Time: 2.05 seconds, Average Loss: 7.7330, Learning Rate: 7e-06\n",
            "Step [5150/75490], Loss: 7.0181, Time: 1.43 seconds, Average Loss: 7.7323, Learning Rate: 7e-06\n",
            "Step [5160/75490], Loss: 7.1761, Time: 1.73 seconds, Average Loss: 7.7303, Learning Rate: 7e-06\n",
            "Step [5170/75490], Loss: 6.4156, Time: 1.70 seconds, Average Loss: 7.7285, Learning Rate: 7e-06\n",
            "Step [5180/75490], Loss: 7.0117, Time: 1.47 seconds, Average Loss: 7.7272, Learning Rate: 7e-06\n",
            "Step [5190/75490], Loss: 7.9981, Time: 2.09 seconds, Average Loss: 7.7262, Learning Rate: 7e-06\n",
            "Step [5200/75490], Loss: 4.7858, Time: 1.57 seconds, Average Loss: 7.7243, Learning Rate: 7e-06\n",
            "Step [5210/75490], Loss: 6.8008, Time: 1.49 seconds, Average Loss: 7.7240, Learning Rate: 7e-06\n",
            "Step [5220/75490], Loss: 7.1936, Time: 1.66 seconds, Average Loss: 7.7226, Learning Rate: 7e-06\n",
            "Step [5230/75490], Loss: 7.8371, Time: 1.44 seconds, Average Loss: 7.7196, Learning Rate: 7e-06\n",
            "Step [5240/75490], Loss: 7.3945, Time: 1.64 seconds, Average Loss: 7.7185, Learning Rate: 7e-06\n",
            "Step [5250/75490], Loss: 6.0396, Time: 1.71 seconds, Average Loss: 7.7164, Learning Rate: 7e-06\n",
            "Step [5260/75490], Loss: 6.4142, Time: 1.57 seconds, Average Loss: 7.7152, Learning Rate: 7e-06\n",
            "Step [5270/75490], Loss: 6.8417, Time: 1.48 seconds, Average Loss: 7.7141, Learning Rate: 7e-06\n",
            "Step [5280/75490], Loss: 5.3150, Time: 1.83 seconds, Average Loss: 7.7123, Learning Rate: 7e-06\n",
            "Step [5290/75490], Loss: 8.6253, Time: 1.73 seconds, Average Loss: 7.7108, Learning Rate: 7e-06\n",
            "Step [5300/75490], Loss: 7.1949, Time: 1.79 seconds, Average Loss: 7.7090, Learning Rate: 7e-06\n",
            "Step [5310/75490], Loss: 4.8927, Time: 1.86 seconds, Average Loss: 7.7069, Learning Rate: 7e-06\n",
            "Step [5320/75490], Loss: 6.5692, Time: 2.09 seconds, Average Loss: 7.7054, Learning Rate: 7e-06\n",
            "Step [5330/75490], Loss: 6.9144, Time: 1.58 seconds, Average Loss: 7.7041, Learning Rate: 7e-06\n",
            "Step [5340/75490], Loss: 6.9677, Time: 1.57 seconds, Average Loss: 7.7019, Learning Rate: 7e-06\n",
            "Step [5350/75490], Loss: 7.6818, Time: 1.55 seconds, Average Loss: 7.7011, Learning Rate: 7e-06\n",
            "Step [5360/75490], Loss: 6.4690, Time: 1.35 seconds, Average Loss: 7.6993, Learning Rate: 7e-06\n",
            "Step [5370/75490], Loss: 6.9978, Time: 1.76 seconds, Average Loss: 7.6979, Learning Rate: 7e-06\n",
            "Step [5380/75490], Loss: 5.5103, Time: 1.40 seconds, Average Loss: 7.6968, Learning Rate: 7e-06\n",
            "Step [5390/75490], Loss: 7.1949, Time: 1.73 seconds, Average Loss: 7.6966, Learning Rate: 7e-06\n",
            "Step [5400/75490], Loss: 7.5811, Time: 1.30 seconds, Average Loss: 7.6944, Learning Rate: 7e-06\n",
            "Step [5410/75490], Loss: 7.3481, Time: 1.49 seconds, Average Loss: 7.6934, Learning Rate: 7e-06\n",
            "Step [5420/75490], Loss: 6.5904, Time: 1.52 seconds, Average Loss: 7.6921, Learning Rate: 7e-06\n",
            "Step [5430/75490], Loss: 8.1354, Time: 1.71 seconds, Average Loss: 7.6913, Learning Rate: 7e-06\n",
            "Step [5440/75490], Loss: 6.8952, Time: 1.53 seconds, Average Loss: 7.6903, Learning Rate: 7e-06\n",
            "Step [5450/75490], Loss: 6.8497, Time: 1.77 seconds, Average Loss: 7.6891, Learning Rate: 7e-06\n",
            "Step [5460/75490], Loss: 7.0014, Time: 1.90 seconds, Average Loss: 7.6882, Learning Rate: 7e-06\n",
            "Step [5470/75490], Loss: 7.4905, Time: 1.81 seconds, Average Loss: 7.6866, Learning Rate: 7e-06\n",
            "Step [5480/75490], Loss: 6.2922, Time: 1.78 seconds, Average Loss: 7.6851, Learning Rate: 7e-06\n",
            "Step [5490/75490], Loss: 6.5496, Time: 1.45 seconds, Average Loss: 7.6840, Learning Rate: 7e-06\n",
            "Step [5500/75490], Loss: 7.3974, Time: 1.75 seconds, Average Loss: 7.6829, Learning Rate: 7e-06\n",
            "Step [5510/75490], Loss: 8.2407, Time: 1.85 seconds, Average Loss: 7.6821, Learning Rate: 7e-06\n",
            "Step [5520/75490], Loss: 6.7853, Time: 1.83 seconds, Average Loss: 7.6802, Learning Rate: 7e-06\n",
            "Step [5530/75490], Loss: 7.0639, Time: 1.91 seconds, Average Loss: 7.6798, Learning Rate: 7e-06\n",
            "Step [5540/75490], Loss: 7.5348, Time: 1.92 seconds, Average Loss: 7.6783, Learning Rate: 7e-06\n",
            "Step [5550/75490], Loss: 6.8341, Time: 2.30 seconds, Average Loss: 7.6767, Learning Rate: 7e-06\n",
            "Step [5560/75490], Loss: 6.7658, Time: 1.58 seconds, Average Loss: 7.6758, Learning Rate: 7e-06\n",
            "Step [5570/75490], Loss: 4.4076, Time: 2.24 seconds, Average Loss: 7.6740, Learning Rate: 7e-06\n",
            "Step [5580/75490], Loss: 8.9315, Time: 1.57 seconds, Average Loss: 7.6736, Learning Rate: 7e-06\n",
            "Step [5590/75490], Loss: 6.6817, Time: 1.74 seconds, Average Loss: 7.6721, Learning Rate: 7e-06\n",
            "Step [5600/75490], Loss: 4.1440, Time: 1.70 seconds, Average Loss: 7.6703, Learning Rate: 7e-06\n",
            "Step [5610/75490], Loss: 7.1725, Time: 1.89 seconds, Average Loss: 7.6693, Learning Rate: 7e-06\n",
            "Step [5620/75490], Loss: 7.7452, Time: 1.53 seconds, Average Loss: 7.6680, Learning Rate: 7e-06\n",
            "Step [5630/75490], Loss: 7.5517, Time: 1.84 seconds, Average Loss: 7.6669, Learning Rate: 7e-06\n",
            "Step [5640/75490], Loss: 7.5534, Time: 1.90 seconds, Average Loss: 7.6662, Learning Rate: 7e-06\n",
            "Step [5650/75490], Loss: 6.3970, Time: 1.80 seconds, Average Loss: 7.6653, Learning Rate: 7e-06\n",
            "Step [5660/75490], Loss: 6.8537, Time: 1.73 seconds, Average Loss: 7.6644, Learning Rate: 7e-06\n",
            "Step [5670/75490], Loss: 7.5671, Time: 1.43 seconds, Average Loss: 7.6636, Learning Rate: 7e-06\n",
            "Step [5680/75490], Loss: 5.7862, Time: 1.70 seconds, Average Loss: 7.6627, Learning Rate: 7e-06\n",
            "Step [5690/75490], Loss: 7.4517, Time: 1.24 seconds, Average Loss: 7.6615, Learning Rate: 7e-06\n",
            "Step [5700/75490], Loss: 6.6917, Time: 1.38 seconds, Average Loss: 7.6597, Learning Rate: 7e-06\n",
            "Step [5710/75490], Loss: 6.6752, Time: 1.68 seconds, Average Loss: 7.6579, Learning Rate: 7e-06\n",
            "Step [5720/75490], Loss: 7.4807, Time: 1.88 seconds, Average Loss: 7.6568, Learning Rate: 7e-06\n",
            "Step [5730/75490], Loss: 7.3417, Time: 1.59 seconds, Average Loss: 7.6549, Learning Rate: 7e-06\n",
            "Step [5740/75490], Loss: 6.7585, Time: 1.48 seconds, Average Loss: 7.6535, Learning Rate: 7e-06\n",
            "Step [5750/75490], Loss: 6.8680, Time: 2.01 seconds, Average Loss: 7.6522, Learning Rate: 7e-06\n",
            "Step [5760/75490], Loss: 7.2066, Time: 1.39 seconds, Average Loss: 7.6510, Learning Rate: 7e-06\n",
            "Step [5770/75490], Loss: 8.9441, Time: 1.56 seconds, Average Loss: 7.6505, Learning Rate: 7e-06\n",
            "Step [5780/75490], Loss: 6.3187, Time: 1.86 seconds, Average Loss: 7.6481, Learning Rate: 7e-06\n",
            "Step [5790/75490], Loss: 6.2378, Time: 1.39 seconds, Average Loss: 7.6469, Learning Rate: 7e-06\n",
            "Step [5800/75490], Loss: 6.4037, Time: 2.23 seconds, Average Loss: 7.6454, Learning Rate: 7e-06\n",
            "Step [5810/75490], Loss: 6.5659, Time: 1.95 seconds, Average Loss: 7.6442, Learning Rate: 7e-06\n",
            "Step [5820/75490], Loss: 7.7363, Time: 1.62 seconds, Average Loss: 7.6432, Learning Rate: 7e-06\n",
            "Step [5830/75490], Loss: 7.8278, Time: 1.66 seconds, Average Loss: 7.6420, Learning Rate: 7e-06\n",
            "Step [5840/75490], Loss: 7.2728, Time: 1.67 seconds, Average Loss: 7.6413, Learning Rate: 7e-06\n",
            "Step [5850/75490], Loss: 6.2900, Time: 1.84 seconds, Average Loss: 7.6398, Learning Rate: 7e-06\n",
            "Step [5860/75490], Loss: 6.5049, Time: 1.57 seconds, Average Loss: 7.6380, Learning Rate: 7e-06\n",
            "Step [5870/75490], Loss: 6.2416, Time: 2.22 seconds, Average Loss: 7.6369, Learning Rate: 7e-06\n",
            "Step [5880/75490], Loss: 5.1339, Time: 1.74 seconds, Average Loss: 7.6355, Learning Rate: 7e-06\n",
            "Step [5890/75490], Loss: 7.2287, Time: 1.86 seconds, Average Loss: 7.6343, Learning Rate: 7e-06\n",
            "Step [5900/75490], Loss: 6.5558, Time: 1.82 seconds, Average Loss: 7.6330, Learning Rate: 7e-06\n",
            "Step [5910/75490], Loss: 7.3446, Time: 1.56 seconds, Average Loss: 7.6323, Learning Rate: 7e-06\n",
            "Step [5920/75490], Loss: 6.1393, Time: 1.48 seconds, Average Loss: 7.6309, Learning Rate: 7e-06\n",
            "Step [5930/75490], Loss: 7.8317, Time: 1.24 seconds, Average Loss: 7.6294, Learning Rate: 7e-06\n",
            "Step [5940/75490], Loss: 7.5850, Time: 1.39 seconds, Average Loss: 7.6275, Learning Rate: 7e-06\n",
            "Step [5950/75490], Loss: 7.1227, Time: 1.38 seconds, Average Loss: 7.6268, Learning Rate: 7e-06\n",
            "Step [5960/75490], Loss: 7.1298, Time: 1.69 seconds, Average Loss: 7.6252, Learning Rate: 7e-06\n",
            "Step [5970/75490], Loss: 6.8188, Time: 1.51 seconds, Average Loss: 7.6245, Learning Rate: 7e-06\n",
            "Step [5980/75490], Loss: 6.8638, Time: 1.75 seconds, Average Loss: 7.6237, Learning Rate: 7e-06\n",
            "Step [5990/75490], Loss: 6.6904, Time: 1.79 seconds, Average Loss: 7.6227, Learning Rate: 7e-06\n",
            "Step [6000/75490], Loss: 7.0605, Time: 1.68 seconds, Average Loss: 7.6212, Learning Rate: 7e-06\n",
            "Step [6010/75490], Loss: 6.8786, Time: 1.70 seconds, Average Loss: 7.6204, Learning Rate: 7e-06\n",
            "Step [6020/75490], Loss: 7.7708, Time: 1.68 seconds, Average Loss: 7.6189, Learning Rate: 7e-06\n",
            "Step [6030/75490], Loss: 7.4124, Time: 1.55 seconds, Average Loss: 7.6188, Learning Rate: 7e-06\n",
            "Step [6040/75490], Loss: 6.0285, Time: 1.37 seconds, Average Loss: 7.6179, Learning Rate: 7e-06\n",
            "Step [6050/75490], Loss: 7.4459, Time: 2.09 seconds, Average Loss: 7.6169, Learning Rate: 7e-06\n",
            "Step [6060/75490], Loss: 6.4622, Time: 1.57 seconds, Average Loss: 7.6161, Learning Rate: 7e-06\n",
            "Step [6070/75490], Loss: 7.0380, Time: 1.61 seconds, Average Loss: 7.6152, Learning Rate: 7e-06\n",
            "Step [6080/75490], Loss: 8.5831, Time: 1.72 seconds, Average Loss: 7.6140, Learning Rate: 7e-06\n",
            "Step [6090/75490], Loss: 7.2294, Time: 1.58 seconds, Average Loss: 7.6125, Learning Rate: 7e-06\n",
            "Step [6100/75490], Loss: 6.4801, Time: 1.56 seconds, Average Loss: 7.6120, Learning Rate: 7e-06\n",
            "Step [6110/75490], Loss: 6.9777, Time: 1.80 seconds, Average Loss: 7.6104, Learning Rate: 7e-06\n",
            "Step [6120/75490], Loss: 9.5765, Time: 1.25 seconds, Average Loss: 7.6095, Learning Rate: 7e-06\n",
            "Step [6130/75490], Loss: 7.5678, Time: 1.87 seconds, Average Loss: 7.6094, Learning Rate: 7e-06\n",
            "Step [6140/75490], Loss: 7.5113, Time: 1.46 seconds, Average Loss: 7.6090, Learning Rate: 7e-06\n",
            "Step [6150/75490], Loss: 8.1642, Time: 1.46 seconds, Average Loss: 7.6084, Learning Rate: 7e-06\n",
            "Step [6160/75490], Loss: 6.9732, Time: 2.08 seconds, Average Loss: 7.6080, Learning Rate: 7e-06\n",
            "Step [6170/75490], Loss: 7.2487, Time: 2.44 seconds, Average Loss: 7.6073, Learning Rate: 7e-06\n",
            "Step [6180/75490], Loss: 7.3960, Time: 1.56 seconds, Average Loss: 7.6068, Learning Rate: 7e-06\n",
            "Step [6190/75490], Loss: 7.7592, Time: 2.04 seconds, Average Loss: 7.6049, Learning Rate: 7e-06\n",
            "Step [6200/75490], Loss: 6.6392, Time: 1.82 seconds, Average Loss: 7.6040, Learning Rate: 7e-06\n",
            "Step [6210/75490], Loss: 7.3856, Time: 2.05 seconds, Average Loss: 7.6033, Learning Rate: 7e-06\n",
            "Step [6220/75490], Loss: 7.6444, Time: 1.74 seconds, Average Loss: 7.6026, Learning Rate: 7e-06\n",
            "Step [6230/75490], Loss: 6.7746, Time: 2.07 seconds, Average Loss: 7.6017, Learning Rate: 7e-06\n",
            "Step [6240/75490], Loss: 7.2292, Time: 2.00 seconds, Average Loss: 7.6015, Learning Rate: 7e-06\n",
            "Step [6250/75490], Loss: 7.6527, Time: 1.09 seconds, Average Loss: 7.6002, Learning Rate: 7e-06\n",
            "Step [6260/75490], Loss: 5.7120, Time: 1.87 seconds, Average Loss: 7.5982, Learning Rate: 7e-06\n",
            "Step [6270/75490], Loss: 7.2477, Time: 1.99 seconds, Average Loss: 7.5968, Learning Rate: 7e-06\n",
            "Step [6280/75490], Loss: 5.6040, Time: 1.24 seconds, Average Loss: 7.5951, Learning Rate: 7e-06\n",
            "Step [6290/75490], Loss: 7.7315, Time: 1.92 seconds, Average Loss: 7.5936, Learning Rate: 7e-06\n",
            "Step [6300/75490], Loss: 6.9926, Time: 1.63 seconds, Average Loss: 7.5921, Learning Rate: 7e-06\n",
            "Step [6310/75490], Loss: 6.4411, Time: 1.71 seconds, Average Loss: 7.5908, Learning Rate: 7e-06\n",
            "Step [6320/75490], Loss: 7.5409, Time: 1.86 seconds, Average Loss: 7.5894, Learning Rate: 7e-06\n",
            "Step [6330/75490], Loss: 7.0118, Time: 1.98 seconds, Average Loss: 7.5891, Learning Rate: 7e-06\n",
            "Step [6340/75490], Loss: 7.2649, Time: 1.58 seconds, Average Loss: 7.5889, Learning Rate: 7e-06\n",
            "Step [6350/75490], Loss: 7.1332, Time: 1.60 seconds, Average Loss: 7.5879, Learning Rate: 7e-06\n",
            "Step [6360/75490], Loss: 7.6492, Time: 1.87 seconds, Average Loss: 7.5865, Learning Rate: 7e-06\n",
            "Step [6370/75490], Loss: 5.9706, Time: 1.71 seconds, Average Loss: 7.5856, Learning Rate: 7e-06\n",
            "Step [6380/75490], Loss: 7.5622, Time: 1.44 seconds, Average Loss: 7.5850, Learning Rate: 7e-06\n",
            "Step [6390/75490], Loss: 7.8544, Time: 2.09 seconds, Average Loss: 7.5850, Learning Rate: 7e-06\n",
            "Step [6400/75490], Loss: 6.1338, Time: 1.38 seconds, Average Loss: 7.5838, Learning Rate: 7e-06\n",
            "Step [6410/75490], Loss: 6.8965, Time: 2.02 seconds, Average Loss: 7.5824, Learning Rate: 7e-06\n",
            "Step [6420/75490], Loss: 6.6098, Time: 1.41 seconds, Average Loss: 7.5825, Learning Rate: 7e-06\n",
            "Step [6430/75490], Loss: 6.8221, Time: 2.04 seconds, Average Loss: 7.5814, Learning Rate: 7e-06\n",
            "Step [6440/75490], Loss: 6.2129, Time: 1.46 seconds, Average Loss: 7.5802, Learning Rate: 7e-06\n",
            "Step [6450/75490], Loss: 6.3462, Time: 1.89 seconds, Average Loss: 7.5791, Learning Rate: 7e-06\n",
            "Step [6460/75490], Loss: 6.9245, Time: 1.73 seconds, Average Loss: 7.5783, Learning Rate: 7e-06\n",
            "Step [6470/75490], Loss: 8.5094, Time: 2.32 seconds, Average Loss: 7.5777, Learning Rate: 7e-06\n",
            "Step [6480/75490], Loss: 6.5927, Time: 1.65 seconds, Average Loss: 7.5769, Learning Rate: 7e-06\n",
            "Step [6490/75490], Loss: 6.5826, Time: 1.55 seconds, Average Loss: 7.5763, Learning Rate: 7e-06\n",
            "Step [6500/75490], Loss: 6.1802, Time: 1.54 seconds, Average Loss: 7.5752, Learning Rate: 7e-06\n",
            "Step [6510/75490], Loss: 7.2060, Time: 1.67 seconds, Average Loss: 7.5747, Learning Rate: 7e-06\n",
            "Step [6520/75490], Loss: 6.8862, Time: 1.80 seconds, Average Loss: 7.5737, Learning Rate: 7e-06\n",
            "Step [6530/75490], Loss: 6.6354, Time: 1.81 seconds, Average Loss: 7.5729, Learning Rate: 7e-06\n",
            "Step [6540/75490], Loss: 6.7364, Time: 1.65 seconds, Average Loss: 7.5719, Learning Rate: 7e-06\n",
            "Step [6550/75490], Loss: 7.3783, Time: 2.04 seconds, Average Loss: 7.5705, Learning Rate: 7e-06\n",
            "Step [6560/75490], Loss: 6.5409, Time: 1.71 seconds, Average Loss: 7.5696, Learning Rate: 7e-06\n",
            "Step [6570/75490], Loss: 6.2350, Time: 1.18 seconds, Average Loss: 7.5682, Learning Rate: 7e-06\n",
            "Step [6580/75490], Loss: 7.0302, Time: 2.00 seconds, Average Loss: 7.5671, Learning Rate: 7e-06\n",
            "Step [6590/75490], Loss: 5.9337, Time: 1.42 seconds, Average Loss: 7.5656, Learning Rate: 7e-06\n",
            "Step [6600/75490], Loss: 8.8783, Time: 2.39 seconds, Average Loss: 7.5644, Learning Rate: 7e-06\n",
            "Step [6610/75490], Loss: 6.5690, Time: 1.84 seconds, Average Loss: 7.5626, Learning Rate: 7e-06\n",
            "Step [6620/75490], Loss: 7.6982, Time: 1.82 seconds, Average Loss: 7.5616, Learning Rate: 7e-06\n",
            "Step [6630/75490], Loss: 7.9763, Time: 2.06 seconds, Average Loss: 7.5603, Learning Rate: 7e-06\n",
            "Step [6640/75490], Loss: 7.4439, Time: 1.41 seconds, Average Loss: 7.5588, Learning Rate: 7e-06\n",
            "Step [6650/75490], Loss: 7.2850, Time: 1.06 seconds, Average Loss: 7.5570, Learning Rate: 7e-06\n",
            "Step [6660/75490], Loss: 7.1871, Time: 1.39 seconds, Average Loss: 7.5560, Learning Rate: 7e-06\n",
            "Step [6670/75490], Loss: 6.7322, Time: 1.55 seconds, Average Loss: 7.5546, Learning Rate: 7e-06\n",
            "Step [6680/75490], Loss: 6.7328, Time: 2.00 seconds, Average Loss: 7.5538, Learning Rate: 7e-06\n",
            "Step [6690/75490], Loss: 6.4865, Time: 1.80 seconds, Average Loss: 7.5526, Learning Rate: 7e-06\n",
            "Step [6700/75490], Loss: 6.9620, Time: 1.58 seconds, Average Loss: 7.5512, Learning Rate: 7e-06\n",
            "Step [6710/75490], Loss: 5.8350, Time: 1.71 seconds, Average Loss: 7.5507, Learning Rate: 7e-06\n",
            "Step [6720/75490], Loss: 7.0731, Time: 1.86 seconds, Average Loss: 7.5498, Learning Rate: 7e-06\n",
            "Step [6730/75490], Loss: 5.3795, Time: 1.57 seconds, Average Loss: 7.5487, Learning Rate: 7e-06\n",
            "Step [6740/75490], Loss: 6.3779, Time: 1.57 seconds, Average Loss: 7.5479, Learning Rate: 7e-06\n",
            "Step [6750/75490], Loss: 5.1798, Time: 2.35 seconds, Average Loss: 7.5464, Learning Rate: 7e-06\n",
            "Step [6760/75490], Loss: 6.9935, Time: 1.71 seconds, Average Loss: 7.5459, Learning Rate: 7e-06\n",
            "Step [6770/75490], Loss: 7.8084, Time: 1.49 seconds, Average Loss: 7.5453, Learning Rate: 7e-06\n",
            "Step [6780/75490], Loss: 5.4378, Time: 1.56 seconds, Average Loss: 7.5440, Learning Rate: 7e-06\n",
            "Step [6790/75490], Loss: 7.0766, Time: 1.55 seconds, Average Loss: 7.5431, Learning Rate: 7e-06\n",
            "Step [6800/75490], Loss: 7.4368, Time: 1.44 seconds, Average Loss: 7.5419, Learning Rate: 7e-06\n",
            "Step [6810/75490], Loss: 6.6585, Time: 1.60 seconds, Average Loss: 7.5413, Learning Rate: 7e-06\n",
            "Step [6820/75490], Loss: 6.8715, Time: 1.38 seconds, Average Loss: 7.5403, Learning Rate: 7e-06\n",
            "Step [6830/75490], Loss: 7.3269, Time: 2.06 seconds, Average Loss: 7.5390, Learning Rate: 7e-06\n",
            "Step [6840/75490], Loss: 6.3177, Time: 1.24 seconds, Average Loss: 7.5378, Learning Rate: 7e-06\n",
            "Step [6850/75490], Loss: 6.7080, Time: 1.53 seconds, Average Loss: 7.5366, Learning Rate: 7e-06\n",
            "Step [6860/75490], Loss: 8.2626, Time: 1.80 seconds, Average Loss: 7.5357, Learning Rate: 7e-06\n",
            "Step [6870/75490], Loss: 5.8442, Time: 2.24 seconds, Average Loss: 7.5355, Learning Rate: 7e-06\n",
            "Step [6880/75490], Loss: 6.6520, Time: 1.33 seconds, Average Loss: 7.5341, Learning Rate: 7e-06\n",
            "Step [6890/75490], Loss: 6.6486, Time: 1.89 seconds, Average Loss: 7.5330, Learning Rate: 7e-06\n",
            "Step [6900/75490], Loss: 4.9878, Time: 1.50 seconds, Average Loss: 7.5316, Learning Rate: 7e-06\n",
            "Step [6910/75490], Loss: 7.3155, Time: 1.52 seconds, Average Loss: 7.5302, Learning Rate: 7e-06\n",
            "Step [6920/75490], Loss: 7.0609, Time: 1.74 seconds, Average Loss: 7.5294, Learning Rate: 7e-06\n",
            "Step [6930/75490], Loss: 5.7248, Time: 1.70 seconds, Average Loss: 7.5282, Learning Rate: 7e-06\n",
            "Step [6940/75490], Loss: 7.5885, Time: 1.46 seconds, Average Loss: 7.5276, Learning Rate: 7e-06\n",
            "Step [6950/75490], Loss: 7.5034, Time: 2.06 seconds, Average Loss: 7.5264, Learning Rate: 7e-06\n",
            "Step [6960/75490], Loss: 5.3428, Time: 1.59 seconds, Average Loss: 7.5254, Learning Rate: 7e-06\n",
            "Step [6970/75490], Loss: 6.6755, Time: 2.33 seconds, Average Loss: 7.5244, Learning Rate: 7e-06\n",
            "Step [6980/75490], Loss: 7.8781, Time: 1.94 seconds, Average Loss: 7.5239, Learning Rate: 7e-06\n",
            "Step [6990/75490], Loss: 6.3022, Time: 1.93 seconds, Average Loss: 7.5226, Learning Rate: 7e-06\n",
            "Step [7000/75490], Loss: 6.7848, Time: 2.19 seconds, Average Loss: 7.5216, Learning Rate: 7e-06\n",
            "Step [7010/75490], Loss: 6.6719, Time: 1.80 seconds, Average Loss: 7.5210, Learning Rate: 7e-06\n",
            "Step [7020/75490], Loss: 7.3698, Time: 1.50 seconds, Average Loss: 7.5204, Learning Rate: 7e-06\n",
            "Step [7030/75490], Loss: 6.0804, Time: 1.97 seconds, Average Loss: 7.5185, Learning Rate: 7e-06\n",
            "Step [7040/75490], Loss: 7.5221, Time: 2.06 seconds, Average Loss: 7.5184, Learning Rate: 7e-06\n",
            "Step [7050/75490], Loss: 6.6105, Time: 1.48 seconds, Average Loss: 7.5176, Learning Rate: 7e-06\n",
            "Step [7060/75490], Loss: 7.2411, Time: 1.57 seconds, Average Loss: 7.5164, Learning Rate: 7e-06\n",
            "Step [7070/75490], Loss: 7.6432, Time: 1.39 seconds, Average Loss: 7.5153, Learning Rate: 7e-06\n",
            "Step [7080/75490], Loss: 7.2605, Time: 1.72 seconds, Average Loss: 7.5151, Learning Rate: 7e-06\n",
            "Step [7090/75490], Loss: 5.8020, Time: 1.42 seconds, Average Loss: 7.5136, Learning Rate: 7e-06\n",
            "Step [7100/75490], Loss: 6.4643, Time: 1.87 seconds, Average Loss: 7.5128, Learning Rate: 7e-06\n",
            "Step [7110/75490], Loss: 6.6767, Time: 1.65 seconds, Average Loss: 7.5108, Learning Rate: 7e-06\n",
            "Step [7120/75490], Loss: 5.2000, Time: 1.77 seconds, Average Loss: 7.5092, Learning Rate: 7e-06\n",
            "Step [7130/75490], Loss: 6.7277, Time: 1.48 seconds, Average Loss: 7.5076, Learning Rate: 7e-06\n",
            "Step [7140/75490], Loss: 7.6593, Time: 1.46 seconds, Average Loss: 7.5065, Learning Rate: 7e-06\n",
            "Step [7150/75490], Loss: 6.6594, Time: 1.39 seconds, Average Loss: 7.5057, Learning Rate: 7e-06\n",
            "Step [7160/75490], Loss: 7.1232, Time: 1.52 seconds, Average Loss: 7.5056, Learning Rate: 7e-06\n",
            "Step [7170/75490], Loss: 8.1858, Time: 1.51 seconds, Average Loss: 7.5044, Learning Rate: 7e-06\n",
            "Step [7180/75490], Loss: 4.8501, Time: 2.11 seconds, Average Loss: 7.5039, Learning Rate: 7e-06\n",
            "Step [7190/75490], Loss: 5.8593, Time: 1.37 seconds, Average Loss: 7.5022, Learning Rate: 7e-06\n",
            "Step [7200/75490], Loss: 9.2409, Time: 1.76 seconds, Average Loss: 7.5010, Learning Rate: 7e-06\n",
            "Step [7210/75490], Loss: 7.1474, Time: 1.59 seconds, Average Loss: 7.5005, Learning Rate: 7e-06\n",
            "Step [7220/75490], Loss: 7.6199, Time: 1.77 seconds, Average Loss: 7.5001, Learning Rate: 7e-06\n",
            "Step [7230/75490], Loss: 7.3869, Time: 1.61 seconds, Average Loss: 7.4997, Learning Rate: 7e-06\n",
            "Step [7240/75490], Loss: 6.8870, Time: 1.81 seconds, Average Loss: 7.4986, Learning Rate: 7e-06\n",
            "Step [7250/75490], Loss: 7.2031, Time: 1.53 seconds, Average Loss: 7.4973, Learning Rate: 7e-06\n",
            "Step [7260/75490], Loss: 6.8076, Time: 2.32 seconds, Average Loss: 7.4970, Learning Rate: 7e-06\n",
            "Step [7270/75490], Loss: 7.0858, Time: 1.68 seconds, Average Loss: 7.4962, Learning Rate: 7e-06\n",
            "Step [7280/75490], Loss: 5.8820, Time: 1.57 seconds, Average Loss: 7.4951, Learning Rate: 7e-06\n",
            "Step [7290/75490], Loss: 7.0845, Time: 1.78 seconds, Average Loss: 7.4940, Learning Rate: 7e-06\n",
            "Step [7300/75490], Loss: 5.9160, Time: 1.86 seconds, Average Loss: 7.4931, Learning Rate: 7e-06\n",
            "Step [7310/75490], Loss: 6.3833, Time: 1.90 seconds, Average Loss: 7.4920, Learning Rate: 7e-06\n",
            "Step [7320/75490], Loss: 7.4207, Time: 1.98 seconds, Average Loss: 7.4918, Learning Rate: 7e-06\n",
            "Step [7330/75490], Loss: 6.4146, Time: 1.69 seconds, Average Loss: 7.4912, Learning Rate: 7e-06\n",
            "Step [7340/75490], Loss: 6.4245, Time: 1.43 seconds, Average Loss: 7.4894, Learning Rate: 7e-06\n",
            "Step [7350/75490], Loss: 6.0067, Time: 1.27 seconds, Average Loss: 7.4890, Learning Rate: 7e-06\n",
            "Step [7360/75490], Loss: 7.0481, Time: 2.28 seconds, Average Loss: 7.4889, Learning Rate: 7e-06\n",
            "Step [7370/75490], Loss: 6.8210, Time: 1.98 seconds, Average Loss: 7.4879, Learning Rate: 7e-06\n",
            "Step [7380/75490], Loss: 7.2624, Time: 1.60 seconds, Average Loss: 7.4873, Learning Rate: 7e-06\n",
            "Step [7390/75490], Loss: 5.9937, Time: 2.17 seconds, Average Loss: 7.4861, Learning Rate: 7e-06\n",
            "Step [7400/75490], Loss: 6.2371, Time: 1.88 seconds, Average Loss: 7.4853, Learning Rate: 7e-06\n",
            "Step [7410/75490], Loss: 8.1470, Time: 1.36 seconds, Average Loss: 7.4842, Learning Rate: 7e-06\n",
            "Step [7420/75490], Loss: 6.6836, Time: 1.76 seconds, Average Loss: 7.4826, Learning Rate: 7e-06\n",
            "Step [7430/75490], Loss: 7.5621, Time: 1.58 seconds, Average Loss: 7.4819, Learning Rate: 7e-06\n",
            "Step [7440/75490], Loss: 6.6993, Time: 1.86 seconds, Average Loss: 7.4811, Learning Rate: 7e-06\n",
            "Step [7450/75490], Loss: 7.5873, Time: 1.67 seconds, Average Loss: 7.4803, Learning Rate: 7e-06\n",
            "Step [7460/75490], Loss: 6.1121, Time: 1.87 seconds, Average Loss: 7.4793, Learning Rate: 7e-06\n",
            "Step [7470/75490], Loss: 6.9090, Time: 1.77 seconds, Average Loss: 7.4785, Learning Rate: 7e-06\n",
            "Step [7480/75490], Loss: 6.7658, Time: 1.86 seconds, Average Loss: 7.4772, Learning Rate: 7e-06\n",
            "Step [7490/75490], Loss: 7.1481, Time: 1.79 seconds, Average Loss: 7.4767, Learning Rate: 7e-06\n",
            "Step [7500/75490], Loss: 7.9417, Time: 1.71 seconds, Average Loss: 7.4758, Learning Rate: 7e-06\n",
            "Step [7510/75490], Loss: 5.1995, Time: 1.75 seconds, Average Loss: 7.4750, Learning Rate: 7e-06\n",
            "Step [7520/75490], Loss: 6.4557, Time: 1.80 seconds, Average Loss: 7.4743, Learning Rate: 7e-06\n",
            "Step [7530/75490], Loss: 7.3328, Time: 1.89 seconds, Average Loss: 7.4737, Learning Rate: 7e-06\n",
            "Step [7540/75490], Loss: 7.9465, Time: 1.99 seconds, Average Loss: 7.4729, Learning Rate: 7e-06\n",
            "Step [7550/75490], Loss: 6.0052, Time: 1.53 seconds, Average Loss: 7.4717, Learning Rate: 7e-06\n",
            "Step [7560/75490], Loss: 6.2971, Time: 1.81 seconds, Average Loss: 7.4711, Learning Rate: 7e-06\n",
            "Step [7570/75490], Loss: 6.4819, Time: 1.43 seconds, Average Loss: 7.4705, Learning Rate: 7e-06\n",
            "Step [7580/75490], Loss: 5.1705, Time: 1.34 seconds, Average Loss: 7.4695, Learning Rate: 7e-06\n",
            "Step [7590/75490], Loss: 7.9756, Time: 1.35 seconds, Average Loss: 7.4681, Learning Rate: 7e-06\n",
            "Step [7600/75490], Loss: 7.9569, Time: 2.00 seconds, Average Loss: 7.4676, Learning Rate: 7e-06\n",
            "Step [7610/75490], Loss: 7.2193, Time: 1.81 seconds, Average Loss: 7.4671, Learning Rate: 7e-06\n",
            "Step [7620/75490], Loss: 6.4741, Time: 1.70 seconds, Average Loss: 7.4668, Learning Rate: 7e-06\n",
            "Step [7630/75490], Loss: 6.2414, Time: 1.87 seconds, Average Loss: 7.4651, Learning Rate: 7e-06\n",
            "Step [7640/75490], Loss: 7.1234, Time: 2.00 seconds, Average Loss: 7.4640, Learning Rate: 7e-06\n",
            "Step [7650/75490], Loss: 7.5867, Time: 1.91 seconds, Average Loss: 7.4629, Learning Rate: 7e-06\n",
            "Step [7660/75490], Loss: 6.9840, Time: 1.56 seconds, Average Loss: 7.4619, Learning Rate: 7e-06\n",
            "Step [7670/75490], Loss: 7.6250, Time: 1.84 seconds, Average Loss: 7.4606, Learning Rate: 7e-06\n",
            "Step [7680/75490], Loss: 5.3101, Time: 1.67 seconds, Average Loss: 7.4595, Learning Rate: 7e-06\n",
            "Step [7690/75490], Loss: 7.8021, Time: 2.24 seconds, Average Loss: 7.4592, Learning Rate: 7e-06\n",
            "Step [7700/75490], Loss: 7.1669, Time: 1.17 seconds, Average Loss: 7.4586, Learning Rate: 7e-06\n",
            "Step [7710/75490], Loss: 7.4246, Time: 1.90 seconds, Average Loss: 7.4580, Learning Rate: 7e-06\n",
            "Step [7720/75490], Loss: 6.9739, Time: 1.57 seconds, Average Loss: 7.4569, Learning Rate: 7e-06\n",
            "Step [7730/75490], Loss: 7.1898, Time: 1.42 seconds, Average Loss: 7.4562, Learning Rate: 7e-06\n",
            "Step [7740/75490], Loss: 7.4209, Time: 2.14 seconds, Average Loss: 7.4555, Learning Rate: 7e-06\n",
            "Step [7750/75490], Loss: 6.6414, Time: 1.51 seconds, Average Loss: 7.4551, Learning Rate: 7e-06\n",
            "Step [7760/75490], Loss: 7.7177, Time: 1.09 seconds, Average Loss: 7.4553, Learning Rate: 7e-06\n",
            "Step [7770/75490], Loss: 5.6067, Time: 1.72 seconds, Average Loss: 7.4544, Learning Rate: 7e-06\n",
            "Step [7780/75490], Loss: 7.0100, Time: 1.60 seconds, Average Loss: 7.4537, Learning Rate: 7e-06\n",
            "Step [7790/75490], Loss: 7.8105, Time: 1.47 seconds, Average Loss: 7.4529, Learning Rate: 7e-06\n",
            "Step [7800/75490], Loss: 7.9427, Time: 2.00 seconds, Average Loss: 7.4518, Learning Rate: 7e-06\n",
            "Step [7810/75490], Loss: 7.1017, Time: 1.72 seconds, Average Loss: 7.4513, Learning Rate: 7e-06\n",
            "Step [7820/75490], Loss: 6.8365, Time: 1.67 seconds, Average Loss: 7.4500, Learning Rate: 7e-06\n",
            "Step [7830/75490], Loss: 7.2614, Time: 1.74 seconds, Average Loss: 7.4498, Learning Rate: 7e-06\n",
            "Step [7840/75490], Loss: 8.1976, Time: 2.02 seconds, Average Loss: 7.4485, Learning Rate: 7e-06\n",
            "Step [7850/75490], Loss: 7.2431, Time: 1.59 seconds, Average Loss: 7.4481, Learning Rate: 7e-06\n",
            "Step [7860/75490], Loss: 6.9736, Time: 1.30 seconds, Average Loss: 7.4477, Learning Rate: 7e-06\n",
            "Step [7870/75490], Loss: 7.0810, Time: 1.59 seconds, Average Loss: 7.4472, Learning Rate: 7e-06\n",
            "Step [7880/75490], Loss: 7.4533, Time: 2.13 seconds, Average Loss: 7.4464, Learning Rate: 7e-06\n",
            "Step [7890/75490], Loss: 7.5471, Time: 2.04 seconds, Average Loss: 7.4455, Learning Rate: 7e-06\n",
            "Step [7900/75490], Loss: 7.2432, Time: 1.87 seconds, Average Loss: 7.4451, Learning Rate: 7e-06\n",
            "Step [7910/75490], Loss: 6.8197, Time: 1.53 seconds, Average Loss: 7.4442, Learning Rate: 7e-06\n",
            "Step [7920/75490], Loss: 7.0322, Time: 1.89 seconds, Average Loss: 7.4434, Learning Rate: 7e-06\n",
            "Step [7930/75490], Loss: 6.4285, Time: 1.90 seconds, Average Loss: 7.4420, Learning Rate: 7e-06\n",
            "Step [7940/75490], Loss: 7.9110, Time: 1.44 seconds, Average Loss: 7.4416, Learning Rate: 7e-06\n",
            "Step [7950/75490], Loss: 6.4010, Time: 2.11 seconds, Average Loss: 7.4409, Learning Rate: 7e-06\n",
            "Step [7960/75490], Loss: 7.3394, Time: 1.86 seconds, Average Loss: 7.4403, Learning Rate: 7e-06\n",
            "Step [7970/75490], Loss: 4.5706, Time: 1.54 seconds, Average Loss: 7.4391, Learning Rate: 7e-06\n",
            "Step [7980/75490], Loss: 6.9182, Time: 1.41 seconds, Average Loss: 7.4387, Learning Rate: 7e-06\n",
            "Step [7990/75490], Loss: 6.7796, Time: 1.53 seconds, Average Loss: 7.4376, Learning Rate: 7e-06\n",
            "Step [8000/75490], Loss: 6.6675, Time: 1.54 seconds, Average Loss: 7.4368, Learning Rate: 7e-06\n",
            "Step [8010/75490], Loss: 8.2492, Time: 1.81 seconds, Average Loss: 7.4367, Learning Rate: 7e-06\n",
            "Step [8020/75490], Loss: 8.0548, Time: 1.47 seconds, Average Loss: 7.4362, Learning Rate: 7e-06\n",
            "Step [8030/75490], Loss: 8.6993, Time: 1.68 seconds, Average Loss: 7.4353, Learning Rate: 7e-06\n",
            "Step [8040/75490], Loss: 5.6363, Time: 2.16 seconds, Average Loss: 7.4346, Learning Rate: 7e-06\n",
            "Step [8050/75490], Loss: 7.9455, Time: 1.91 seconds, Average Loss: 7.4343, Learning Rate: 7e-06\n",
            "Step [8060/75490], Loss: 7.0452, Time: 1.56 seconds, Average Loss: 7.4340, Learning Rate: 7e-06\n",
            "Step [8070/75490], Loss: 4.7923, Time: 1.53 seconds, Average Loss: 7.4325, Learning Rate: 7e-06\n",
            "Step [8080/75490], Loss: 6.4305, Time: 1.40 seconds, Average Loss: 7.4322, Learning Rate: 7e-06\n",
            "Step [8090/75490], Loss: 6.6151, Time: 1.73 seconds, Average Loss: 7.4318, Learning Rate: 7e-06\n",
            "Step [8100/75490], Loss: 7.2373, Time: 1.94 seconds, Average Loss: 7.4312, Learning Rate: 7e-06\n",
            "Step [8110/75490], Loss: 4.2349, Time: 1.58 seconds, Average Loss: 7.4297, Learning Rate: 7e-06\n",
            "Step [8120/75490], Loss: 7.1087, Time: 2.20 seconds, Average Loss: 7.4290, Learning Rate: 7e-06\n",
            "Step [8130/75490], Loss: 6.1211, Time: 1.40 seconds, Average Loss: 7.4282, Learning Rate: 7e-06\n",
            "Step [8140/75490], Loss: 6.2025, Time: 2.24 seconds, Average Loss: 7.4272, Learning Rate: 7e-06\n",
            "Step [8150/75490], Loss: 7.4084, Time: 2.05 seconds, Average Loss: 7.4265, Learning Rate: 7e-06\n",
            "Step [8160/75490], Loss: 6.7419, Time: 2.20 seconds, Average Loss: 7.4252, Learning Rate: 7e-06\n",
            "Step [8170/75490], Loss: 5.4711, Time: 1.98 seconds, Average Loss: 7.4244, Learning Rate: 7e-06\n",
            "Step [8180/75490], Loss: 6.8491, Time: 1.17 seconds, Average Loss: 7.4238, Learning Rate: 7e-06\n",
            "Step [8190/75490], Loss: 4.1407, Time: 1.45 seconds, Average Loss: 7.4224, Learning Rate: 7e-06\n",
            "Step [8200/75490], Loss: 7.0110, Time: 1.39 seconds, Average Loss: 7.4210, Learning Rate: 7e-06\n",
            "Step [8210/75490], Loss: 7.0698, Time: 1.69 seconds, Average Loss: 7.4206, Learning Rate: 7e-06\n",
            "Step [8220/75490], Loss: 6.7713, Time: 1.89 seconds, Average Loss: 7.4200, Learning Rate: 7e-06\n",
            "Step [8230/75490], Loss: 7.1248, Time: 1.44 seconds, Average Loss: 7.4190, Learning Rate: 7e-06\n",
            "Step [8240/75490], Loss: 6.8208, Time: 2.33 seconds, Average Loss: 7.4183, Learning Rate: 7e-06\n",
            "Step [8250/75490], Loss: 6.3019, Time: 1.59 seconds, Average Loss: 7.4175, Learning Rate: 7e-06\n",
            "Step [8260/75490], Loss: 6.1876, Time: 1.81 seconds, Average Loss: 7.4168, Learning Rate: 7e-06\n",
            "Step [8270/75490], Loss: 5.1481, Time: 1.91 seconds, Average Loss: 7.4158, Learning Rate: 7e-06\n",
            "Step [8280/75490], Loss: 5.0149, Time: 1.73 seconds, Average Loss: 7.4152, Learning Rate: 7e-06\n",
            "Step [8290/75490], Loss: 6.9563, Time: 2.04 seconds, Average Loss: 7.4144, Learning Rate: 7e-06\n",
            "Step [8300/75490], Loss: 7.6890, Time: 2.17 seconds, Average Loss: 7.4137, Learning Rate: 7e-06\n",
            "Step [8310/75490], Loss: 7.4870, Time: 1.71 seconds, Average Loss: 7.4128, Learning Rate: 7e-06\n",
            "Step [8320/75490], Loss: 6.2448, Time: 1.69 seconds, Average Loss: 7.4119, Learning Rate: 7e-06\n",
            "Step [8330/75490], Loss: 7.0136, Time: 1.55 seconds, Average Loss: 7.4107, Learning Rate: 7e-06\n",
            "Step [8340/75490], Loss: 7.4408, Time: 1.57 seconds, Average Loss: 7.4102, Learning Rate: 7e-06\n",
            "Step [8350/75490], Loss: 6.7546, Time: 1.35 seconds, Average Loss: 7.4093, Learning Rate: 7e-06\n",
            "Step [8360/75490], Loss: 5.7608, Time: 1.63 seconds, Average Loss: 7.4088, Learning Rate: 7e-06\n",
            "Step [8370/75490], Loss: 8.5945, Time: 1.78 seconds, Average Loss: 7.4084, Learning Rate: 7e-06\n",
            "Step [8380/75490], Loss: 6.9814, Time: 1.72 seconds, Average Loss: 7.4073, Learning Rate: 7e-06\n",
            "Step [8390/75490], Loss: 7.9776, Time: 2.19 seconds, Average Loss: 7.4069, Learning Rate: 7e-06\n",
            "Step [8400/75490], Loss: 6.6237, Time: 1.72 seconds, Average Loss: 7.4066, Learning Rate: 7e-06\n",
            "Step [8410/75490], Loss: 7.0313, Time: 1.50 seconds, Average Loss: 7.4053, Learning Rate: 7e-06\n",
            "Step [8420/75490], Loss: 6.4011, Time: 1.94 seconds, Average Loss: 7.4042, Learning Rate: 7e-06\n",
            "Step [8430/75490], Loss: 7.0640, Time: 2.03 seconds, Average Loss: 7.4032, Learning Rate: 7e-06\n",
            "Step [8440/75490], Loss: 7.0843, Time: 2.23 seconds, Average Loss: 7.4027, Learning Rate: 7e-06\n",
            "Step [8450/75490], Loss: 5.8944, Time: 1.69 seconds, Average Loss: 7.4019, Learning Rate: 7e-06\n",
            "Step [8460/75490], Loss: 7.7568, Time: 1.84 seconds, Average Loss: 7.4011, Learning Rate: 7e-06\n",
            "Step [8470/75490], Loss: 7.6342, Time: 1.61 seconds, Average Loss: 7.4009, Learning Rate: 7e-06\n",
            "Step [8480/75490], Loss: 6.8870, Time: 1.56 seconds, Average Loss: 7.3997, Learning Rate: 7e-06\n",
            "Step [8490/75490], Loss: 5.6243, Time: 1.63 seconds, Average Loss: 7.3987, Learning Rate: 7e-06\n",
            "Step [8500/75490], Loss: 6.5186, Time: 2.02 seconds, Average Loss: 7.3979, Learning Rate: 7e-06\n",
            "Step [8510/75490], Loss: 7.3006, Time: 2.01 seconds, Average Loss: 7.3966, Learning Rate: 7e-06\n",
            "Step [8520/75490], Loss: 7.2339, Time: 1.27 seconds, Average Loss: 7.3956, Learning Rate: 7e-06\n",
            "Step [8530/75490], Loss: 5.6359, Time: 1.28 seconds, Average Loss: 7.3943, Learning Rate: 7e-06\n",
            "Step [8540/75490], Loss: 6.5661, Time: 1.71 seconds, Average Loss: 7.3936, Learning Rate: 7e-06\n",
            "Step [8550/75490], Loss: 5.1396, Time: 1.80 seconds, Average Loss: 7.3932, Learning Rate: 7e-06\n",
            "Step [8560/75490], Loss: 6.6682, Time: 1.82 seconds, Average Loss: 7.3927, Learning Rate: 7e-06\n",
            "Step [8570/75490], Loss: 7.7891, Time: 1.59 seconds, Average Loss: 7.3923, Learning Rate: 7e-06\n",
            "Step [8580/75490], Loss: 7.1243, Time: 1.68 seconds, Average Loss: 7.3911, Learning Rate: 7e-06\n",
            "Step [8590/75490], Loss: 5.9068, Time: 2.21 seconds, Average Loss: 7.3901, Learning Rate: 7e-06\n",
            "Step [8600/75490], Loss: 7.3497, Time: 1.69 seconds, Average Loss: 7.3889, Learning Rate: 7e-06\n",
            "Step [8610/75490], Loss: 6.5051, Time: 1.74 seconds, Average Loss: 7.3876, Learning Rate: 7e-06\n",
            "Step [8620/75490], Loss: 5.7003, Time: 2.07 seconds, Average Loss: 7.3866, Learning Rate: 7e-06\n",
            "Step [8630/75490], Loss: 7.9668, Time: 1.61 seconds, Average Loss: 7.3861, Learning Rate: 7e-06\n",
            "Step [8640/75490], Loss: 7.0188, Time: 1.66 seconds, Average Loss: 7.3850, Learning Rate: 7e-06\n",
            "Step [8650/75490], Loss: 7.3007, Time: 1.99 seconds, Average Loss: 7.3846, Learning Rate: 7e-06\n",
            "Step [8660/75490], Loss: 6.1208, Time: 1.88 seconds, Average Loss: 7.3841, Learning Rate: 7e-06\n",
            "Step [8670/75490], Loss: 7.1638, Time: 1.59 seconds, Average Loss: 7.3834, Learning Rate: 7e-06\n",
            "Step [8680/75490], Loss: 6.9228, Time: 1.66 seconds, Average Loss: 7.3829, Learning Rate: 7e-06\n",
            "Step [8690/75490], Loss: 6.7370, Time: 1.65 seconds, Average Loss: 7.3816, Learning Rate: 7e-06\n",
            "Step [8700/75490], Loss: 7.0265, Time: 1.75 seconds, Average Loss: 7.3809, Learning Rate: 7e-06\n",
            "Step [8710/75490], Loss: 6.4249, Time: 1.36 seconds, Average Loss: 7.3803, Learning Rate: 7e-06\n",
            "Step [8720/75490], Loss: 6.9240, Time: 1.94 seconds, Average Loss: 7.3794, Learning Rate: 7e-06\n",
            "Step [8730/75490], Loss: 5.8018, Time: 1.85 seconds, Average Loss: 7.3788, Learning Rate: 7e-06\n",
            "Step [8740/75490], Loss: 6.7632, Time: 1.93 seconds, Average Loss: 7.3782, Learning Rate: 7e-06\n",
            "Step [8750/75490], Loss: 6.3044, Time: 1.94 seconds, Average Loss: 7.3773, Learning Rate: 7e-06\n",
            "Step [8760/75490], Loss: 6.9851, Time: 2.11 seconds, Average Loss: 7.3765, Learning Rate: 7e-06\n",
            "Step [8770/75490], Loss: 6.4298, Time: 1.19 seconds, Average Loss: 7.3751, Learning Rate: 7e-06\n",
            "Step [8780/75490], Loss: 6.5175, Time: 1.58 seconds, Average Loss: 7.3747, Learning Rate: 7e-06\n",
            "Step [8790/75490], Loss: 6.8870, Time: 1.68 seconds, Average Loss: 7.3744, Learning Rate: 7e-06\n",
            "Step [8800/75490], Loss: 7.0725, Time: 1.54 seconds, Average Loss: 7.3737, Learning Rate: 7e-06\n",
            "Step [8810/75490], Loss: 4.1388, Time: 1.74 seconds, Average Loss: 7.3727, Learning Rate: 7e-06\n",
            "Step [8820/75490], Loss: 7.4677, Time: 1.88 seconds, Average Loss: 7.3716, Learning Rate: 7e-06\n",
            "Step [8830/75490], Loss: 6.3606, Time: 1.81 seconds, Average Loss: 7.3711, Learning Rate: 7e-06\n",
            "Step [8840/75490], Loss: 6.4408, Time: 2.41 seconds, Average Loss: 7.3705, Learning Rate: 7e-06\n",
            "Step [8850/75490], Loss: 6.3091, Time: 1.38 seconds, Average Loss: 7.3694, Learning Rate: 7e-06\n",
            "Step [8860/75490], Loss: 7.0563, Time: 1.55 seconds, Average Loss: 7.3686, Learning Rate: 7e-06\n",
            "Step [8870/75490], Loss: 7.3748, Time: 1.57 seconds, Average Loss: 7.3682, Learning Rate: 7e-06\n",
            "Step [8880/75490], Loss: 6.3859, Time: 2.26 seconds, Average Loss: 7.3671, Learning Rate: 7e-06\n",
            "Step [8890/75490], Loss: 8.1305, Time: 3.43 seconds, Average Loss: 7.3669, Learning Rate: 7e-06\n",
            "Step [8900/75490], Loss: 6.4507, Time: 1.72 seconds, Average Loss: 7.3661, Learning Rate: 7e-06\n",
            "Step [8910/75490], Loss: 8.0575, Time: 1.58 seconds, Average Loss: 7.3654, Learning Rate: 7e-06\n",
            "Step [8920/75490], Loss: 6.2608, Time: 1.63 seconds, Average Loss: 7.3649, Learning Rate: 7e-06\n",
            "Step [8930/75490], Loss: 6.4465, Time: 2.29 seconds, Average Loss: 7.3638, Learning Rate: 7e-06\n",
            "Step [8940/75490], Loss: 6.6764, Time: 1.09 seconds, Average Loss: 7.3628, Learning Rate: 7e-06\n",
            "Step [8950/75490], Loss: 7.0383, Time: 1.54 seconds, Average Loss: 7.3622, Learning Rate: 7e-06\n",
            "Step [8960/75490], Loss: 6.0413, Time: 1.90 seconds, Average Loss: 7.3615, Learning Rate: 7e-06\n",
            "Step [8970/75490], Loss: 6.0778, Time: 1.90 seconds, Average Loss: 7.3608, Learning Rate: 7e-06\n",
            "Step [8980/75490], Loss: 6.7200, Time: 1.74 seconds, Average Loss: 7.3595, Learning Rate: 7e-06\n",
            "Step [8990/75490], Loss: 7.0360, Time: 1.63 seconds, Average Loss: 7.3589, Learning Rate: 7e-06\n",
            "Step [9000/75490], Loss: 5.6799, Time: 1.56 seconds, Average Loss: 7.3582, Learning Rate: 7e-06\n",
            "Step [9010/75490], Loss: 7.0644, Time: 1.85 seconds, Average Loss: 7.3568, Learning Rate: 7e-06\n",
            "Step [9020/75490], Loss: 5.9038, Time: 1.69 seconds, Average Loss: 7.3557, Learning Rate: 7e-06\n",
            "Step [9030/75490], Loss: 6.5354, Time: 1.97 seconds, Average Loss: 7.3542, Learning Rate: 7e-06\n",
            "Step [9040/75490], Loss: 7.7386, Time: 1.52 seconds, Average Loss: 7.3538, Learning Rate: 7e-06\n",
            "Step [9050/75490], Loss: 6.5084, Time: 1.48 seconds, Average Loss: 7.3532, Learning Rate: 7e-06\n",
            "Step [9060/75490], Loss: 6.8235, Time: 1.45 seconds, Average Loss: 7.3531, Learning Rate: 7e-06\n",
            "Step [9070/75490], Loss: 7.3453, Time: 1.99 seconds, Average Loss: 7.3525, Learning Rate: 7e-06\n",
            "Step [9080/75490], Loss: 6.9140, Time: 1.70 seconds, Average Loss: 7.3520, Learning Rate: 7e-06\n",
            "Step [9090/75490], Loss: 8.8080, Time: 1.45 seconds, Average Loss: 7.3523, Learning Rate: 7e-06\n",
            "Step [9100/75490], Loss: 6.8665, Time: 1.89 seconds, Average Loss: 7.3515, Learning Rate: 7e-06\n",
            "Step [9110/75490], Loss: 8.6927, Time: 1.77 seconds, Average Loss: 7.3509, Learning Rate: 7e-06\n",
            "Step [9120/75490], Loss: 6.9729, Time: 1.56 seconds, Average Loss: 7.3508, Learning Rate: 7e-06\n",
            "Step [9130/75490], Loss: 6.5749, Time: 1.83 seconds, Average Loss: 7.3501, Learning Rate: 7e-06\n",
            "Step [9140/75490], Loss: 6.3674, Time: 1.47 seconds, Average Loss: 7.3490, Learning Rate: 7e-06\n",
            "Step [9150/75490], Loss: 7.1080, Time: 1.75 seconds, Average Loss: 7.3484, Learning Rate: 7e-06\n",
            "Step [9160/75490], Loss: 6.2490, Time: 1.96 seconds, Average Loss: 7.3476, Learning Rate: 7e-06\n",
            "Step [9170/75490], Loss: 4.7959, Time: 2.32 seconds, Average Loss: 7.3472, Learning Rate: 7e-06\n",
            "Step [9180/75490], Loss: 5.5710, Time: 1.71 seconds, Average Loss: 7.3463, Learning Rate: 7e-06\n",
            "Step [9190/75490], Loss: 7.3021, Time: 1.27 seconds, Average Loss: 7.3456, Learning Rate: 7e-06\n",
            "Step [9200/75490], Loss: 5.2361, Time: 1.55 seconds, Average Loss: 7.3449, Learning Rate: 7e-06\n",
            "Step [9210/75490], Loss: 7.7795, Time: 1.52 seconds, Average Loss: 7.3440, Learning Rate: 7e-06\n",
            "Step [9220/75490], Loss: 6.6104, Time: 1.57 seconds, Average Loss: 7.3431, Learning Rate: 7e-06\n",
            "Step [9230/75490], Loss: 7.1297, Time: 1.63 seconds, Average Loss: 7.3426, Learning Rate: 7e-06\n",
            "Step [9240/75490], Loss: 7.2216, Time: 1.69 seconds, Average Loss: 7.3417, Learning Rate: 7e-06\n",
            "Step [9250/75490], Loss: 9.3183, Time: 1.73 seconds, Average Loss: 7.3416, Learning Rate: 7e-06\n",
            "Step [9260/75490], Loss: 6.9258, Time: 1.52 seconds, Average Loss: 7.3408, Learning Rate: 7e-06\n",
            "Step [9270/75490], Loss: 5.8517, Time: 1.36 seconds, Average Loss: 7.3398, Learning Rate: 7e-06\n",
            "Step [9280/75490], Loss: 4.2847, Time: 1.90 seconds, Average Loss: 7.3385, Learning Rate: 7e-06\n",
            "Step [9290/75490], Loss: 4.9638, Time: 1.88 seconds, Average Loss: 7.3370, Learning Rate: 7e-06\n",
            "Step [9300/75490], Loss: 7.0133, Time: 1.22 seconds, Average Loss: 7.3367, Learning Rate: 7e-06\n",
            "Step [9310/75490], Loss: 6.3124, Time: 1.70 seconds, Average Loss: 7.3363, Learning Rate: 7e-06\n",
            "Step [9320/75490], Loss: 6.7528, Time: 1.75 seconds, Average Loss: 7.3358, Learning Rate: 7e-06\n",
            "Step [9330/75490], Loss: 7.5033, Time: 1.89 seconds, Average Loss: 7.3350, Learning Rate: 7e-06\n",
            "Step [9340/75490], Loss: 7.8041, Time: 1.72 seconds, Average Loss: 7.3343, Learning Rate: 7e-06\n",
            "Step [9350/75490], Loss: 4.0891, Time: 1.73 seconds, Average Loss: 7.3334, Learning Rate: 7e-06\n",
            "Step [9360/75490], Loss: 7.7459, Time: 1.39 seconds, Average Loss: 7.3322, Learning Rate: 7e-06\n",
            "Step [9370/75490], Loss: 7.9322, Time: 1.57 seconds, Average Loss: 7.3316, Learning Rate: 7e-06\n",
            "Step [9380/75490], Loss: 6.9495, Time: 1.73 seconds, Average Loss: 7.3313, Learning Rate: 7e-06\n",
            "Step [9390/75490], Loss: 6.4783, Time: 1.67 seconds, Average Loss: 7.3302, Learning Rate: 7e-06\n",
            "Step [9400/75490], Loss: 7.3544, Time: 2.12 seconds, Average Loss: 7.3294, Learning Rate: 7e-06\n",
            "Step [9410/75490], Loss: 7.2852, Time: 1.37 seconds, Average Loss: 7.3288, Learning Rate: 7e-06\n",
            "Step [9420/75490], Loss: 7.7982, Time: 1.69 seconds, Average Loss: 7.3281, Learning Rate: 7e-06\n",
            "Step [9430/75490], Loss: 5.2145, Time: 1.57 seconds, Average Loss: 7.3277, Learning Rate: 7e-06\n",
            "Step [9440/75490], Loss: 5.8078, Time: 1.96 seconds, Average Loss: 7.3272, Learning Rate: 7e-06\n",
            "Step [9450/75490], Loss: 5.5648, Time: 1.71 seconds, Average Loss: 7.3266, Learning Rate: 7e-06\n",
            "Step [9460/75490], Loss: 7.0139, Time: 1.70 seconds, Average Loss: 7.3257, Learning Rate: 7e-06\n",
            "Step [9470/75490], Loss: 5.4373, Time: 1.85 seconds, Average Loss: 7.3251, Learning Rate: 7e-06\n",
            "Step [9480/75490], Loss: 6.3871, Time: 1.85 seconds, Average Loss: 7.3247, Learning Rate: 7e-06\n",
            "Step [9490/75490], Loss: 7.0210, Time: 1.98 seconds, Average Loss: 7.3242, Learning Rate: 7e-06\n",
            "Step [9500/75490], Loss: 8.4562, Time: 2.71 seconds, Average Loss: 7.3243, Learning Rate: 7e-06\n",
            "Step [9510/75490], Loss: 7.1856, Time: 1.53 seconds, Average Loss: 7.3236, Learning Rate: 7e-06\n",
            "Step [9520/75490], Loss: 6.5413, Time: 1.78 seconds, Average Loss: 7.3221, Learning Rate: 7e-06\n",
            "Step [9530/75490], Loss: 7.2693, Time: 1.80 seconds, Average Loss: 7.3219, Learning Rate: 7e-06\n",
            "Step [9540/75490], Loss: 6.0003, Time: 2.03 seconds, Average Loss: 7.3214, Learning Rate: 7e-06\n",
            "Step [9550/75490], Loss: 7.3313, Time: 1.31 seconds, Average Loss: 7.3206, Learning Rate: 7e-06\n",
            "Step [9560/75490], Loss: 5.0489, Time: 2.06 seconds, Average Loss: 7.3198, Learning Rate: 7e-06\n",
            "Step [9570/75490], Loss: 5.5007, Time: 1.73 seconds, Average Loss: 7.3190, Learning Rate: 7e-06\n",
            "Step [9580/75490], Loss: 7.0690, Time: 1.55 seconds, Average Loss: 7.3186, Learning Rate: 7e-06\n",
            "Step [9590/75490], Loss: 6.9128, Time: 1.73 seconds, Average Loss: 7.3176, Learning Rate: 7e-06\n",
            "Step [9600/75490], Loss: 7.3004, Time: 1.57 seconds, Average Loss: 7.3173, Learning Rate: 7e-06\n",
            "Step [9610/75490], Loss: 7.5177, Time: 1.28 seconds, Average Loss: 7.3172, Learning Rate: 7e-06\n",
            "Step [9620/75490], Loss: 6.2528, Time: 1.38 seconds, Average Loss: 7.3161, Learning Rate: 7e-06\n",
            "Step [9630/75490], Loss: 6.2663, Time: 1.60 seconds, Average Loss: 7.3154, Learning Rate: 7e-06\n",
            "Step [9640/75490], Loss: 7.5821, Time: 1.72 seconds, Average Loss: 7.3148, Learning Rate: 7e-06\n",
            "Step [9650/75490], Loss: 4.8370, Time: 2.24 seconds, Average Loss: 7.3139, Learning Rate: 7e-06\n",
            "Step [9660/75490], Loss: 5.7105, Time: 1.88 seconds, Average Loss: 7.3132, Learning Rate: 7e-06\n",
            "Step [9670/75490], Loss: 7.4715, Time: 1.69 seconds, Average Loss: 7.3127, Learning Rate: 7e-06\n",
            "Step [9680/75490], Loss: 5.8199, Time: 1.42 seconds, Average Loss: 7.3122, Learning Rate: 7e-06\n",
            "Step [9690/75490], Loss: 7.6461, Time: 1.45 seconds, Average Loss: 7.3119, Learning Rate: 7e-06\n",
            "Step [9700/75490], Loss: 6.3889, Time: 1.81 seconds, Average Loss: 7.3112, Learning Rate: 7e-06\n",
            "Step [9710/75490], Loss: 7.3506, Time: 1.43 seconds, Average Loss: 7.3104, Learning Rate: 7e-06\n",
            "Step [9720/75490], Loss: 7.3953, Time: 1.89 seconds, Average Loss: 7.3097, Learning Rate: 7e-06\n",
            "Step [9730/75490], Loss: 5.9750, Time: 1.60 seconds, Average Loss: 7.3090, Learning Rate: 7e-06\n",
            "Step [9740/75490], Loss: 6.4139, Time: 1.69 seconds, Average Loss: 7.3082, Learning Rate: 7e-06\n",
            "Step [9750/75490], Loss: 6.8455, Time: 1.38 seconds, Average Loss: 7.3081, Learning Rate: 7e-06\n",
            "Step [9760/75490], Loss: 7.4446, Time: 1.51 seconds, Average Loss: 7.3070, Learning Rate: 7e-06\n",
            "Step [9770/75490], Loss: 7.1448, Time: 1.75 seconds, Average Loss: 7.3061, Learning Rate: 7e-06\n",
            "Step [9780/75490], Loss: 6.9133, Time: 1.99 seconds, Average Loss: 7.3060, Learning Rate: 7e-06\n",
            "Step [9790/75490], Loss: 6.6321, Time: 1.87 seconds, Average Loss: 7.3048, Learning Rate: 7e-06\n",
            "Step [9800/75490], Loss: 6.5906, Time: 1.73 seconds, Average Loss: 7.3040, Learning Rate: 7e-06\n",
            "Step [9810/75490], Loss: 4.1338, Time: 1.46 seconds, Average Loss: 7.3031, Learning Rate: 7e-06\n",
            "Step [9820/75490], Loss: 5.1336, Time: 1.74 seconds, Average Loss: 7.3025, Learning Rate: 7e-06\n",
            "Step [9830/75490], Loss: 6.6067, Time: 1.50 seconds, Average Loss: 7.3019, Learning Rate: 7e-06\n",
            "Step [9840/75490], Loss: 6.9200, Time: 1.42 seconds, Average Loss: 7.3015, Learning Rate: 7e-06\n",
            "Step [9850/75490], Loss: 7.1144, Time: 1.76 seconds, Average Loss: 7.3009, Learning Rate: 7e-06\n",
            "Step [9860/75490], Loss: 8.4152, Time: 1.69 seconds, Average Loss: 7.3010, Learning Rate: 7e-06\n",
            "Step [9870/75490], Loss: 5.7807, Time: 1.97 seconds, Average Loss: 7.3006, Learning Rate: 7e-06\n",
            "Step [9880/75490], Loss: 6.6837, Time: 2.09 seconds, Average Loss: 7.2995, Learning Rate: 7e-06\n",
            "Step [9890/75490], Loss: 5.9124, Time: 2.06 seconds, Average Loss: 7.2989, Learning Rate: 7e-06\n",
            "Step [9900/75490], Loss: 7.1899, Time: 1.72 seconds, Average Loss: 7.2982, Learning Rate: 7e-06\n",
            "Step [9910/75490], Loss: 6.7472, Time: 1.25 seconds, Average Loss: 7.2975, Learning Rate: 7e-06\n",
            "Step [9920/75490], Loss: 7.7663, Time: 1.83 seconds, Average Loss: 7.2969, Learning Rate: 7e-06\n",
            "Step [9930/75490], Loss: 6.1110, Time: 1.39 seconds, Average Loss: 7.2962, Learning Rate: 7e-06\n",
            "Step [9940/75490], Loss: 7.1160, Time: 1.83 seconds, Average Loss: 7.2956, Learning Rate: 7e-06\n",
            "Step [9950/75490], Loss: 3.7042, Time: 1.52 seconds, Average Loss: 7.2947, Learning Rate: 7e-06\n",
            "Step [9960/75490], Loss: 6.6471, Time: 1.28 seconds, Average Loss: 7.2940, Learning Rate: 7e-06\n",
            "Step [9970/75490], Loss: 6.1946, Time: 1.85 seconds, Average Loss: 7.2934, Learning Rate: 7e-06\n",
            "Step [9980/75490], Loss: 8.2655, Time: 1.32 seconds, Average Loss: 7.2929, Learning Rate: 7e-06\n",
            "Step [9990/75490], Loss: 7.2627, Time: 1.42 seconds, Average Loss: 7.2917, Learning Rate: 7e-06\n",
            "Step [10000/75490], Loss: 7.3884, Time: 1.75 seconds, Average Loss: 7.2915, Learning Rate: 7e-06\n",
            "Step [10010/75490], Loss: 7.0125, Time: 1.90 seconds, Average Loss: 7.2913, Learning Rate: 7e-06\n",
            "Step [10020/75490], Loss: 6.6919, Time: 1.49 seconds, Average Loss: 7.2905, Learning Rate: 7e-06\n",
            "Step [10030/75490], Loss: 10.6722, Time: 1.43 seconds, Average Loss: 7.2908, Learning Rate: 7e-06\n",
            "Step [10040/75490], Loss: 6.9239, Time: 1.71 seconds, Average Loss: 7.2902, Learning Rate: 7e-06\n",
            "Step [10050/75490], Loss: 7.6943, Time: 1.56 seconds, Average Loss: 7.2895, Learning Rate: 7e-06\n",
            "Step [10060/75490], Loss: 6.5337, Time: 1.71 seconds, Average Loss: 7.2892, Learning Rate: 7e-06\n",
            "Step [10070/75490], Loss: 6.8664, Time: 1.44 seconds, Average Loss: 7.2881, Learning Rate: 7e-06\n",
            "Step [10080/75490], Loss: 6.7730, Time: 1.98 seconds, Average Loss: 7.2871, Learning Rate: 7e-06\n",
            "Step [10090/75490], Loss: 7.7705, Time: 2.12 seconds, Average Loss: 7.2867, Learning Rate: 7e-06\n",
            "Step [10100/75490], Loss: 6.7264, Time: 1.56 seconds, Average Loss: 7.2865, Learning Rate: 7e-06\n",
            "Step [10110/75490], Loss: 5.7067, Time: 1.55 seconds, Average Loss: 7.2860, Learning Rate: 7e-06\n",
            "Step [10120/75490], Loss: 5.5796, Time: 1.71 seconds, Average Loss: 7.2850, Learning Rate: 7e-06\n",
            "Step [10130/75490], Loss: 7.1723, Time: 1.59 seconds, Average Loss: 7.2837, Learning Rate: 7e-06\n",
            "Step [10140/75490], Loss: 6.5172, Time: 1.72 seconds, Average Loss: 7.2832, Learning Rate: 7e-06\n",
            "Step [10150/75490], Loss: 6.2007, Time: 1.79 seconds, Average Loss: 7.2823, Learning Rate: 7e-06\n",
            "Step [10160/75490], Loss: 7.0296, Time: 1.87 seconds, Average Loss: 7.2820, Learning Rate: 7e-06\n",
            "Step [10170/75490], Loss: 7.7744, Time: 1.82 seconds, Average Loss: 7.2817, Learning Rate: 7e-06\n",
            "Step [10180/75490], Loss: 5.7346, Time: 1.39 seconds, Average Loss: 7.2816, Learning Rate: 7e-06\n",
            "Step [10190/75490], Loss: 6.5932, Time: 1.52 seconds, Average Loss: 7.2809, Learning Rate: 7e-06\n",
            "Step [10200/75490], Loss: 7.7666, Time: 1.73 seconds, Average Loss: 7.2804, Learning Rate: 7e-06\n",
            "Step [10210/75490], Loss: 6.5813, Time: 1.89 seconds, Average Loss: 7.2802, Learning Rate: 7e-06\n",
            "Step [10220/75490], Loss: 8.0103, Time: 1.49 seconds, Average Loss: 7.2798, Learning Rate: 7e-06\n",
            "Step [10230/75490], Loss: 6.7506, Time: 2.16 seconds, Average Loss: 7.2785, Learning Rate: 7e-06\n",
            "Step [10240/75490], Loss: 7.2855, Time: 1.79 seconds, Average Loss: 7.2778, Learning Rate: 7e-06\n",
            "Step [10250/75490], Loss: 7.6231, Time: 1.84 seconds, Average Loss: 7.2775, Learning Rate: 7e-06\n",
            "Step [10260/75490], Loss: 5.8356, Time: 2.01 seconds, Average Loss: 7.2768, Learning Rate: 7e-06\n",
            "Step [10270/75490], Loss: 6.5048, Time: 1.70 seconds, Average Loss: 7.2762, Learning Rate: 7e-06\n",
            "Step [10280/75490], Loss: 4.0508, Time: 1.39 seconds, Average Loss: 7.2756, Learning Rate: 7e-06\n",
            "Step [10290/75490], Loss: 6.6713, Time: 1.54 seconds, Average Loss: 7.2748, Learning Rate: 7e-06\n",
            "Step [10300/75490], Loss: 6.4832, Time: 1.39 seconds, Average Loss: 7.2745, Learning Rate: 7e-06\n",
            "Step [10310/75490], Loss: 5.6923, Time: 1.95 seconds, Average Loss: 7.2736, Learning Rate: 7e-06\n",
            "Step [10320/75490], Loss: 6.0628, Time: 1.81 seconds, Average Loss: 7.2728, Learning Rate: 7e-06\n",
            "Step [10330/75490], Loss: 6.6872, Time: 1.43 seconds, Average Loss: 7.2718, Learning Rate: 7e-06\n",
            "Step [10340/75490], Loss: 7.0031, Time: 1.24 seconds, Average Loss: 7.2710, Learning Rate: 7e-06\n",
            "Step [10350/75490], Loss: 5.4861, Time: 1.70 seconds, Average Loss: 7.2699, Learning Rate: 7e-06\n",
            "Step [10360/75490], Loss: 6.1624, Time: 1.90 seconds, Average Loss: 7.2692, Learning Rate: 7e-06\n",
            "Step [10370/75490], Loss: 5.4384, Time: 1.38 seconds, Average Loss: 7.2680, Learning Rate: 7e-06\n",
            "Step [10380/75490], Loss: 6.4055, Time: 1.54 seconds, Average Loss: 7.2671, Learning Rate: 7e-06\n",
            "Step [10390/75490], Loss: 6.7285, Time: 1.94 seconds, Average Loss: 7.2665, Learning Rate: 7e-06\n",
            "Step [10400/75490], Loss: 9.9411, Time: 2.31 seconds, Average Loss: 7.2663, Learning Rate: 7e-06\n",
            "Step [10410/75490], Loss: 7.7164, Time: 1.78 seconds, Average Loss: 7.2659, Learning Rate: 7e-06\n",
            "Step [10420/75490], Loss: 6.8285, Time: 1.87 seconds, Average Loss: 7.2659, Learning Rate: 7e-06\n",
            "Step [10430/75490], Loss: 5.7871, Time: 1.37 seconds, Average Loss: 7.2657, Learning Rate: 7e-06\n",
            "Step [10440/75490], Loss: 10.7330, Time: 2.19 seconds, Average Loss: 7.2659, Learning Rate: 7e-06\n",
            "Step [10450/75490], Loss: 9.4121, Time: 1.73 seconds, Average Loss: 7.2652, Learning Rate: 7e-06\n",
            "Step [10460/75490], Loss: 7.3558, Time: 1.84 seconds, Average Loss: 7.2646, Learning Rate: 7e-06\n",
            "Step [10470/75490], Loss: 4.1565, Time: 2.09 seconds, Average Loss: 7.2642, Learning Rate: 7e-06\n",
            "Step [10480/75490], Loss: 6.4057, Time: 1.87 seconds, Average Loss: 7.2636, Learning Rate: 7e-06\n",
            "Step [10490/75490], Loss: 7.7964, Time: 1.96 seconds, Average Loss: 7.2631, Learning Rate: 7e-06\n",
            "Step [10500/75490], Loss: 6.3053, Time: 1.80 seconds, Average Loss: 7.2631, Learning Rate: 7e-06\n",
            "Step [10510/75490], Loss: 5.9187, Time: 2.19 seconds, Average Loss: 7.2621, Learning Rate: 7e-06\n",
            "Step [10520/75490], Loss: 5.9547, Time: 1.99 seconds, Average Loss: 7.2616, Learning Rate: 7e-06\n",
            "Step [10530/75490], Loss: 5.9197, Time: 1.39 seconds, Average Loss: 7.2611, Learning Rate: 7e-06\n",
            "Step [10540/75490], Loss: 6.6845, Time: 2.28 seconds, Average Loss: 7.2602, Learning Rate: 7e-06\n",
            "Step [10550/75490], Loss: 6.3750, Time: 1.92 seconds, Average Loss: 7.2598, Learning Rate: 7e-06\n",
            "Step [10560/75490], Loss: 7.7525, Time: 1.97 seconds, Average Loss: 7.2592, Learning Rate: 7e-06\n",
            "Step [10570/75490], Loss: 6.2188, Time: 1.51 seconds, Average Loss: 7.2585, Learning Rate: 7e-06\n",
            "Step [10580/75490], Loss: 6.3188, Time: 1.53 seconds, Average Loss: 7.2580, Learning Rate: 7e-06\n",
            "Step [10590/75490], Loss: 8.0422, Time: 1.98 seconds, Average Loss: 7.2575, Learning Rate: 7e-06\n",
            "Step [10600/75490], Loss: 5.7213, Time: 1.70 seconds, Average Loss: 7.2567, Learning Rate: 7e-06\n",
            "Step [10610/75490], Loss: 6.5263, Time: 1.45 seconds, Average Loss: 7.2564, Learning Rate: 7e-06\n",
            "Step [10620/75490], Loss: 6.3605, Time: 1.42 seconds, Average Loss: 7.2557, Learning Rate: 7e-06\n",
            "Step [10630/75490], Loss: 7.3299, Time: 2.07 seconds, Average Loss: 7.2552, Learning Rate: 7e-06\n",
            "Step [10640/75490], Loss: 7.6479, Time: 1.62 seconds, Average Loss: 7.2550, Learning Rate: 7e-06\n",
            "Step [10650/75490], Loss: 3.4723, Time: 1.87 seconds, Average Loss: 7.2542, Learning Rate: 7e-06\n",
            "Step [10660/75490], Loss: 7.5856, Time: 1.23 seconds, Average Loss: 7.2535, Learning Rate: 7e-06\n",
            "Step [10670/75490], Loss: 7.0782, Time: 1.65 seconds, Average Loss: 7.2534, Learning Rate: 7e-06\n",
            "Step [10680/75490], Loss: 5.7271, Time: 1.83 seconds, Average Loss: 7.2529, Learning Rate: 7e-06\n",
            "Step [10690/75490], Loss: 6.6339, Time: 2.02 seconds, Average Loss: 7.2523, Learning Rate: 7e-06\n",
            "Step [10700/75490], Loss: 7.5125, Time: 1.84 seconds, Average Loss: 7.2519, Learning Rate: 7e-06\n",
            "Step [10710/75490], Loss: 6.8581, Time: 2.05 seconds, Average Loss: 7.2515, Learning Rate: 7e-06\n",
            "Step [10720/75490], Loss: 6.7721, Time: 2.13 seconds, Average Loss: 7.2512, Learning Rate: 7e-06\n",
            "Step [10730/75490], Loss: 6.0577, Time: 2.08 seconds, Average Loss: 7.2505, Learning Rate: 7e-06\n",
            "Step [10740/75490], Loss: 3.8683, Time: 2.11 seconds, Average Loss: 7.2494, Learning Rate: 7e-06\n",
            "Step [10750/75490], Loss: 7.5263, Time: 1.57 seconds, Average Loss: 7.2486, Learning Rate: 7e-06\n",
            "Step [10760/75490], Loss: 6.5387, Time: 1.41 seconds, Average Loss: 7.2476, Learning Rate: 7e-06\n",
            "Step [10770/75490], Loss: 7.2984, Time: 2.17 seconds, Average Loss: 7.2473, Learning Rate: 7e-06\n",
            "Step [10780/75490], Loss: 6.2524, Time: 1.64 seconds, Average Loss: 7.2469, Learning Rate: 7e-06\n",
            "Step [10790/75490], Loss: 7.1638, Time: 1.90 seconds, Average Loss: 7.2460, Learning Rate: 7e-06\n",
            "Step [10800/75490], Loss: 5.3438, Time: 2.00 seconds, Average Loss: 7.2449, Learning Rate: 7e-06\n",
            "Step [10810/75490], Loss: 7.6858, Time: 1.70 seconds, Average Loss: 7.2446, Learning Rate: 7e-06\n",
            "Step [10820/75490], Loss: 6.6714, Time: 2.15 seconds, Average Loss: 7.2443, Learning Rate: 7e-06\n",
            "Step [10830/75490], Loss: 6.6127, Time: 1.37 seconds, Average Loss: 7.2438, Learning Rate: 7e-06\n",
            "Step [10840/75490], Loss: 6.2754, Time: 2.09 seconds, Average Loss: 7.2427, Learning Rate: 7e-06\n",
            "Step [10850/75490], Loss: 6.7493, Time: 1.58 seconds, Average Loss: 7.2421, Learning Rate: 7e-06\n",
            "Step [10860/75490], Loss: 8.2844, Time: 2.32 seconds, Average Loss: 7.2416, Learning Rate: 7e-06\n",
            "Step [10870/75490], Loss: 7.2517, Time: 1.75 seconds, Average Loss: 7.2409, Learning Rate: 7e-06\n",
            "Step [10880/75490], Loss: 6.2097, Time: 1.59 seconds, Average Loss: 7.2402, Learning Rate: 7e-06\n",
            "Step [10890/75490], Loss: 6.6695, Time: 1.35 seconds, Average Loss: 7.2402, Learning Rate: 7e-06\n",
            "Step [10900/75490], Loss: 4.7230, Time: 1.94 seconds, Average Loss: 7.2395, Learning Rate: 7e-06\n",
            "Step [10910/75490], Loss: 5.9257, Time: 1.66 seconds, Average Loss: 7.2385, Learning Rate: 7e-06\n",
            "Step [10920/75490], Loss: 7.1676, Time: 1.09 seconds, Average Loss: 7.2379, Learning Rate: 7e-06\n",
            "Step [10930/75490], Loss: 6.1303, Time: 1.65 seconds, Average Loss: 7.2373, Learning Rate: 7e-06\n",
            "Step [10940/75490], Loss: 4.7751, Time: 1.47 seconds, Average Loss: 7.2367, Learning Rate: 7e-06\n",
            "Step [10950/75490], Loss: 7.2373, Time: 1.47 seconds, Average Loss: 7.2363, Learning Rate: 7e-06\n",
            "Step [10960/75490], Loss: 6.4644, Time: 1.76 seconds, Average Loss: 7.2356, Learning Rate: 7e-06\n",
            "Step [10970/75490], Loss: 7.8961, Time: 1.72 seconds, Average Loss: 7.2347, Learning Rate: 7e-06\n",
            "Step [10980/75490], Loss: 6.7340, Time: 1.21 seconds, Average Loss: 7.2341, Learning Rate: 7e-06\n",
            "Step [10990/75490], Loss: 6.4987, Time: 1.59 seconds, Average Loss: 7.2336, Learning Rate: 7e-06\n",
            "Step [11000/75490], Loss: 7.8962, Time: 1.40 seconds, Average Loss: 7.2331, Learning Rate: 7e-06\n",
            "Step [11010/75490], Loss: 6.8006, Time: 2.00 seconds, Average Loss: 7.2327, Learning Rate: 7e-06\n",
            "Step [11020/75490], Loss: 6.7830, Time: 1.85 seconds, Average Loss: 7.2327, Learning Rate: 7e-06\n",
            "Step [11030/75490], Loss: 8.0198, Time: 1.95 seconds, Average Loss: 7.2320, Learning Rate: 7e-06\n",
            "Step [11040/75490], Loss: 5.9487, Time: 2.16 seconds, Average Loss: 7.2316, Learning Rate: 7e-06\n",
            "Step [11050/75490], Loss: 7.2509, Time: 2.14 seconds, Average Loss: 7.2315, Learning Rate: 7e-06\n",
            "Step [11060/75490], Loss: 6.1699, Time: 2.09 seconds, Average Loss: 7.2306, Learning Rate: 7e-06\n",
            "Step [11070/75490], Loss: 3.9747, Time: 1.84 seconds, Average Loss: 7.2298, Learning Rate: 7e-06\n",
            "Step [11080/75490], Loss: 6.7622, Time: 1.53 seconds, Average Loss: 7.2291, Learning Rate: 7e-06\n",
            "Step [11090/75490], Loss: 6.8253, Time: 1.56 seconds, Average Loss: 7.2285, Learning Rate: 7e-06\n",
            "Step [11100/75490], Loss: 6.5711, Time: 1.78 seconds, Average Loss: 7.2280, Learning Rate: 7e-06\n",
            "Step [11110/75490], Loss: 6.7842, Time: 1.74 seconds, Average Loss: 7.2271, Learning Rate: 7e-06\n",
            "Step [11120/75490], Loss: 6.3736, Time: 1.38 seconds, Average Loss: 7.2265, Learning Rate: 7e-06\n",
            "Step [11130/75490], Loss: 6.4116, Time: 1.88 seconds, Average Loss: 7.2261, Learning Rate: 7e-06\n",
            "Step [11140/75490], Loss: 6.4360, Time: 1.73 seconds, Average Loss: 7.2255, Learning Rate: 7e-06\n",
            "Step [11150/75490], Loss: 6.6831, Time: 1.71 seconds, Average Loss: 7.2247, Learning Rate: 7e-06\n",
            "Step [11160/75490], Loss: 5.9583, Time: 1.59 seconds, Average Loss: 7.2241, Learning Rate: 7e-06\n",
            "Step [11170/75490], Loss: 6.4225, Time: 2.21 seconds, Average Loss: 7.2233, Learning Rate: 7e-06\n",
            "Step [11180/75490], Loss: 6.9943, Time: 1.43 seconds, Average Loss: 7.2228, Learning Rate: 7e-06\n",
            "Step [11190/75490], Loss: 5.1716, Time: 1.62 seconds, Average Loss: 7.2223, Learning Rate: 7e-06\n",
            "Step [11200/75490], Loss: 7.8123, Time: 1.76 seconds, Average Loss: 7.2222, Learning Rate: 7e-06\n",
            "Step [11210/75490], Loss: 6.3021, Time: 1.68 seconds, Average Loss: 7.2208, Learning Rate: 7e-06\n",
            "Step [11220/75490], Loss: 7.8047, Time: 3.54 seconds, Average Loss: 7.2203, Learning Rate: 7e-06\n",
            "Step [11230/75490], Loss: 7.4518, Time: 1.75 seconds, Average Loss: 7.2193, Learning Rate: 7e-06\n",
            "Step [11240/75490], Loss: 8.4285, Time: 1.53 seconds, Average Loss: 7.2190, Learning Rate: 7e-06\n",
            "Step [11250/75490], Loss: 7.9825, Time: 1.60 seconds, Average Loss: 7.2185, Learning Rate: 7e-06\n",
            "Step [11260/75490], Loss: 9.0365, Time: 2.11 seconds, Average Loss: 7.2181, Learning Rate: 7e-06\n",
            "Step [11270/75490], Loss: 6.9485, Time: 2.18 seconds, Average Loss: 7.2176, Learning Rate: 7e-06\n",
            "Step [11280/75490], Loss: 6.5236, Time: 1.81 seconds, Average Loss: 7.2169, Learning Rate: 7e-06\n",
            "Step [11290/75490], Loss: 6.4370, Time: 1.72 seconds, Average Loss: 7.2161, Learning Rate: 7e-06\n",
            "Step [11300/75490], Loss: 6.1958, Time: 1.41 seconds, Average Loss: 7.2153, Learning Rate: 7e-06\n",
            "Step [11310/75490], Loss: 7.2375, Time: 1.75 seconds, Average Loss: 7.2145, Learning Rate: 7e-06\n",
            "Step [11320/75490], Loss: 7.5205, Time: 1.25 seconds, Average Loss: 7.2142, Learning Rate: 7e-06\n",
            "Step [11330/75490], Loss: 6.2587, Time: 2.21 seconds, Average Loss: 7.2141, Learning Rate: 7e-06\n",
            "Step [11340/75490], Loss: 5.9931, Time: 1.92 seconds, Average Loss: 7.2135, Learning Rate: 7e-06\n",
            "Step [11350/75490], Loss: 7.4773, Time: 1.88 seconds, Average Loss: 7.2127, Learning Rate: 7e-06\n",
            "Step [11360/75490], Loss: 10.9714, Time: 1.94 seconds, Average Loss: 7.2121, Learning Rate: 7e-06\n",
            "Step [11370/75490], Loss: 6.4181, Time: 1.82 seconds, Average Loss: 7.2115, Learning Rate: 7e-06\n",
            "Step [11380/75490], Loss: 6.1017, Time: 2.23 seconds, Average Loss: 7.2110, Learning Rate: 7e-06\n",
            "Step [11390/75490], Loss: 5.4513, Time: 1.58 seconds, Average Loss: 7.2104, Learning Rate: 7e-06\n",
            "Step [11400/75490], Loss: 5.7487, Time: 1.86 seconds, Average Loss: 7.2098, Learning Rate: 7e-06\n",
            "Step [11410/75490], Loss: 5.8500, Time: 1.43 seconds, Average Loss: 7.2092, Learning Rate: 7e-06\n",
            "Step [11420/75490], Loss: 5.4051, Time: 1.27 seconds, Average Loss: 7.2078, Learning Rate: 7e-06\n",
            "Step [11430/75490], Loss: 8.4610, Time: 1.49 seconds, Average Loss: 7.2078, Learning Rate: 7e-06\n",
            "Step [11440/75490], Loss: 2.6852, Time: 1.53 seconds, Average Loss: 7.2062, Learning Rate: 7e-06\n",
            "Step [11450/75490], Loss: 5.5597, Time: 1.69 seconds, Average Loss: 7.2052, Learning Rate: 7e-06\n",
            "Step [11460/75490], Loss: 5.8995, Time: 1.48 seconds, Average Loss: 7.2045, Learning Rate: 7e-06\n",
            "Step [11470/75490], Loss: 7.0124, Time: 1.57 seconds, Average Loss: 7.2041, Learning Rate: 7e-06\n",
            "Step [11480/75490], Loss: 6.2757, Time: 1.79 seconds, Average Loss: 7.2037, Learning Rate: 7e-06\n",
            "Step [11490/75490], Loss: 6.3978, Time: 1.55 seconds, Average Loss: 7.2032, Learning Rate: 7e-06\n",
            "Step [11500/75490], Loss: 5.5030, Time: 1.90 seconds, Average Loss: 7.2029, Learning Rate: 7e-06\n",
            "Step [11510/75490], Loss: 6.3149, Time: 1.78 seconds, Average Loss: 7.2026, Learning Rate: 7e-06\n",
            "Step [11520/75490], Loss: 6.3485, Time: 1.87 seconds, Average Loss: 7.2021, Learning Rate: 7e-06\n",
            "Step [11530/75490], Loss: 5.8099, Time: 1.55 seconds, Average Loss: 7.2014, Learning Rate: 7e-06\n",
            "Step [11540/75490], Loss: 6.3351, Time: 2.11 seconds, Average Loss: 7.2015, Learning Rate: 7e-06\n",
            "Step [11550/75490], Loss: 6.6289, Time: 1.42 seconds, Average Loss: 7.2006, Learning Rate: 7e-06\n",
            "Step [11560/75490], Loss: 6.6830, Time: 2.00 seconds, Average Loss: 7.2000, Learning Rate: 7e-06\n",
            "Step [11570/75490], Loss: 6.6100, Time: 1.51 seconds, Average Loss: 7.1992, Learning Rate: 7e-06\n",
            "Step [11580/75490], Loss: 6.3407, Time: 1.45 seconds, Average Loss: 7.1986, Learning Rate: 7e-06\n",
            "Step [11590/75490], Loss: 6.5337, Time: 2.04 seconds, Average Loss: 7.1977, Learning Rate: 7e-06\n",
            "Step [11600/75490], Loss: 5.6300, Time: 1.71 seconds, Average Loss: 7.1966, Learning Rate: 7e-06\n",
            "Step [11610/75490], Loss: 7.1366, Time: 1.98 seconds, Average Loss: 7.1961, Learning Rate: 7e-06\n",
            "Step [11620/75490], Loss: 5.2126, Time: 2.17 seconds, Average Loss: 7.1955, Learning Rate: 7e-06\n",
            "Step [11630/75490], Loss: 7.2483, Time: 1.85 seconds, Average Loss: 7.1952, Learning Rate: 7e-06\n",
            "Step [11640/75490], Loss: 9.0036, Time: 1.56 seconds, Average Loss: 7.1951, Learning Rate: 7e-06\n",
            "Step [11650/75490], Loss: 4.9827, Time: 2.42 seconds, Average Loss: 7.1943, Learning Rate: 7e-06\n",
            "Step [11660/75490], Loss: 6.9063, Time: 1.26 seconds, Average Loss: 7.1934, Learning Rate: 7e-06\n",
            "Step [11670/75490], Loss: 5.6176, Time: 1.70 seconds, Average Loss: 7.1927, Learning Rate: 7e-06\n",
            "Step [11680/75490], Loss: 6.2205, Time: 1.98 seconds, Average Loss: 7.1920, Learning Rate: 7e-06\n",
            "Step [11690/75490], Loss: 7.4026, Time: 1.24 seconds, Average Loss: 7.1917, Learning Rate: 7e-06\n",
            "Step [11700/75490], Loss: 6.6471, Time: 1.40 seconds, Average Loss: 7.1913, Learning Rate: 7e-06\n",
            "Step [11710/75490], Loss: 5.5542, Time: 2.09 seconds, Average Loss: 7.1905, Learning Rate: 7e-06\n",
            "Step [11720/75490], Loss: 8.9841, Time: 2.06 seconds, Average Loss: 7.1899, Learning Rate: 7e-06\n",
            "Step [11730/75490], Loss: 6.6311, Time: 1.98 seconds, Average Loss: 7.1895, Learning Rate: 7e-06\n",
            "Step [11740/75490], Loss: 7.2949, Time: 2.23 seconds, Average Loss: 7.1888, Learning Rate: 7e-06\n",
            "Step [11750/75490], Loss: 5.5131, Time: 1.56 seconds, Average Loss: 7.1879, Learning Rate: 7e-06\n",
            "Step [11760/75490], Loss: 5.7868, Time: 1.74 seconds, Average Loss: 7.1873, Learning Rate: 7e-06\n",
            "Step [11770/75490], Loss: 5.5145, Time: 1.29 seconds, Average Loss: 7.1869, Learning Rate: 7e-06\n",
            "Step [11780/75490], Loss: 7.0890, Time: 1.86 seconds, Average Loss: 7.1868, Learning Rate: 7e-06\n",
            "Step [11790/75490], Loss: 4.9768, Time: 1.94 seconds, Average Loss: 7.1862, Learning Rate: 7e-06\n",
            "Step [11800/75490], Loss: 6.2962, Time: 1.82 seconds, Average Loss: 7.1859, Learning Rate: 7e-06\n",
            "Step [11810/75490], Loss: 6.4802, Time: 1.97 seconds, Average Loss: 7.1850, Learning Rate: 7e-06\n",
            "Step [11820/75490], Loss: 7.4108, Time: 1.68 seconds, Average Loss: 7.1850, Learning Rate: 7e-06\n",
            "Step [11830/75490], Loss: 4.8816, Time: 1.50 seconds, Average Loss: 7.1839, Learning Rate: 7e-06\n",
            "Step [11840/75490], Loss: 6.3349, Time: 1.76 seconds, Average Loss: 7.1832, Learning Rate: 7e-06\n",
            "Step [11850/75490], Loss: 7.0503, Time: 1.11 seconds, Average Loss: 7.1824, Learning Rate: 7e-06\n",
            "Step [11860/75490], Loss: 6.2558, Time: 1.82 seconds, Average Loss: 7.1816, Learning Rate: 7e-06\n",
            "Step [11870/75490], Loss: 4.8486, Time: 2.05 seconds, Average Loss: 7.1808, Learning Rate: 7e-06\n",
            "Step [11880/75490], Loss: 7.0643, Time: 1.69 seconds, Average Loss: 7.1804, Learning Rate: 7e-06\n",
            "Step [11890/75490], Loss: 6.6146, Time: 1.57 seconds, Average Loss: 7.1792, Learning Rate: 7e-06\n",
            "Step [11900/75490], Loss: 7.6021, Time: 2.10 seconds, Average Loss: 7.1787, Learning Rate: 7e-06\n",
            "Step [11910/75490], Loss: 6.5268, Time: 1.74 seconds, Average Loss: 7.1783, Learning Rate: 7e-06\n",
            "Step [11920/75490], Loss: 6.3644, Time: 1.34 seconds, Average Loss: 7.1774, Learning Rate: 7e-06\n",
            "Step [11930/75490], Loss: 6.2581, Time: 1.95 seconds, Average Loss: 7.1770, Learning Rate: 7e-06\n",
            "Step [11940/75490], Loss: 7.4135, Time: 1.37 seconds, Average Loss: 7.1768, Learning Rate: 7e-06\n",
            "Step [11950/75490], Loss: 7.0245, Time: 1.47 seconds, Average Loss: 7.1761, Learning Rate: 7e-06\n",
            "Step [11960/75490], Loss: 6.7596, Time: 1.66 seconds, Average Loss: 7.1751, Learning Rate: 7e-06\n",
            "Step [11970/75490], Loss: 5.5643, Time: 1.84 seconds, Average Loss: 7.1741, Learning Rate: 7e-06\n",
            "Step [11980/75490], Loss: 6.6623, Time: 1.86 seconds, Average Loss: 7.1733, Learning Rate: 7e-06\n",
            "Step [11990/75490], Loss: 7.0243, Time: 1.94 seconds, Average Loss: 7.1727, Learning Rate: 7e-06\n",
            "Step [12000/75490], Loss: 7.0516, Time: 1.61 seconds, Average Loss: 7.1720, Learning Rate: 7e-06\n",
            "Step [12010/75490], Loss: 6.1229, Time: 2.10 seconds, Average Loss: 7.1715, Learning Rate: 7e-06\n",
            "Step [12020/75490], Loss: 4.4849, Time: 2.44 seconds, Average Loss: 7.1712, Learning Rate: 7e-06\n",
            "Step [12030/75490], Loss: 7.1684, Time: 1.70 seconds, Average Loss: 7.1706, Learning Rate: 7e-06\n",
            "Step [12040/75490], Loss: 6.9772, Time: 1.99 seconds, Average Loss: 7.1703, Learning Rate: 7e-06\n",
            "Step [12050/75490], Loss: 3.7464, Time: 1.71 seconds, Average Loss: 7.1697, Learning Rate: 7e-06\n",
            "Step [12060/75490], Loss: 6.8383, Time: 2.33 seconds, Average Loss: 7.1695, Learning Rate: 7e-06\n",
            "Step [12070/75490], Loss: 6.6656, Time: 1.92 seconds, Average Loss: 7.1692, Learning Rate: 7e-06\n",
            "Step [12080/75490], Loss: 6.2723, Time: 1.31 seconds, Average Loss: 7.1686, Learning Rate: 7e-06\n",
            "Step [12090/75490], Loss: 7.1140, Time: 2.08 seconds, Average Loss: 7.1685, Learning Rate: 7e-06\n",
            "Step [12100/75490], Loss: 6.0909, Time: 1.37 seconds, Average Loss: 7.1682, Learning Rate: 7e-06\n",
            "Step [12110/75490], Loss: 6.0292, Time: 1.81 seconds, Average Loss: 7.1676, Learning Rate: 7e-06\n",
            "Step [12120/75490], Loss: 6.2928, Time: 1.39 seconds, Average Loss: 7.1673, Learning Rate: 7e-06\n",
            "Step [12130/75490], Loss: 6.3731, Time: 1.88 seconds, Average Loss: 7.1660, Learning Rate: 7e-06\n",
            "Step [12140/75490], Loss: 6.7934, Time: 1.87 seconds, Average Loss: 7.1649, Learning Rate: 7e-06\n",
            "Step [12150/75490], Loss: 6.6828, Time: 2.07 seconds, Average Loss: 7.1645, Learning Rate: 7e-06\n",
            "Step [12160/75490], Loss: 6.9662, Time: 1.83 seconds, Average Loss: 7.1634, Learning Rate: 7e-06\n",
            "Step [12170/75490], Loss: 4.9048, Time: 1.80 seconds, Average Loss: 7.1628, Learning Rate: 7e-06\n",
            "Step [12180/75490], Loss: 7.6864, Time: 1.60 seconds, Average Loss: 7.1624, Learning Rate: 7e-06\n",
            "Step [12190/75490], Loss: 6.7087, Time: 1.89 seconds, Average Loss: 7.1617, Learning Rate: 7e-06\n",
            "Step [12200/75490], Loss: 5.9719, Time: 1.59 seconds, Average Loss: 7.1614, Learning Rate: 7e-06\n",
            "Step [12210/75490], Loss: 5.8341, Time: 1.61 seconds, Average Loss: 7.1608, Learning Rate: 7e-06\n",
            "Step [12220/75490], Loss: 6.8504, Time: 1.76 seconds, Average Loss: 7.1603, Learning Rate: 7e-06\n",
            "Step [12230/75490], Loss: 5.9642, Time: 1.90 seconds, Average Loss: 7.1600, Learning Rate: 7e-06\n",
            "Step [12240/75490], Loss: 8.3094, Time: 1.61 seconds, Average Loss: 7.1598, Learning Rate: 7e-06\n",
            "Step [12250/75490], Loss: 7.3819, Time: 1.61 seconds, Average Loss: 7.1591, Learning Rate: 7e-06\n",
            "Step [12260/75490], Loss: 6.7854, Time: 1.66 seconds, Average Loss: 7.1585, Learning Rate: 7e-06\n",
            "Step [12270/75490], Loss: 6.9318, Time: 4.74 seconds, Average Loss: 7.1582, Learning Rate: 7e-06\n",
            "Step [12280/75490], Loss: 6.9617, Time: 1.92 seconds, Average Loss: 7.1583, Learning Rate: 7e-06\n",
            "Step [12290/75490], Loss: 7.1260, Time: 1.84 seconds, Average Loss: 7.1579, Learning Rate: 7e-06\n",
            "Step [12300/75490], Loss: 7.1687, Time: 1.35 seconds, Average Loss: 7.1573, Learning Rate: 7e-06\n",
            "Step [12310/75490], Loss: 7.1400, Time: 1.40 seconds, Average Loss: 7.1571, Learning Rate: 7e-06\n",
            "Step [12320/75490], Loss: 5.4503, Time: 1.75 seconds, Average Loss: 7.1560, Learning Rate: 7e-06\n",
            "Step [12330/75490], Loss: 6.3791, Time: 2.17 seconds, Average Loss: 7.1555, Learning Rate: 7e-06\n",
            "Step [12340/75490], Loss: 6.5410, Time: 1.82 seconds, Average Loss: 7.1546, Learning Rate: 7e-06\n",
            "Step [12350/75490], Loss: 3.9199, Time: 1.26 seconds, Average Loss: 7.1541, Learning Rate: 7e-06\n",
            "Step [12360/75490], Loss: 9.3015, Time: 1.68 seconds, Average Loss: 7.1539, Learning Rate: 7e-06\n",
            "Step [12370/75490], Loss: 8.6509, Time: 1.31 seconds, Average Loss: 7.1533, Learning Rate: 7e-06\n",
            "Step [12380/75490], Loss: 7.2224, Time: 1.77 seconds, Average Loss: 7.1530, Learning Rate: 7e-06\n",
            "Step [12390/75490], Loss: 6.2260, Time: 1.93 seconds, Average Loss: 7.1524, Learning Rate: 7e-06\n",
            "Step [12400/75490], Loss: 6.5723, Time: 1.37 seconds, Average Loss: 7.1522, Learning Rate: 7e-06\n",
            "Step [12410/75490], Loss: 5.3804, Time: 1.35 seconds, Average Loss: 7.1515, Learning Rate: 7e-06\n",
            "Step [12420/75490], Loss: 6.6441, Time: 1.83 seconds, Average Loss: 7.1512, Learning Rate: 7e-06\n",
            "Step [12430/75490], Loss: 6.0903, Time: 1.87 seconds, Average Loss: 7.1504, Learning Rate: 7e-06\n",
            "Step [12440/75490], Loss: 6.6540, Time: 1.55 seconds, Average Loss: 7.1499, Learning Rate: 7e-06\n",
            "Step [12450/75490], Loss: 6.7863, Time: 1.75 seconds, Average Loss: 7.1493, Learning Rate: 7e-06\n",
            "Step [12460/75490], Loss: 7.2903, Time: 1.71 seconds, Average Loss: 7.1488, Learning Rate: 7e-06\n",
            "Step [12470/75490], Loss: 6.7350, Time: 1.82 seconds, Average Loss: 7.1482, Learning Rate: 7e-06\n",
            "Step [12480/75490], Loss: 7.7376, Time: 1.83 seconds, Average Loss: 7.1480, Learning Rate: 7e-06\n",
            "Step [12490/75490], Loss: 5.7480, Time: 1.85 seconds, Average Loss: 7.1475, Learning Rate: 7e-06\n",
            "Step [12500/75490], Loss: 6.3100, Time: 1.69 seconds, Average Loss: 7.1473, Learning Rate: 7e-06\n",
            "Step [12510/75490], Loss: 6.5866, Time: 2.15 seconds, Average Loss: 7.1471, Learning Rate: 7e-06\n",
            "Step [12520/75490], Loss: 6.9705, Time: 1.97 seconds, Average Loss: 7.1466, Learning Rate: 7e-06\n",
            "Step [12530/75490], Loss: 4.1685, Time: 1.98 seconds, Average Loss: 7.1459, Learning Rate: 7e-06\n",
            "Step [12540/75490], Loss: 6.9438, Time: 2.17 seconds, Average Loss: 7.1454, Learning Rate: 7e-06\n",
            "Step [12550/75490], Loss: 7.6921, Time: 1.71 seconds, Average Loss: 7.1450, Learning Rate: 7e-06\n",
            "Step [12560/75490], Loss: 7.3067, Time: 2.05 seconds, Average Loss: 7.1447, Learning Rate: 7e-06\n",
            "Step [12570/75490], Loss: 8.4728, Time: 1.87 seconds, Average Loss: 7.1443, Learning Rate: 7e-06\n",
            "Step [12580/75490], Loss: 6.5085, Time: 1.89 seconds, Average Loss: 7.1431, Learning Rate: 7e-06\n",
            "Step [12590/75490], Loss: 7.2648, Time: 1.85 seconds, Average Loss: 7.1426, Learning Rate: 7e-06\n",
            "Step [12600/75490], Loss: 4.7973, Time: 2.20 seconds, Average Loss: 7.1420, Learning Rate: 7e-06\n",
            "Step [12610/75490], Loss: 6.4220, Time: 1.50 seconds, Average Loss: 7.1412, Learning Rate: 7e-06\n",
            "Step [12620/75490], Loss: 5.8067, Time: 1.77 seconds, Average Loss: 7.1406, Learning Rate: 7e-06\n",
            "Step [12630/75490], Loss: 4.8509, Time: 1.58 seconds, Average Loss: 7.1399, Learning Rate: 7e-06\n",
            "Step [12640/75490], Loss: 7.9012, Time: 1.82 seconds, Average Loss: 7.1395, Learning Rate: 7e-06\n",
            "Step [12650/75490], Loss: 6.6268, Time: 1.95 seconds, Average Loss: 7.1395, Learning Rate: 7e-06\n",
            "Step [12660/75490], Loss: 4.0578, Time: 1.54 seconds, Average Loss: 7.1387, Learning Rate: 7e-06\n",
            "Step [12670/75490], Loss: 5.2800, Time: 1.55 seconds, Average Loss: 7.1383, Learning Rate: 7e-06\n",
            "Step [12680/75490], Loss: 6.8230, Time: 1.58 seconds, Average Loss: 7.1379, Learning Rate: 7e-06\n",
            "Step [12690/75490], Loss: 6.0582, Time: 1.73 seconds, Average Loss: 7.1372, Learning Rate: 7e-06\n",
            "Step [12700/75490], Loss: 4.9899, Time: 2.05 seconds, Average Loss: 7.1366, Learning Rate: 7e-06\n",
            "Step [12710/75490], Loss: 7.1442, Time: 2.10 seconds, Average Loss: 7.1359, Learning Rate: 7e-06\n",
            "Step [12720/75490], Loss: 7.9012, Time: 1.97 seconds, Average Loss: 7.1357, Learning Rate: 7e-06\n",
            "Step [12730/75490], Loss: 6.3300, Time: 1.91 seconds, Average Loss: 7.1352, Learning Rate: 7e-06\n",
            "Step [12740/75490], Loss: 7.1921, Time: 1.68 seconds, Average Loss: 7.1346, Learning Rate: 7e-06\n",
            "Step [12750/75490], Loss: 7.3725, Time: 1.91 seconds, Average Loss: 7.1343, Learning Rate: 7e-06\n",
            "Step [12760/75490], Loss: 6.6913, Time: 1.80 seconds, Average Loss: 7.1333, Learning Rate: 7e-06\n",
            "Step [12770/75490], Loss: 6.8709, Time: 1.46 seconds, Average Loss: 7.1330, Learning Rate: 7e-06\n",
            "Step [12780/75490], Loss: 6.8784, Time: 1.58 seconds, Average Loss: 7.1325, Learning Rate: 7e-06\n",
            "Step [12790/75490], Loss: 6.0687, Time: 1.69 seconds, Average Loss: 7.1317, Learning Rate: 7e-06\n",
            "Step [12800/75490], Loss: 5.1424, Time: 1.51 seconds, Average Loss: 7.1312, Learning Rate: 7e-06\n",
            "Step [12810/75490], Loss: 7.3683, Time: 1.34 seconds, Average Loss: 7.1304, Learning Rate: 7e-06\n",
            "Step [12820/75490], Loss: 3.5929, Time: 1.99 seconds, Average Loss: 7.1294, Learning Rate: 7e-06\n",
            "Step [12830/75490], Loss: 6.0806, Time: 2.05 seconds, Average Loss: 7.1289, Learning Rate: 7e-06\n",
            "Step [12840/75490], Loss: 6.0349, Time: 1.72 seconds, Average Loss: 7.1284, Learning Rate: 7e-06\n",
            "Step [12850/75490], Loss: 6.3176, Time: 2.27 seconds, Average Loss: 7.1283, Learning Rate: 7e-06\n",
            "Step [12860/75490], Loss: 5.6047, Time: 1.87 seconds, Average Loss: 7.1277, Learning Rate: 7e-06\n",
            "Step [12870/75490], Loss: 5.6148, Time: 1.65 seconds, Average Loss: 7.1272, Learning Rate: 7e-06\n",
            "Step [12880/75490], Loss: 5.9280, Time: 1.58 seconds, Average Loss: 7.1261, Learning Rate: 7e-06\n",
            "Step [12890/75490], Loss: 6.0446, Time: 1.78 seconds, Average Loss: 7.1257, Learning Rate: 7e-06\n",
            "Step [12900/75490], Loss: 6.2687, Time: 1.51 seconds, Average Loss: 7.1249, Learning Rate: 7e-06\n",
            "Step [12910/75490], Loss: 6.7290, Time: 1.58 seconds, Average Loss: 7.1245, Learning Rate: 7e-06\n",
            "Step [12920/75490], Loss: 6.8400, Time: 1.77 seconds, Average Loss: 7.1240, Learning Rate: 7e-06\n",
            "Step [12930/75490], Loss: 4.5772, Time: 2.08 seconds, Average Loss: 7.1236, Learning Rate: 7e-06\n",
            "Step [12940/75490], Loss: 6.4613, Time: 2.30 seconds, Average Loss: 7.1225, Learning Rate: 7e-06\n",
            "Step [12950/75490], Loss: 5.5997, Time: 1.93 seconds, Average Loss: 7.1215, Learning Rate: 7e-06\n",
            "Step [12960/75490], Loss: 6.3109, Time: 1.82 seconds, Average Loss: 7.1211, Learning Rate: 7e-06\n",
            "Step [12970/75490], Loss: 4.7422, Time: 1.53 seconds, Average Loss: 7.1205, Learning Rate: 7e-06\n",
            "Step [12980/75490], Loss: 6.2511, Time: 2.17 seconds, Average Loss: 7.1197, Learning Rate: 7e-06\n",
            "Step [12990/75490], Loss: 5.2209, Time: 1.73 seconds, Average Loss: 7.1192, Learning Rate: 7e-06\n",
            "Step [13000/75490], Loss: 6.3489, Time: 1.78 seconds, Average Loss: 7.1186, Learning Rate: 7e-06\n",
            "Step [13010/75490], Loss: 7.2536, Time: 1.49 seconds, Average Loss: 7.1183, Learning Rate: 7e-06\n",
            "Step [13020/75490], Loss: 2.6953, Time: 2.13 seconds, Average Loss: 7.1171, Learning Rate: 7e-06\n",
            "Step [13030/75490], Loss: 7.4353, Time: 2.09 seconds, Average Loss: 7.1163, Learning Rate: 7e-06\n",
            "Step [13040/75490], Loss: 7.2053, Time: 1.70 seconds, Average Loss: 7.1161, Learning Rate: 7e-06\n",
            "Step [13050/75490], Loss: 6.9362, Time: 1.41 seconds, Average Loss: 7.1158, Learning Rate: 7e-06\n",
            "Step [13060/75490], Loss: 6.2144, Time: 1.22 seconds, Average Loss: 7.1153, Learning Rate: 7e-06\n",
            "Step [13070/75490], Loss: 5.6262, Time: 1.59 seconds, Average Loss: 7.1149, Learning Rate: 7e-06\n",
            "Step [13080/75490], Loss: 7.4949, Time: 2.04 seconds, Average Loss: 7.1143, Learning Rate: 7e-06\n",
            "Step [13090/75490], Loss: 7.8831, Time: 1.80 seconds, Average Loss: 7.1144, Learning Rate: 7e-06\n",
            "Step [13100/75490], Loss: 6.6199, Time: 2.12 seconds, Average Loss: 7.1138, Learning Rate: 7e-06\n",
            "Step [13110/75490], Loss: 6.4401, Time: 1.64 seconds, Average Loss: 7.1131, Learning Rate: 7e-06\n",
            "Step [13120/75490], Loss: 6.7521, Time: 1.75 seconds, Average Loss: 7.1121, Learning Rate: 7e-06\n",
            "Step [13130/75490], Loss: 6.1989, Time: 2.22 seconds, Average Loss: 7.1114, Learning Rate: 7e-06\n",
            "Step [13140/75490], Loss: 8.2191, Time: 2.16 seconds, Average Loss: 7.1110, Learning Rate: 7e-06\n",
            "Step [13150/75490], Loss: 7.8669, Time: 1.71 seconds, Average Loss: 7.1107, Learning Rate: 7e-06\n",
            "Step [13160/75490], Loss: 7.8599, Time: 1.74 seconds, Average Loss: 7.1102, Learning Rate: 7e-06\n",
            "Step [13170/75490], Loss: 7.3505, Time: 1.82 seconds, Average Loss: 7.1099, Learning Rate: 7e-06\n",
            "Step [13180/75490], Loss: 6.6052, Time: 1.57 seconds, Average Loss: 7.1093, Learning Rate: 7e-06\n",
            "Step [13190/75490], Loss: 7.3266, Time: 1.70 seconds, Average Loss: 7.1090, Learning Rate: 7e-06\n",
            "Step [13200/75490], Loss: 5.0041, Time: 1.29 seconds, Average Loss: 7.1082, Learning Rate: 7e-06\n",
            "Step [13210/75490], Loss: 6.2512, Time: 1.71 seconds, Average Loss: 7.1076, Learning Rate: 7e-06\n",
            "Step [13220/75490], Loss: 7.3450, Time: 1.83 seconds, Average Loss: 7.1073, Learning Rate: 7e-06\n",
            "Step [13230/75490], Loss: 4.8028, Time: 1.57 seconds, Average Loss: 7.1060, Learning Rate: 7e-06\n",
            "Step [13240/75490], Loss: 7.1810, Time: 1.61 seconds, Average Loss: 7.1057, Learning Rate: 7e-06\n",
            "Step [13250/75490], Loss: 5.6153, Time: 1.77 seconds, Average Loss: 7.1049, Learning Rate: 7e-06\n",
            "Step [13260/75490], Loss: 9.0153, Time: 1.76 seconds, Average Loss: 7.1047, Learning Rate: 7e-06\n",
            "Step [13270/75490], Loss: 6.6354, Time: 1.70 seconds, Average Loss: 7.1041, Learning Rate: 7e-06\n",
            "Step [13280/75490], Loss: 6.8291, Time: 1.57 seconds, Average Loss: 7.1037, Learning Rate: 7e-06\n",
            "Step [13290/75490], Loss: 7.0506, Time: 1.85 seconds, Average Loss: 7.1033, Learning Rate: 7e-06\n",
            "Step [13300/75490], Loss: 6.2766, Time: 1.53 seconds, Average Loss: 7.1026, Learning Rate: 7e-06\n",
            "Step [13310/75490], Loss: 6.3925, Time: 1.99 seconds, Average Loss: 7.1022, Learning Rate: 7e-06\n",
            "Step [13320/75490], Loss: 5.4463, Time: 1.77 seconds, Average Loss: 7.1017, Learning Rate: 7e-06\n",
            "Step [13330/75490], Loss: 6.2408, Time: 1.92 seconds, Average Loss: 7.1012, Learning Rate: 7e-06\n",
            "Step [13340/75490], Loss: 6.1495, Time: 1.74 seconds, Average Loss: 7.1005, Learning Rate: 7e-06\n",
            "Step [13350/75490], Loss: 7.3440, Time: 1.55 seconds, Average Loss: 7.1000, Learning Rate: 7e-06\n",
            "Step [13360/75490], Loss: 7.6510, Time: 1.54 seconds, Average Loss: 7.0997, Learning Rate: 7e-06\n",
            "Step [13370/75490], Loss: 7.3799, Time: 1.57 seconds, Average Loss: 7.0994, Learning Rate: 7e-06\n",
            "Step [13380/75490], Loss: 6.0530, Time: 1.67 seconds, Average Loss: 7.0984, Learning Rate: 7e-06\n",
            "Step [13390/75490], Loss: 6.6972, Time: 1.42 seconds, Average Loss: 7.0979, Learning Rate: 7e-06\n",
            "Step [13400/75490], Loss: 5.9849, Time: 1.61 seconds, Average Loss: 7.0974, Learning Rate: 7e-06\n",
            "Step [13410/75490], Loss: 6.9045, Time: 1.98 seconds, Average Loss: 7.0974, Learning Rate: 7e-06\n",
            "Step [13420/75490], Loss: 5.4238, Time: 1.73 seconds, Average Loss: 7.0971, Learning Rate: 7e-06\n",
            "Step [13430/75490], Loss: 4.7659, Time: 1.77 seconds, Average Loss: 7.0962, Learning Rate: 7e-06\n",
            "Step [13440/75490], Loss: 6.6414, Time: 1.92 seconds, Average Loss: 7.0954, Learning Rate: 7e-06\n",
            "Step [13450/75490], Loss: 5.7178, Time: 1.34 seconds, Average Loss: 7.0947, Learning Rate: 7e-06\n",
            "Step [13460/75490], Loss: 5.4290, Time: 1.37 seconds, Average Loss: 7.0940, Learning Rate: 7e-06\n",
            "Step [13470/75490], Loss: 5.8429, Time: 1.90 seconds, Average Loss: 7.0931, Learning Rate: 7e-06\n",
            "Step [13480/75490], Loss: 6.2413, Time: 2.09 seconds, Average Loss: 7.0925, Learning Rate: 7e-06\n",
            "Step [13490/75490], Loss: 5.6846, Time: 2.06 seconds, Average Loss: 7.0922, Learning Rate: 7e-06\n",
            "Step [13500/75490], Loss: 5.3979, Time: 1.32 seconds, Average Loss: 7.0916, Learning Rate: 7e-06\n",
            "Step [13510/75490], Loss: 6.7125, Time: 2.08 seconds, Average Loss: 7.0914, Learning Rate: 7e-06\n",
            "Step [13520/75490], Loss: 7.5647, Time: 3.04 seconds, Average Loss: 7.0908, Learning Rate: 7e-06\n",
            "Step [13530/75490], Loss: 7.6611, Time: 1.44 seconds, Average Loss: 7.0905, Learning Rate: 7e-06\n",
            "Step [13540/75490], Loss: 6.2771, Time: 1.61 seconds, Average Loss: 7.0902, Learning Rate: 7e-06\n",
            "Step [13550/75490], Loss: 7.8171, Time: 1.65 seconds, Average Loss: 7.0898, Learning Rate: 7e-06\n",
            "Step [13560/75490], Loss: 6.1765, Time: 1.83 seconds, Average Loss: 7.0896, Learning Rate: 7e-06\n",
            "Step [13570/75490], Loss: 6.0028, Time: 1.57 seconds, Average Loss: 7.0892, Learning Rate: 7e-06\n",
            "Step [13580/75490], Loss: 7.0184, Time: 1.81 seconds, Average Loss: 7.0890, Learning Rate: 7e-06\n",
            "Step [13590/75490], Loss: 5.1971, Time: 2.41 seconds, Average Loss: 7.0887, Learning Rate: 7e-06\n",
            "Step [13600/75490], Loss: 6.2444, Time: 1.68 seconds, Average Loss: 7.0884, Learning Rate: 7e-06\n",
            "Step [13610/75490], Loss: 7.6034, Time: 1.90 seconds, Average Loss: 7.0881, Learning Rate: 7e-06\n",
            "Step [13620/75490], Loss: 6.4063, Time: 1.74 seconds, Average Loss: 7.0873, Learning Rate: 7e-06\n",
            "Step [13630/75490], Loss: 6.4020, Time: 2.05 seconds, Average Loss: 7.0868, Learning Rate: 7e-06\n",
            "Step [13640/75490], Loss: 7.3131, Time: 1.36 seconds, Average Loss: 7.0862, Learning Rate: 7e-06\n",
            "Step [13650/75490], Loss: 6.4795, Time: 1.79 seconds, Average Loss: 7.0860, Learning Rate: 7e-06\n",
            "Step [13660/75490], Loss: 6.2166, Time: 1.60 seconds, Average Loss: 7.0856, Learning Rate: 7e-06\n",
            "Step [13670/75490], Loss: 7.0705, Time: 1.89 seconds, Average Loss: 7.0855, Learning Rate: 7e-06\n",
            "Step [13680/75490], Loss: 6.1226, Time: 1.85 seconds, Average Loss: 7.0846, Learning Rate: 7e-06\n",
            "Step [13690/75490], Loss: 5.6162, Time: 1.58 seconds, Average Loss: 7.0839, Learning Rate: 7e-06\n",
            "Step [13700/75490], Loss: 6.9831, Time: 1.59 seconds, Average Loss: 7.0831, Learning Rate: 7e-06\n",
            "Step [13710/75490], Loss: 6.2225, Time: 1.08 seconds, Average Loss: 7.0823, Learning Rate: 7e-06\n",
            "Step [13720/75490], Loss: 6.7170, Time: 1.27 seconds, Average Loss: 7.0821, Learning Rate: 7e-06\n",
            "Step [13730/75490], Loss: 6.2937, Time: 1.93 seconds, Average Loss: 7.0818, Learning Rate: 7e-06\n",
            "Step [13740/75490], Loss: 5.7570, Time: 2.04 seconds, Average Loss: 7.0816, Learning Rate: 7e-06\n",
            "Step [13750/75490], Loss: 7.1237, Time: 1.64 seconds, Average Loss: 7.0812, Learning Rate: 7e-06\n",
            "Step [13760/75490], Loss: 5.9453, Time: 4.37 seconds, Average Loss: 7.0804, Learning Rate: 7e-06\n",
            "Step [13770/75490], Loss: 6.2650, Time: 1.76 seconds, Average Loss: 7.0798, Learning Rate: 7e-06\n",
            "Step [13780/75490], Loss: 6.4101, Time: 1.39 seconds, Average Loss: 7.0793, Learning Rate: 7e-06\n",
            "Step [13790/75490], Loss: 5.3487, Time: 2.22 seconds, Average Loss: 7.0786, Learning Rate: 7e-06\n",
            "Step [13800/75490], Loss: 6.8053, Time: 1.64 seconds, Average Loss: 7.0782, Learning Rate: 7e-06\n",
            "Step [13810/75490], Loss: 6.0171, Time: 1.61 seconds, Average Loss: 7.0774, Learning Rate: 7e-06\n",
            "Step [13820/75490], Loss: 6.2275, Time: 1.61 seconds, Average Loss: 7.0768, Learning Rate: 7e-06\n",
            "Step [13830/75490], Loss: 9.4654, Time: 2.27 seconds, Average Loss: 7.0764, Learning Rate: 7e-06\n",
            "Step [13840/75490], Loss: 6.8576, Time: 1.55 seconds, Average Loss: 7.0757, Learning Rate: 7e-06\n",
            "Step [13850/75490], Loss: 6.5360, Time: 1.30 seconds, Average Loss: 7.0754, Learning Rate: 7e-06\n",
            "Step [13860/75490], Loss: 6.2220, Time: 1.95 seconds, Average Loss: 7.0749, Learning Rate: 7e-06\n",
            "Step [13870/75490], Loss: 4.3518, Time: 1.50 seconds, Average Loss: 7.0745, Learning Rate: 7e-06\n",
            "Step [13880/75490], Loss: 6.6105, Time: 1.92 seconds, Average Loss: 7.0740, Learning Rate: 7e-06\n",
            "Step [13890/75490], Loss: 7.0755, Time: 1.73 seconds, Average Loss: 7.0740, Learning Rate: 7e-06\n",
            "Step [13900/75490], Loss: 7.3277, Time: 1.87 seconds, Average Loss: 7.0734, Learning Rate: 7e-06\n",
            "Step [13910/75490], Loss: 5.6779, Time: 1.38 seconds, Average Loss: 7.0729, Learning Rate: 7e-06\n",
            "Step [13920/75490], Loss: 6.4751, Time: 1.51 seconds, Average Loss: 7.0725, Learning Rate: 7e-06\n",
            "Step [13930/75490], Loss: 7.3383, Time: 1.97 seconds, Average Loss: 7.0718, Learning Rate: 7e-06\n",
            "Step [13940/75490], Loss: 7.1912, Time: 1.38 seconds, Average Loss: 7.0716, Learning Rate: 7e-06\n",
            "Step [13950/75490], Loss: 7.0603, Time: 1.84 seconds, Average Loss: 7.0712, Learning Rate: 7e-06\n",
            "Step [13960/75490], Loss: 4.9499, Time: 1.66 seconds, Average Loss: 7.0703, Learning Rate: 7e-06\n",
            "Step [13970/75490], Loss: 5.4882, Time: 1.57 seconds, Average Loss: 7.0695, Learning Rate: 7e-06\n",
            "Step [13980/75490], Loss: 6.5111, Time: 2.20 seconds, Average Loss: 7.0687, Learning Rate: 7e-06\n",
            "Step [13990/75490], Loss: 5.5706, Time: 1.23 seconds, Average Loss: 7.0681, Learning Rate: 7e-06\n",
            "Step [14000/75490], Loss: 5.3726, Time: 1.49 seconds, Average Loss: 7.0669, Learning Rate: 7e-06\n",
            "Step [14010/75490], Loss: 6.6151, Time: 1.86 seconds, Average Loss: 7.0663, Learning Rate: 7e-06\n",
            "Step [14020/75490], Loss: 7.1174, Time: 1.40 seconds, Average Loss: 7.0655, Learning Rate: 7e-06\n",
            "Step [14030/75490], Loss: 5.5290, Time: 1.59 seconds, Average Loss: 7.0651, Learning Rate: 7e-06\n",
            "Step [14040/75490], Loss: 6.2978, Time: 1.37 seconds, Average Loss: 7.0646, Learning Rate: 7e-06\n",
            "Step [14050/75490], Loss: 5.2606, Time: 1.76 seconds, Average Loss: 7.0635, Learning Rate: 7e-06\n",
            "Step [14060/75490], Loss: 7.9703, Time: 1.43 seconds, Average Loss: 7.0630, Learning Rate: 7e-06\n",
            "Step [14070/75490], Loss: 6.7787, Time: 1.72 seconds, Average Loss: 7.0624, Learning Rate: 7e-06\n",
            "Step [14080/75490], Loss: 6.1833, Time: 2.54 seconds, Average Loss: 7.0624, Learning Rate: 7e-06\n",
            "Step [14090/75490], Loss: 6.3779, Time: 1.92 seconds, Average Loss: 7.0618, Learning Rate: 7e-06\n",
            "Step [14100/75490], Loss: 6.4366, Time: 1.55 seconds, Average Loss: 7.0615, Learning Rate: 7e-06\n",
            "Step [14110/75490], Loss: 6.1190, Time: 2.01 seconds, Average Loss: 7.0611, Learning Rate: 7e-06\n",
            "Step [14120/75490], Loss: 6.9909, Time: 2.10 seconds, Average Loss: 7.0609, Learning Rate: 7e-06\n",
            "Step [14130/75490], Loss: 6.2199, Time: 1.79 seconds, Average Loss: 7.0601, Learning Rate: 7e-06\n",
            "Step [14140/75490], Loss: 7.6018, Time: 1.17 seconds, Average Loss: 7.0598, Learning Rate: 7e-06\n",
            "Step [14150/75490], Loss: 6.7009, Time: 1.39 seconds, Average Loss: 7.0597, Learning Rate: 7e-06\n",
            "Step [14160/75490], Loss: 6.0155, Time: 1.50 seconds, Average Loss: 7.0589, Learning Rate: 7e-06\n",
            "Step [14170/75490], Loss: 5.6529, Time: 1.53 seconds, Average Loss: 7.0587, Learning Rate: 7e-06\n",
            "Step [14180/75490], Loss: 6.5472, Time: 1.93 seconds, Average Loss: 7.0582, Learning Rate: 7e-06\n",
            "Step [14190/75490], Loss: 6.3722, Time: 1.38 seconds, Average Loss: 7.0573, Learning Rate: 7e-06\n",
            "Step [14200/75490], Loss: 6.7310, Time: 1.88 seconds, Average Loss: 7.0566, Learning Rate: 7e-06\n",
            "Step [14210/75490], Loss: 7.2075, Time: 2.10 seconds, Average Loss: 7.0563, Learning Rate: 7e-06\n",
            "Step [14220/75490], Loss: 4.8041, Time: 1.75 seconds, Average Loss: 7.0557, Learning Rate: 7e-06\n",
            "Step [14230/75490], Loss: 8.4190, Time: 1.59 seconds, Average Loss: 7.0555, Learning Rate: 7e-06\n",
            "Step [14240/75490], Loss: 6.1142, Time: 1.24 seconds, Average Loss: 7.0544, Learning Rate: 7e-06\n",
            "Step [14250/75490], Loss: 6.9744, Time: 1.70 seconds, Average Loss: 7.0536, Learning Rate: 7e-06\n",
            "Step [14260/75490], Loss: 6.6234, Time: 1.78 seconds, Average Loss: 7.0537, Learning Rate: 7e-06\n",
            "Step [14270/75490], Loss: 7.2624, Time: 1.72 seconds, Average Loss: 7.0533, Learning Rate: 7e-06\n",
            "Step [14280/75490], Loss: 7.3381, Time: 1.94 seconds, Average Loss: 7.0527, Learning Rate: 7e-06\n",
            "Step [14290/75490], Loss: 5.3823, Time: 1.77 seconds, Average Loss: 7.0517, Learning Rate: 7e-06\n",
            "Step [14300/75490], Loss: 6.7058, Time: 1.49 seconds, Average Loss: 7.0511, Learning Rate: 7e-06\n",
            "Step [14310/75490], Loss: 7.5249, Time: 1.57 seconds, Average Loss: 7.0503, Learning Rate: 7e-06\n",
            "Step [14320/75490], Loss: 6.5603, Time: 1.53 seconds, Average Loss: 7.0504, Learning Rate: 7e-06\n",
            "Step [14330/75490], Loss: 6.2873, Time: 1.53 seconds, Average Loss: 7.0499, Learning Rate: 7e-06\n",
            "Step [14340/75490], Loss: 7.2298, Time: 1.88 seconds, Average Loss: 7.0494, Learning Rate: 7e-06\n",
            "Step [14350/75490], Loss: 7.0424, Time: 1.69 seconds, Average Loss: 7.0492, Learning Rate: 7e-06\n",
            "Step [14360/75490], Loss: 4.2501, Time: 2.02 seconds, Average Loss: 7.0486, Learning Rate: 7e-06\n",
            "Step [14370/75490], Loss: 7.1427, Time: 1.72 seconds, Average Loss: 7.0483, Learning Rate: 7e-06\n",
            "Step [14380/75490], Loss: 7.2091, Time: 2.22 seconds, Average Loss: 7.0475, Learning Rate: 7e-06\n",
            "Step [14390/75490], Loss: 5.1464, Time: 2.00 seconds, Average Loss: 7.0469, Learning Rate: 7e-06\n",
            "Step [14400/75490], Loss: 5.1341, Time: 1.85 seconds, Average Loss: 7.0466, Learning Rate: 7e-06\n",
            "Step [14410/75490], Loss: 6.8819, Time: 1.85 seconds, Average Loss: 7.0460, Learning Rate: 7e-06\n",
            "Step [14420/75490], Loss: 6.2315, Time: 1.72 seconds, Average Loss: 7.0453, Learning Rate: 7e-06\n",
            "Step [14430/75490], Loss: 5.6635, Time: 1.69 seconds, Average Loss: 7.0445, Learning Rate: 7e-06\n",
            "Step [14440/75490], Loss: 5.1719, Time: 1.90 seconds, Average Loss: 7.0438, Learning Rate: 7e-06\n",
            "Step [14450/75490], Loss: 5.5930, Time: 1.58 seconds, Average Loss: 7.0432, Learning Rate: 7e-06\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.82 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1.36 GiB is free. Process 469828 has 13.38 GiB memory in use. Of the allocated memory 11.81 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-db3a57a6855c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-34b79c61f6c8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, opt, loss_fn, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Epoch {epoch + 1}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-3265e44860ea>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, opt, loss_fn, dataloader)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# print(pred[0,:,:], y_expected[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_expected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.82 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1.36 GiB is free. Process 469828 has 13.38 GiB memory in use. Of the allocated memory 11.81 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'transformer_model.pth'\n",
        "model_path = '/content/drive/MyDrive/transformer/'\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "os.chdir(model_path)\n",
        "\n",
        "torch.save(model.state_dict(), model_name)"
      ],
      "metadata": {
        "id": "nxU_vo1cphkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_saved = Transformer(\n",
        "    num_tokens=enc_voc_size, dim_model=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dropout_p=0.1\n",
        ").to(device)\n",
        "\n",
        "model_location = os.path.join(model_path, model_name)\n",
        "model_saved.load_state_dict(torch.load(model_location, map_location=device))"
      ],
      "metadata": {
        "id": "0DDNm0Rypor8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, input_sequence, max_length=512, SOS_token=trg_sos_idx, EOS_token=trg_eos_idx):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
        "\n",
        "    num_tokens = len(input_sequence[0])\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Get source mask\n",
        "        tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
        "\n",
        "        pred = model(input_sequence, y_input, tgt_mask)\n",
        "\n",
        "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
        "        next_item = torch.tensor([[next_item]], device=device)\n",
        "\n",
        "        # Concatenate previous input with predicted best word\n",
        "        y_input = torch.cat((y_input, next_item), dim=1)\n",
        "\n",
        "        # Stop if model predicts end of sentence\n",
        "        if next_item.view(-1).item() == EOS_token:\n",
        "            break\n",
        "\n",
        "    return y_input.view(-1).tolist()"
      ],
      "metadata": {
        "id": "09QJSOOGUGT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_encode(text, tokenizer):\n",
        "  input_ids = np.array(tokenizer.encode(text).ids)\n",
        "\n",
        "  SOS_token = np.array([trg_sos_idx])\n",
        "  EOS_token = np.array([trg_eos_idx])\n",
        "  #print(SOS_token, EOS_token)\n",
        "  X = np.concatenate((SOS_token, input_ids, EOS_token))\n",
        "  print(X)\n",
        "  return X"
      ],
      "metadata": {
        "id": "Lc_CBTvSUNdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    torch.tensor([text_to_encode(\"dosarul sa stricta.\", tokenizer)], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device),\n",
        "    # torch.tensor([[2, 0, 1, 3]], dtype=torch.long, device=device)\n",
        "]\n",
        "\n",
        "for idx, example in enumerate(examples):\n",
        "    result = predict(model_saved, example)\n",
        "    print(f\"Example {idx}\")\n",
        "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
        "    print(f\"Continuation: {result}\")\n",
        "    print(f\"Text-OUTPUT:\", tokenizer.decode(result[1:-1]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "8Q3F9SSXpuHi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}